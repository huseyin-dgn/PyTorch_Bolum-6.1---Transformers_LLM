{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7784340f",
   "metadata": {},
   "source": [
    "---\n",
    "### KOD İÇERİĞİ AŞAĞIDAKİ GİBİDİR.\n",
    "---\n",
    "# 1️⃣ Tokenizer & Dataset\n",
    "\n",
    "## TokenizerDataset (Dataset class)\n",
    "- `__init__` → `sources`, `targets`, `tokenizer_name`, `max_length`, `decoder_start_token_id`  \n",
    "- `__len__` → dataset uzunluğu  \n",
    "- `__getitem__` → \n",
    "  - `encoder_input_ids`, `encoder_attention_mask`\n",
    "  - `decoder_input_ids`, `decoder_attention_mask`\n",
    "  - `decoder_target_ids`\n",
    "\n",
    "## collate_fn\n",
    "- Batch verilerini stack ederek tensor haline getirir:  \n",
    "  - `'encoder_input_ids'`, `'encoder_attention_mask'`  \n",
    "  - `'decoder_input_ids'`, `'decoder_attention_mask'`  \n",
    "  - `'decoder_target_ids'`  \n",
    "\n",
    "## DataLoader\n",
    "- `batch_size`, `shuffle`, `collate_fn` ile tanımlanır  \n",
    "\n",
    "\n",
    "# 2️⃣ Model Yapısı (Seq2Seq LLM)\n",
    "\n",
    "## 2.1 Encoder\n",
    "- **TokenEmbed** → Token embedding  \n",
    "- **PositionelEncod** → Positional encoding  \n",
    "- **MultiHeadAttention** → Self-attention  \n",
    "- **FeedForward** → FFN (GELU veya SwiGLU ile)  \n",
    "- **TransformerEncoderBlockLLM** → \n",
    "  - LayerNorm  \n",
    "  - Self-Attention  \n",
    "  - FeedForward  \n",
    "  - DropPath  \n",
    "- **TransformersEncoderLLM** → \n",
    "  - Embedding + Positional encoding  \n",
    "  - Encoder blokları (ModuleList)  \n",
    "  - LayerNorm  \n",
    "\n",
    "## 2.2 Decoder\n",
    "- **TransformerDecoderBlockLLM** → \n",
    "  - Masked Self-Attention  \n",
    "  - Cross-Attention  \n",
    "  - FeedForward  \n",
    "  - LayerNorm + DropPath + Gamma parametreleri  \n",
    "- **TransformerDecoderLLM** → \n",
    "  - Embedding + Positional encoding  \n",
    "  - Decoder blokları  \n",
    "  - LayerNorm  \n",
    "  - `lm_head` → vocab boyutunda linear layer  \n",
    "\n",
    "## 2.3 Seq2Seq Model\n",
    "- **Seq2SeqLLM** → \n",
    "  - Encoder ve Decoder’ı birleştirir  \n",
    "  - `forward` → `src_tokens` → encoder → `tgt_tokens` → decoder → logits  \n",
    "\n",
    "\n",
    "# 3️⃣ Generation (Top-K / Top-P)\n",
    "- **generate_seq2seq**\n",
    "  - `src_texts` → tokenize edilir  \n",
    "  - `dec_input_ids` → başlangıç tokenları (pad veya bos)  \n",
    "  - Döngü ile max_len boyunca token üretimi:\n",
    "    - Top-K filtreleme  \n",
    "    - Top-P (nucleus) filtreleme  \n",
    "    - `torch.multinomial` ile sampling  \n",
    "  - Sonuç → decoded string listesi  \n",
    "\n",
    "\n",
    "# 4️⃣ Loss & Metrics\n",
    "\n",
    "## 4.1 Loss\n",
    "- **masked_cross_entropy_loss**\n",
    "  - CrossEntropyLoss with `ignore_index=pad_token_id`  \n",
    "\n",
    "## 4.2 Accuracy\n",
    "- **accuracy_fn** → pad mask ile token-level accuracy  \n",
    "- **top_k_accuracy_fn** → top-k içinde doğru token sayısı  \n",
    "\n",
    "## 4.3 Perplexity & BLEU\n",
    "- **calculate_llm_metrics**\n",
    "  - Perplexity → CrossEntropyLoss(reduction='none') ve exp(mean)  \n",
    "  - Sentence-level BLEU → `sentence_bleu` + smoothing  \n",
    "  - Corpus-level BLEU → `corpus_bleu` + smoothing  \n",
    "\n",
    "\n",
    "# 5️⃣ Training Loop\n",
    "- **LLM_Train**\n",
    "  - Epoch loop  \n",
    "  - Forward pass: `model(src_tokens, tgt_tokens)`  \n",
    "  - Loss backward → optimizer.step + scheduler.step  \n",
    "  - Metrics hesaplama: `accuracy`, `top5`, `ppl`, `BLEU (sent + corpus)`  \n",
    "  - Her `sample_interval` epoch: Top-K + Top-P örnek generation  \n",
    "  - Postfix ve epoch print ile log  \n",
    "\n",
    "\n",
    "\n",
    "# 6️⃣ Optimizer & Scheduler\n",
    "- **AdamW** → lr, weight_decay  \n",
    "- **get_linear_schedule_with_warmup**\n",
    "  - `num_warmup_steps`  \n",
    "  - `num_training_steps`  \n",
    "\n",
    "\n",
    "\n",
    "# 7️⃣ Device / CUDA\n",
    "- `device = \"cuda\" if torch.cuda.is_available() else \"cpu\"`  \n",
    "- Model ve batch tensorları GPU’ya taşınıyor: `.to(device)`  \n",
    "- `torch.cuda.empty_cache()` → notebook’ta yeniden çalıştırmadan önce\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
