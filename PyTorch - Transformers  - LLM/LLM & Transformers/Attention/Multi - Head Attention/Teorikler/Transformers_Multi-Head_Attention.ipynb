{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b789a453",
   "metadata": {},
   "source": [
    "# Multi‑Head Attention — Teori\n",
    "\n",
    "**Amaç:** Bu hücre seti, Transformer modellerinin merkezinde yer alan *Multi‑Head Attention* (Çok Başlıklı Dikkat) mekanizmasını adım adım teorik olarak açıklar. Matematiksel formülasyon, tensör şekilleri, sezgisel açıklama ve uygulamada dikkat edilmesi gereken noktalar yer alır.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Kısa Özet\n",
    "\n",
    "* Attention, bir dizi öğe arasındaki bağımlılıkları öğrenmeyi sağlar; her öğe diğer öğelere \"ne kadar dikkat etmeli\" sorusuna yanıt arar.\n",
    "* Scaled Dot‑Product Attention, Transformer'da kullanılan temel dikkat çeşididir.\n",
    "* Multi‑Head Attention, aynı anda birden fazla dikkat \"başı\" çalıştırarak farklı alt‑uzaylarda ilişkileri yakalar.\n",
    "\n",
    "## 2. Neden dikkat (Attention)?\n",
    "\n",
    "Örnek: bir çeviri görevinde, hedefteki bir kelime kaynak cümlenin hangi kelimelerine dayanmalıdır? Dikkat mekanizması, hedef token için kaynak token'lara ağırlık (score) verir — böylece model bağlamdan dinamik olarak bilgi toplar.\n",
    "\n",
    "\n",
    "## 3. Scaled Dot‑Product Attention (Ölçeklenmiş Nokta Çarpımı Dikkat)\n",
    "\n",
    "Verilen sorgu (queries) $Q$, anahtarlar (keys) $K$ ve değerler (values) $V$ için:\n",
    "\n",
    "[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V\n",
    "]\n",
    "\n",
    "* $Q \\in \\mathbb{R}^{n_q \\times d_k}$, $K \\in \\mathbb{R}^{n_k \\times d_k}$, $V \\in \\mathbb{R}^{n_k \\times d_v}$\n",
    "* Bölme faktörü $\\sqrt{d_k}$, nokta çarpımı skorlarının varyansını kontrol ederek softmax'ten önce numerik kararlılık sağlar.\n",
    "* Çıktı, her sorgu için değerlerin bir ağırlıklı toplamıdır.\n",
    "\n",
    "**Adımlar:**\n",
    "\n",
    "1. Skorları hesapla: $S = QK^{T}$ (boyut: $n_q \\times n_k$)\n",
    "2. Skorları ölçekle: $S' = S / \\sqrt{d_k}$\n",
    "3. Softmax uygula: $A = \\text{softmax}(S')$ (satır bazlı softmax)\n",
    "4. Değerleri topla: $O = A V$ (boyut: $n_q \\times d_v$)\n",
    "\n",
    "## 4. Multi‑Head Attention — Matematik\n",
    "\n",
    "Multi‑Head Attention $h$ adet baş kullanır. Her başın kendi proje (lineer) matrisleri vardır:\n",
    "\n",
    "* $W^{Q}*{i} \\in \\mathbb{R}^{d*{model} \\times d_k}$\n",
    "* $W^{K}*{i} \\in \\mathbb{R}^{d*{model} \\times d_k}$\n",
    "* $W^{V}*{i} \\in \\mathbb{R}^{d*{model} \\times d_v}$\n",
    "\n",
    "Her baş için:\n",
    "\n",
    "[\n",
    "\\text{head}*i = \\text{Attention}(QW^{Q}*{i}, KW^{K}*{i}, VW^{V}*{i})\n",
    "]\n",
    "\n",
    "Tüm başlar birleştirildikten sonra bir çıktı projeksiyonu uygulanır:\n",
    "\n",
    "[\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^{O}\n",
    "]\n",
    "\n",
    "burada $W^{O} \\in \\mathbb{R}^{hd_v \\times d_{model}}$.\n",
    "\n",
    "**Tipik seçim:** $d_k = d_v = d_{model}/h$ iken her bir başın boyutu küçülür ve toplam hesap $d_{model}$'e yakın kalır.\n",
    "\n",
    "\n",
    "## 5. Tensör Şekilleri (Batch içeren hâl)\n",
    "\n",
    "* Girdi: $X$ şekli `(batch_size, seq_len, d_model)`\n",
    "* Projeksiyonlar sonrası Q, K, V: `(batch_size, seq_len, h, d_k)` (veya `(batch_size, h, seq_len, d_k)` olarak yeniden şekillendirilir)\n",
    "* Skor matrisi: `(batch_size, h, seq_len_q, seq_len_k)`\n",
    "* Her başın çıktısı: `(batch_size, h, seq_len_q, d_v)`\n",
    "* Birleştirilmiş çıktı: `(batch_size, seq_len_q, h * d_v)` → ardından `W^O` ile `(batch_size, seq_len_q, d_model)`\n",
    "\n",
    "\n",
    "## 6. Sezgisel Anlatım\n",
    "\n",
    "* Her baş, projeksiyonları sayesinde farklı bir \"altyapı\"ya (subspace) bakar — örneğin başlardan biri sözcük biçimsel eşleşmelere, diğeri sözdizimine, bir diğeri uzun menzilli ilişkilere odaklanabilir.\n",
    "* Parçalamak, modelin eş zamanlı olarak farklı ilişki türlerini öğrenmesine izin verir.\n",
    "\n",
    "\n",
    "\n",
    "## 7. Maskeler ve Kullanım Durumları\n",
    "\n",
    "* **Padding Mask:** Düzensiz uzunluklarda dolgu (padding) tokenlarının dikkat alımını engellemek için.\n",
    "* **Look‑Ahead / Causal Mask:** Otomatik dil modellemede, gelecekteki tokenların görülmesini engellemek için (decoder self‑attention).\n",
    "\n",
    "Maskeler skorlara eklenir (büyük negatif sayılarla) softmax uygulamadan önce.\n",
    "\n",
    "\n",
    "\n",
    "## 8. Hesaplama Karmaşıklığı\n",
    "\n",
    "* Standart attention: $O(n^{2} \\cdot d)$ zaman ve $O(n^{2})$ hafıza (n = seq_len)\n",
    "* Bu sebeple uzun dizilerde dikkat pahalıdır; pek çok çalışma bu maliyeti azaltmaya yönelik alternatifler sunar (sparse, local, linear attention vb.).\n",
    "\n",
    "\n",
    "\n",
    "## 9. Uygulama Notları / İpuçları\n",
    "\n",
    "* `d_model` genelde `num_heads * d_k` ile eşleşmelidir.\n",
    "* Kullandığınız başlangıç (initialization) ve normalizasyon (LayerNorm) sonuçları etkiler.\n",
    "* Dropout, softmax sonrası attention ağırlıklarına uygulanır.\n",
    "* Kullandığınız `dtype` (float32/float16) ve numerik kararlılık için ölçekleme önemlidir.\n",
    "\n",
    "\n",
    "\n",
    "## 10. Kısa Örnek Sorular (Alıştırma)\n",
    "\n",
    "1. Neden skoru $\\sqrt{d_k}$ ile böleriz? (numerik stabilite ve gradient büyüklüğünü kontrol etme)\n",
    "2. Eğer `d_model = 512` ve `num_heads = 8` ise `d_k` kaç olmalı? (cevap: 64)\n",
    "3. Look‑ahead mask nasıl oluşturulur? (üçgen bir maskeler matrisi — üst üçgen `-inf`)\n",
    "\n",
    "\n",
    "\n",
    "## 11. Kaynaklar / İleri Okuma\n",
    "\n",
    "* Vaswani et al., *Attention Is All You Need* (2017)\n",
    "* Transformer ders notları ve görselleştirme araçları\n",
    "\n",
    "\n",
    "\n",
    "*Bu hücre teorik açıklamayı tamamlar. Sonraki hücrede bu matematiği PyTorch / NumPy ile çalışır bir `MultiHeadAttention` sınıfına dönüştüreceğiz.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7c720",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
