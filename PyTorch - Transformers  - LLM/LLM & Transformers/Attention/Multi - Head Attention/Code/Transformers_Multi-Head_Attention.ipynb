{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ccfbbb",
   "metadata": {},
   "source": [
    "## PyTorch ile Basit MultiHeadSelfAttention (BaÅŸlangÄ±Ã§);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09a34c",
   "metadata": {},
   "source": [
    "* AÅŸaÄŸÄ±daki hÃ¼cre, teoride anlattÄ±ÄŸÄ±mÄ±z adÄ±mlarÄ± takip eden, aÃ§Ä±klamalÄ± ve test edilebilir bir PyTorch implementasyonudur. Bu sÃ¼rÃ¼m eÄŸitim ve geliÅŸtirme iÃ§in okunaklÄ± ve geniÅŸletilebilir tutuldu â€” sonraki adÄ±mlarda performans optimizasyonu, flash attention, qkv tek matris haline getirme, rotary embedding, ya da causal masking iÃ§in ilerleyeceÄŸiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6c0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0da2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model : int ,num_heads:int=8 , dp:float = 0.3 ):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0 , \"d_model num_heads'e tam bÃ¶lÃ¼nmelidir\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.q_lin = nn.Linear(d_model , d_model)\n",
    "        self.v_lin = nn.Linear(d_model , d_model)\n",
    "        self.k_lin = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.out_lin = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self,x:torch.Tensor , mask:torch.Tensor = None):\n",
    "        b,n,d = x.size()\n",
    "        q = self.q_lin(x).view(b,n,self.num_heads , self.d_k).transpose(1,2)\n",
    "        k = self.k_lin(x).view(b,n,self.num_heads , self.d_k).transpose(1,2)\n",
    "        v = self.v_lin(x).view(b,n,self.num_heads , self.d_k).transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            elif mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill_(mask == 0 , float(\"-inf\"))\n",
    "        \n",
    "        attn= F.softmax(scores , dim= -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn,v)\n",
    "        out = out.transpose(1,2).contiguous().view(b,n,d)\n",
    "        out = self.out_lin(out)\n",
    "        return out , attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e6d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape = torch.Size([2, 5, 64])\n",
      "attn.shape = torch.Size([2, 8, 5, 5])\n",
      "masked attn sum row (first batch, first head): 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "    batch, seq_len, d_model = 2, 5, 64\n",
    "    x = torch.randn(batch, seq_len, d_model)\n",
    "    model = MultiHeadAttention(d_model=d_model, num_heads=8, dp=0.)\n",
    "    out, attn = model(x)\n",
    "    print('out.shape =', out.shape) # beklenen: (2,5,64)\n",
    "    print('attn.shape =', attn.shape) # beklenen: (2,8,5,5)\n",
    "\n",
    "\n",
    "# padding mask Ã¶rneÄŸi\n",
    "    mask = torch.tensor([[1,1,1,0,0],[1,1,1,1,0]])\n",
    "    out_masked, attn_masked = model(x, mask=mask)\n",
    "    print('masked attn sum row (first batch, first head):', attn_masked[0,0,0].sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e5126",
   "metadata": {},
   "source": [
    "### Åu an elimizde:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3103a8d",
   "metadata": {},
   "source": [
    "\n",
    "âœ… AyrÄ± Q, K, V lineer katmanlarÄ± var.\n",
    "\n",
    "âœ… Dropout ve padding mask desteÄŸi eklendi.\n",
    "\n",
    "âœ… Dikkat skorlarÄ± Ã¶lÃ§ekleniyor ve softmax sonrasÄ± normalize ediliyor.\n",
    "\n",
    "âœ… Multi-Head yapÄ±sÄ± dinamik ve testle doÄŸrulandÄ±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a2417",
   "metadata": {},
   "source": [
    "## ğŸ” Multi-Head Attention â€” Kod Ã–zeti\n",
    "\n",
    "1. **Linear Projeksiyonlar:**  \n",
    "   Girdi (x) â†’ `q_proj`, `k_proj`, `v_proj` katmanlarÄ±yla sÄ±rasÄ±yla **Query (Q)**, **Key (K)**, **Value (V)** matrislerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.\n",
    "\n",
    "2. **Head BÃ¶lÃ¼nmesi:**  \n",
    "   `embed_dim`, `num_heads`â€™e bÃ¶lÃ¼nÃ¼r â†’ her headâ€™in boyutu `head_dim = embed_dim // num_heads` olur.  \n",
    "   Tensorlar `(batch, seq_len, num_heads, head_dim)` biÃ§imine getirilir.\n",
    "\n",
    "3. **Skor HesabÄ±:**  \n",
    "   Her head iÃ§in dikkat skorlarÄ± hesaplanÄ±r:  \n",
    "   \\[\n",
    "   \\text{scores} = \\frac{QK^T}{\\sqrt{head\\_dim}}\n",
    "   \\]\n",
    "   Daha sonra `Softmax` uygulanarak olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± elde edilir.\n",
    "\n",
    "4. **AÄŸÄ±rlÄ±klÄ± Toplama:**  \n",
    "   Bu daÄŸÄ±lÄ±m `V` ile Ã§arpÄ±larak her headâ€™in Ã§Ä±ktÄ±sÄ± bulunur:\n",
    "   \\[\n",
    "   \\text{head\\_output} = \\text{Softmax(scores)} \\times V\n",
    "   \\]\n",
    "\n",
    "5. **BirleÅŸtirme ve Lineer Ã‡Ä±kÄ±ÅŸ:**  \n",
    "   TÃ¼m head Ã§Ä±ktÄ±larÄ± `concat` edilir ve `out_proj` katmanÄ±yla tekrar `embed_dim` boyutuna dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.\n",
    "\n",
    "6. **Opsiyonel Dropout / Norm:**  \n",
    "   EÄŸitim sÄ±rasÄ±nda aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nlemek iÃ§in Dropout ve LayerNorm eklenebilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0178a9",
   "metadata": {},
   "source": [
    "---\n",
    "## Åimdi bu attention mekanizmasÄ±nÄ± daha da ileri taÅŸÄ±yalÄ±m. ;\n",
    "* Q/K/V projeksiyonlarÄ±nÄ± tek lineer katmanda birleÅŸtirme (W_qkv â†’ performans artÄ±ÅŸÄ±).\n",
    "\n",
    "* Causal mask (look-ahead) ekleyerek dil modelleme ve autoregressive gÃ¶revler iÃ§in uygun hale getirme\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3ecf6",
   "metadata": {},
   "source": [
    "## âš™ï¸ GeliÅŸtirme 1 â€” Q/K/V'yi Tek Lineer Katmanda BirleÅŸtirme\n",
    "\n",
    "Normalde `q_proj`, `k_proj`, `v_proj` olmak Ã¼zere 3 ayrÄ± `nn.Linear` kullanÄ±lÄ±r.  \n",
    "Ancak performans kazanmak iÃ§in **tek bir bÃ¼yÃ¼k aÄŸÄ±rlÄ±k matrisi (W_qkv)** ile bu iÅŸlemler birleÅŸtirilebilir.\n",
    "\n",
    "\\[\n",
    "[Q, K, V] = xW_{qkv}\n",
    "\\]\n",
    "\n",
    "Bu yaklaÅŸÄ±m:\n",
    "- Hesaplama maliyetini azaltÄ±r (tek matris Ã§arpÄ±mÄ±),\n",
    "- GPU optimizasyonundan daha iyi faydalanÄ±r.\n",
    "\n",
    "\n",
    "## âš™ï¸ GeliÅŸtirme 2 â€” Causal Mask (Look-Ahead)\n",
    "\n",
    "Dil modelleme ve autoregressive gÃ¶revlerde, modelin **gelecekteki tokenâ€™larÄ± gÃ¶rmemesi** gerekir.  \n",
    "Bu amaÃ§la **causal mask** (Ã¼Ã§gensel maske) uygulanÄ±r:\n",
    "\n",
    "\\[\n",
    "mask_{ij} = \n",
    "\\begin{cases}\n",
    "0, & \\text{if } j \\le i \\\\\n",
    "-\\infty, & \\text{if } j > i\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Softmax Ã¶ncesinde bu maske eklenir:\n",
    "\\[\n",
    "\\text{scores} = \\frac{QK^T}{\\sqrt{d_k}} + \\text{mask}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014fa138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8, dp: float = 0.1, causal: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.causal = causal\n",
    "        \n",
    "        # ğŸ”¹ Q/K/V birleÅŸik projeksiyon katmanÄ±\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1ï¸âƒ£ Tek lineer katmandan Q, K, V Ã¼retimi\n",
    "        qkv = self.qkv_proj(x)  # (B, L, 3E)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # ÃœÃ§ parÃ§aya ayÄ±r\n",
    "        \n",
    "        # 2ï¸âƒ£ Head bÃ¶lÃ¼nmesi\n",
    "        def reshape_heads(t):\n",
    "            return t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        Q, K, V = map(reshape_heads, (Q, K, V))\n",
    "        \n",
    "        # 3ï¸âƒ£ Skor hesabÄ±\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # 4ï¸âƒ£ Causal Mask ekleme (look-ahead)\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, V)  # (B, num_heads, L, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, E)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10, 512)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8, causal=True)\n",
    "y = mha(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89085901",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš™ï¸ AÅŸama 3 â€” Residual BaÄŸlantÄ± + Layer Normalization\n",
    "\n",
    "### ğŸ”¹ Residual Connection\n",
    "Multi-Head Attention bloÄŸu, giriÅŸ bilgisini tamamen kaybetmesin diye:\n",
    "\\[\n",
    "y = x + \\text{Dropout}(\\text{Attention}(x))\n",
    "\\]\n",
    "ÅŸeklinde geri eklenir.  \n",
    "Bu, **gradyan akÄ±ÅŸÄ±nÄ± kolaylaÅŸtÄ±rÄ±r** ve **Ã¶ÄŸrenmeyi stabilize eder**.\n",
    "\n",
    "### ğŸ”¹ Layer Normalization\n",
    "Residual toplama sonrasÄ±, modelin Ã§Ä±ktÄ±sÄ±nÄ± normalize ederek daha kararlÄ± hale getirir:\n",
    "\\[\n",
    "\\text{output} = \\text{LayerNorm}(y)\n",
    "\\]\n",
    "\n",
    "Bu iki adÄ±m sayesinde dikkat bloÄŸu, **Transformer Encoder/Decoder** yapÄ±larÄ±nda doÄŸrudan kullanÄ±labilir hale gelir.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95220ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8, dp: float = 0.1, causal: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.causal = causal\n",
    "        \n",
    "        # Tek lineer katman (Q, K, V birleÅŸik)\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout ve Normalization\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # --- Q, K, V projeksiyonu ---\n",
    "        qkv = self.qkv_proj(x)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # --- Head bÃ¶lÃ¼nmesi ---\n",
    "        def reshape_heads(t):\n",
    "            return t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        Q, K, V = map(reshape_heads, (Q, K, V))\n",
    "        \n",
    "        # --- Skor hesabÄ± ---\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # --- Causal mask ---\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # --- Attention aÄŸÄ±rlÄ±klarÄ± ---\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # --- AÄŸÄ±rlÄ±klÄ± toplama ---\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # --- Residual + Normalization ---\n",
    "        x = x + self.dropout(out)\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10, 512)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8, causal=True)\n",
    "y = mha(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c714c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
