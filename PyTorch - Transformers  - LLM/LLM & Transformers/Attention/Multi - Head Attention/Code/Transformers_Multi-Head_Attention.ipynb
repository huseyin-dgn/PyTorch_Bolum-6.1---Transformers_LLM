{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ccfbbb",
   "metadata": {},
   "source": [
    "## PyTorch ile Basit MultiHeadSelfAttention (Ba≈ülangƒ±√ß);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09a34c",
   "metadata": {},
   "source": [
    "* A≈üaƒüƒ±daki h√ºcre, teoride anlattƒ±ƒüƒ±mƒ±z adƒ±mlarƒ± takip eden, a√ßƒ±klamalƒ± ve test edilebilir bir PyTorch implementasyonudur. Bu s√ºr√ºm eƒüitim ve geli≈ütirme i√ßin okunaklƒ± ve geni≈ületilebilir tutuldu ‚Äî sonraki adƒ±mlarda performans optimizasyonu, flash attention, qkv tek matris haline getirme, rotary embedding, ya da causal masking i√ßin ilerleyeceƒüiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6c0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0da2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model : int ,num_heads:int=8 , dp:float = 0.3 ):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0 , \"d_model num_heads'e tam b√∂l√ºnmelidir\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.q_lin = nn.Linear(d_model , d_model)\n",
    "        self.v_lin = nn.Linear(d_model , d_model)\n",
    "        self.k_lin = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.out_lin = nn.Linear(d_model , d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self,x:torch.Tensor , mask:torch.Tensor = None):\n",
    "        b,n,d = x.size()\n",
    "        q = self.q_lin(x).view(b,n,self.num_heads , self.d_k).transpose(1,2)\n",
    "        k = self.k_lin(x).view(b,n,self.num_heads , self.d_k).transpose(1,2)\n",
    "        v = self.v_lin(x).view(b,n,self.num_heads , self.d_k).transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            elif mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill_(mask == 0 , float(\"-inf\"))\n",
    "        \n",
    "        attn= F.softmax(scores , dim= -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn,v)\n",
    "        out = out.transpose(1,2).contiguous().view(b,n,d)\n",
    "        out = self.out_lin(out)\n",
    "        return out , attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e6d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape = torch.Size([2, 5, 64])\n",
      "attn.shape = torch.Size([2, 8, 5, 5])\n",
      "masked attn sum row (first batch, first head): 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(0)\n",
    "    batch, seq_len, d_model = 2, 5, 64\n",
    "    x = torch.randn(batch, seq_len, d_model)\n",
    "    model = MultiHeadAttention(d_model=d_model, num_heads=8, dp=0.)\n",
    "    out, attn = model(x)\n",
    "    print('out.shape =', out.shape) # beklenen: (2,5,64)\n",
    "    print('attn.shape =', attn.shape) # beklenen: (2,8,5,5)\n",
    "\n",
    "\n",
    "# padding mask √∂rneƒüi\n",
    "    mask = torch.tensor([[1,1,1,0,0],[1,1,1,1,0]])\n",
    "    out_masked, attn_masked = model(x, mask=mask)\n",
    "    print('masked attn sum row (first batch, first head):', attn_masked[0,0,0].sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e5126",
   "metadata": {},
   "source": [
    "### ≈ûu an elimizde:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3103a8d",
   "metadata": {},
   "source": [
    "\n",
    "‚úÖ Ayrƒ± Q, K, V lineer katmanlarƒ± var.\n",
    "\n",
    "‚úÖ Dropout ve padding mask desteƒüi eklendi.\n",
    "\n",
    "‚úÖ Dikkat skorlarƒ± √∂l√ßekleniyor ve softmax sonrasƒ± normalize ediliyor.\n",
    "\n",
    "‚úÖ Multi-Head yapƒ±sƒ± dinamik ve testle doƒürulandƒ±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a2417",
   "metadata": {},
   "source": [
    "## üîç Multi-Head Attention ‚Äî Kod √ñzeti\n",
    "\n",
    "1. **Linear Projeksiyonlar:**  \n",
    "   Girdi (x) ‚Üí `q_proj`, `k_proj`, `v_proj` katmanlarƒ±yla sƒ±rasƒ±yla **Query (Q)**, **Key (K)**, **Value (V)** matrislerine d√∂n√º≈üt√ºr√ºl√ºr.\n",
    "\n",
    "2. **Head B√∂l√ºnmesi:**  \n",
    "   `embed_dim`, `num_heads`‚Äôe b√∂l√ºn√ºr ‚Üí her head‚Äôin boyutu `head_dim = embed_dim // num_heads` olur.  \n",
    "   Tensorlar `(batch, seq_len, num_heads, head_dim)` bi√ßimine getirilir.\n",
    "\n",
    "3. **Skor Hesabƒ±:**  \n",
    "   Her head i√ßin dikkat skorlarƒ± hesaplanƒ±r:  \n",
    "   \\[\n",
    "   \\text{scores} = \\frac{QK^T}{\\sqrt{head\\_dim}}\n",
    "   \\]\n",
    "   Daha sonra `Softmax` uygulanarak olasƒ±lƒ±k daƒüƒ±lƒ±mƒ± elde edilir.\n",
    "\n",
    "4. **Aƒüƒ±rlƒ±klƒ± Toplama:**  \n",
    "   Bu daƒüƒ±lƒ±m `V` ile √ßarpƒ±larak her head‚Äôin √ßƒ±ktƒ±sƒ± bulunur:\n",
    "   \\[\n",
    "   \\text{head\\_output} = \\text{Softmax(scores)} \\times V\n",
    "   \\]\n",
    "\n",
    "5. **Birle≈ütirme ve Lineer √áƒ±kƒ±≈ü:**  \n",
    "   T√ºm head √ßƒ±ktƒ±larƒ± `concat` edilir ve `out_proj` katmanƒ±yla tekrar `embed_dim` boyutuna d√∂n√º≈üt√ºr√ºl√ºr.\n",
    "\n",
    "6. **Opsiyonel Dropout / Norm:**  \n",
    "   Eƒüitim sƒ±rasƒ±nda a≈üƒ±rƒ± √∂ƒürenmeyi √∂nlemek i√ßin Dropout ve LayerNorm eklenebilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0178a9",
   "metadata": {},
   "source": [
    "---\n",
    "## ≈ûimdi bu attention mekanizmasƒ±nƒ± daha da ileri ta≈üƒ±yalƒ±m. ;\n",
    "* Q/K/V projeksiyonlarƒ±nƒ± tek lineer katmanda birle≈ütirme (W_qkv ‚Üí performans artƒ±≈üƒ±).\n",
    "\n",
    "* Causal mask (look-ahead) ekleyerek dil modelleme ve autoregressive g√∂revler i√ßin uygun hale getirme\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3ecf6",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Geli≈ütirme 1 ‚Äî Q/K/V'yi Tek Lineer Katmanda Birle≈ütirme\n",
    "\n",
    "Normalde `q_proj`, `k_proj`, `v_proj` olmak √ºzere 3 ayrƒ± `nn.Linear` kullanƒ±lƒ±r.  \n",
    "Ancak performans kazanmak i√ßin **tek bir b√ºy√ºk aƒüƒ±rlƒ±k matrisi (W_qkv)** ile bu i≈ülemler birle≈ütirilebilir.\n",
    "\n",
    "\\[\n",
    "[Q, K, V] = xW_{qkv}\n",
    "\\]\n",
    "\n",
    "Bu yakla≈üƒ±m:\n",
    "- Hesaplama maliyetini azaltƒ±r (tek matris √ßarpƒ±mƒ±),\n",
    "- GPU optimizasyonundan daha iyi faydalanƒ±r.\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è Geli≈ütirme 2 ‚Äî Causal Mask (Look-Ahead)\n",
    "\n",
    "Dil modelleme ve autoregressive g√∂revlerde, modelin **gelecekteki token‚Äôlarƒ± g√∂rmemesi** gerekir.  \n",
    "Bu ama√ßla **causal mask** (√º√ßgensel maske) uygulanƒ±r:\n",
    "\n",
    "\\[\n",
    "mask_{ij} = \n",
    "\\begin{cases}\n",
    "0, & \\text{if } j \\le i \\\\\n",
    "-\\infty, & \\text{if } j > i\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Softmax √∂ncesinde bu maske eklenir:\n",
    "\\[\n",
    "\\text{scores} = \\frac{QK^T}{\\sqrt{d_k}} + \\text{mask}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014fa138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8, dp: float = 0.1, causal: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.causal = causal\n",
    "        \n",
    "        # üîπ Q/K/V birle≈üik projeksiyon katmanƒ±\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1Ô∏è‚É£ Tek lineer katmandan Q, K, V √ºretimi\n",
    "        qkv = self.qkv_proj(x)  # (B, L, 3E)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)  # √ú√ß par√ßaya ayƒ±r\n",
    "        \n",
    "        # 2Ô∏è‚É£ Head b√∂l√ºnmesi\n",
    "        def reshape_heads(t):\n",
    "            return t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        Q, K, V = map(reshape_heads, (Q, K, V))\n",
    "        \n",
    "        # 3Ô∏è‚É£ Skor hesabƒ±\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # 4Ô∏è‚É£ Causal Mask ekleme (look-ahead)\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, V)  # (B, num_heads, L, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, E)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10, 512)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8, causal=True)\n",
    "y = mha(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89085901",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è A≈üama 3 ‚Äî Residual Baƒülantƒ± + Layer Normalization\n",
    "\n",
    "### üîπ Residual Connection\n",
    "Multi-Head Attention bloƒüu, giri≈ü bilgisini tamamen kaybetmesin diye:\n",
    "\\[\n",
    "y = x + \\text{Dropout}(\\text{Attention}(x))\n",
    "\\]\n",
    "≈üeklinde geri eklenir.  \n",
    "Bu, **gradyan akƒ±≈üƒ±nƒ± kolayla≈ütƒ±rƒ±r** ve **√∂ƒürenmeyi stabilize eder**.\n",
    "\n",
    "### üîπ Layer Normalization\n",
    "Residual toplama sonrasƒ±, modelin √ßƒ±ktƒ±sƒ±nƒ± normalize ederek daha kararlƒ± hale getirir:\n",
    "\\[\n",
    "\\text{output} = \\text{LayerNorm}(y)\n",
    "\\]\n",
    "\n",
    "Bu iki adƒ±m sayesinde dikkat bloƒüu, **Transformer Encoder/Decoder** yapƒ±larƒ±nda doƒürudan kullanƒ±labilir hale gelir.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95220ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8, dp: float = 0.1, causal: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.causal = causal\n",
    "        \n",
    "        # Tek lineer katman (Q, K, V birle≈üik)\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout ve Normalization\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # --- Q, K, V projeksiyonu ---\n",
    "        qkv = self.qkv_proj(x)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # --- Head b√∂l√ºnmesi ---\n",
    "        def reshape_heads(t):\n",
    "            return t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        Q, K, V = map(reshape_heads, (Q, K, V))\n",
    "        \n",
    "        # --- Skor hesabƒ± ---\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # --- Causal mask ---\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # --- Attention aƒüƒ±rlƒ±klarƒ± ---\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # --- Aƒüƒ±rlƒ±klƒ± toplama ---\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # --- Residual + Normalization ---\n",
    "        x = x + self.dropout(out)\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10, 512)\n",
    "mha = MultiHeadAttention(embed_dim=512, num_heads=8, causal=True)\n",
    "y = mha(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c714c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
