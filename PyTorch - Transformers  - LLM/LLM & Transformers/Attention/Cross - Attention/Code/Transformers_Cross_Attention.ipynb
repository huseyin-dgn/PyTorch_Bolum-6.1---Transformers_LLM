{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81f5f39",
   "metadata": {},
   "source": [
    "## Åimdi ilk adÄ±m olan temel tek-head Cross-Attentionâ€™Ä± kodlayalÄ±m ve adÄ±m adÄ±m aÃ§Ä±klayalÄ±m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a3f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 8])\n",
      "Attention map: torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Tek-head Cross-Attention\n",
    "    Q: decoder input â†’ (batch, seq_len_dec, d_k)\n",
    "    K: encoder output â†’ (batch, seq_len_enc, d_k)\n",
    "    V: encoder output â†’ (batch, seq_len_enc, d_v)\n",
    "    mask: opsiyonel (batch, seq_len_enc)\n",
    "    \"\"\"\n",
    "    # Skor hesaplama\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)\n",
    "    \n",
    "    # Mask uygulama,\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax â†’ aÄŸÄ±rlÄ±klÄ± toplam\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(attn, V)\n",
    "    return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_dec, L_enc, d_k, d_v = 2, 4, 6, 8, 8\n",
    "Q = torch.randn(B, L_dec, d_k)\n",
    "K = torch.randn(B, L_enc, d_k)\n",
    "V = torch.randn(B, L_enc, d_v)\n",
    "\n",
    "out, attn_map = cross_attention(Q, K, V)\n",
    "print(\"Output:\", out.shape)        # (B, L_dec, d_v)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, L_dec, L_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172af3cb",
   "metadata": {},
   "source": [
    "### Bu noktada Cross-Attention tek-head olarak Ã§alÄ±ÅŸÄ±yor ve mantÄ±ÄŸÄ±:\n",
    "\n",
    "* Decoder tokenâ€™larÄ± â†’ Q\n",
    "\n",
    "* Encoder tokenâ€™larÄ± â†’ K, V\n",
    "\n",
    "* Skor = QKáµ€ / âˆšd_k\n",
    "\n",
    "* Softmax + context = attn Ã— V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322db7a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9a3a1",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Multi-Head Cross-Attention\n",
    "\n",
    "### AmaÃ§\n",
    "- Tek bir head sadece bir tÃ¼r iliÅŸkiyi Ã¶ÄŸrenebilir.  \n",
    "- Multi-head ile farklÄ± â€œbakÄ±ÅŸ aÃ§Ä±larÄ±â€ paralel olarak Ã¶ÄŸrenilir.  \n",
    "- Decoder tokenâ€™larÄ± encoderâ€™daki farklÄ± temsillere aynÄ± anda dikkat edebilir.\n",
    "\n",
    "### AdÄ±mlar\n",
    "1. Q/K/V projeksiyonlarÄ± â†’ (B, L, E)  \n",
    "2. Headâ€™lere bÃ¶l â†’ (B, num_heads, L, head_dim)  \n",
    "3. Her head iÃ§in skor â†’ softmax â†’ aÄŸÄ±rlÄ±klÄ± toplam  \n",
    "4. Headâ€™leri birleÅŸtir (concat) â†’ lineer Ã§Ä±kÄ±ÅŸ  \n",
    "5. Opsiyonel: Dropout, Residual, LayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad17974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear projeksiyonlar\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout + LayerNorm\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x_dec, x_enc, mask_enc=None):\n",
    "        B, L_dec, E = x_dec.shape\n",
    "        L_enc = x_enc.size(1)\n",
    "        \n",
    "        # 1ï¸âƒ£ Q/K/V projeksiyonlarÄ±\n",
    "        Q = self.q_proj(x_dec).view(B, L_dec, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x_enc).view(B, L_enc, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x_enc).view(B, L_enc, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2ï¸âƒ£ Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Mask\n",
    "        if mask_enc is not None:\n",
    "            mask = mask_enc.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 3ï¸âƒ£ Softmax ve aÄŸÄ±rlÄ±klÄ± toplam\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)  # (B, num_heads, L_dec, head_dim)\n",
    "        \n",
    "        # 4ï¸âƒ£ Head birleÅŸtirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_dec, E)\n",
    "        out = self.layernorm(x_dec + self.dropout(self.out_proj(context)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_dec, L_enc, E = 2, 5, 8, 64\n",
    "x_dec = torch.randn(B, L_dec, E)\n",
    "x_enc = torch.randn(B, L_enc, E)\n",
    "mask_enc = torch.ones(B, L_enc)\n",
    "\n",
    "cross_attn = MultiHeadCrossAttention(E, num_heads=8)\n",
    "y, attn_map = cross_attn(x_dec, x_enc, mask_enc)\n",
    "print(\"Output:\", y.shape)        # (B, L_dec, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_dec, L_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5d298",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Bu noktada:\n",
    "\n",
    "* Decoder tokenâ€™larÄ±, encoder baÄŸlamÄ±nÄ± multi-head ile paralel olarak Ã¶ÄŸreniyor.\n",
    "\n",
    "* Residual + LayerNorm + Dropout eklenmiÅŸ durumda.\n",
    "\n",
    "* Padding mask desteÄŸi var."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a3599",
   "metadata": {},
   "source": [
    "----\n",
    "## ğŸ”¹ Ä°leri DÃ¼zey Cross-Attention (BaÄŸÄ±msÄ±z Blok)\n",
    "\n",
    "### Ã–zellikler:\n",
    "1. **Multi-Head:** Birden fazla head ile paralel dikkat\n",
    "2. **Residual + LayerNorm:** Stabil Ã¶ÄŸrenme\n",
    "3. **Dropout:** Overfittingâ€™i azaltma\n",
    "4. **Masking:** Padding veya look-ahead maskeleri\n",
    "5. **BaÄŸÄ±msÄ±z kullanÄ±m:** ArtÄ±k encoder-decoder zorunluluÄŸu yok, herhangi iki tensor arasÄ±nda Ã§alÄ±ÅŸabilir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a972b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdvancedCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout ve LayerNorm\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, Q_input, KV_input, mask=None, causal=False):\n",
    "        \"\"\"\n",
    "        Q_input: (B, L_q, E) â†’ sorgular\n",
    "        KV_input: (B, L_kv, E) â†’ anahtar ve deÄŸerler\n",
    "        mask: opsiyonel (B, L_kv)\n",
    "        causal: opsiyonel, look-ahead mask\n",
    "        \"\"\"\n",
    "        B, L_q, E = Q_input.shape\n",
    "        L_kv = KV_input.size(1)\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        Q = self.q_proj(Q_input).view(B, L_q, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Padding mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.ones(L_q, L_kv, device=Q_input.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax ve aÄŸÄ±rlÄ±klÄ± toplam\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleÅŸtirme ve Ã§Ä±kÄ±ÅŸ\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_q, E)\n",
    "        out = self.layernorm(Q_input + self.dropout(self.out_proj(context)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_q, L_kv, E = 2, 5, 7, 64\n",
    "Q_input = torch.randn(B, L_q, E)\n",
    "KV_input = torch.randn(B, L_kv, E)\n",
    "mask = torch.ones(B, L_kv)\n",
    "\n",
    "adv_cross_attn = AdvancedCrossAttention(E, num_heads=8)\n",
    "y, attn_map = adv_cross_attn(Q_input, KV_input, mask=mask, causal=True)\n",
    "\n",
    "print(\"Output:\", y.shape)        # (B, L_q, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_q, L_kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e403651",
   "metadata": {},
   "source": [
    "---\n",
    "### O zaman ÅŸimdi AdvancedCrossAttentionâ€™Ä± bir adÄ±m daha ileriye taÅŸÄ±yalÄ±m:\n",
    "\n",
    "## Hedefler:\n",
    "\n",
    "* Dynamic Q/K/V boyutlarÄ±: Girdiler farklÄ± embedding boyutlarÄ±na sahip olabilir, otomatik uyarlansÄ±n.\n",
    "\n",
    "* Opsiyonel Fusion / Gating MekanizmasÄ±: Q ve context Ã§Ä±ktÄ±sÄ± birleÅŸtirilirken geliÅŸmiÅŸ bir fÃ¼zyon ile daha zengin temsil saÄŸlansÄ±n.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62440404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DynamicFusionCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim_q, embed_dim_kv, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        # Multi-head parametreleri\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim_q // num_heads\n",
    "        \n",
    "        # Linear projeksiyonlar\n",
    "        self.q_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        self.k_proj = nn.Linear(embed_dim_kv, embed_dim_q)  # farklÄ± embed_dim support\n",
    "        self.v_proj = nn.Linear(embed_dim_kv, embed_dim_q)\n",
    "        self.out_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        # Gating mekanizmasÄ±\n",
    "        self.gate_proj = nn.Linear(2*embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim_q)\n",
    "        \n",
    "    def forward(self, Q_input, KV_input, mask=None, causal=False):\n",
    "        B, L_q, E_q = Q_input.shape\n",
    "        L_kv = KV_input.size(1)\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        Q = self.q_proj(Q_input).view(B, L_q, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.ones(L_q, L_kv, device=Q_input.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax ve context\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleÅŸtirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_q, E_q)\n",
    "        \n",
    "        # Fusion / Gating\n",
    "        fusion_input = torch.cat([context, Q_input], dim=-1)\n",
    "        gate = torch.sigmoid(self.gate_proj(fusion_input))\n",
    "        fused_out = gate * context + (1 - gate) * Q_input\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(Q_input + self.dropout(self.out_proj(fused_out)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_q, L_kv, E_q, E_kv = 2, 5, 7, 64, 80\n",
    "Q_input = torch.randn(B, L_q, E_q)\n",
    "KV_input = torch.randn(B, L_kv, E_kv)\n",
    "mask = torch.ones(B, L_kv)\n",
    "\n",
    "dyn_cross_attn = DynamicFusionCrossAttention(E_q, E_kv, num_heads=8)\n",
    "y, attn_map = dyn_cross_attn(Q_input, KV_input, mask=mask, causal=True)\n",
    "\n",
    "print(\"Output:\", y.shape)        # (B, L_q, E_q)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_q, L_kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da286f80",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Avantajlar:\n",
    "\n",
    "* Q/K/V farklÄ± embedding boyutlarÄ±yla Ã§alÄ±ÅŸabilir.\n",
    "\n",
    "* Gating mekanizmasÄ± sayesinde context ve orijinal Q optimal ÅŸekilde birleÅŸir.\n",
    "\n",
    "- Padding ve causal mask desteÄŸi mevcut.\n",
    "\n",
    "- Multi-head + residual + layernorm + dropout korunuyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf763fd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77155c0",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Ultimate Dynamic Cross-Attention\n",
    "\n",
    "### Ã–zellikler\n",
    "- Herhangi bir batch, sequence uzunluÄŸu ve embedding boyutunda Ã§alÄ±ÅŸabilir.  \n",
    "- Multi-head ile paralel dikkat.  \n",
    "- Mask ve causal desteÄŸi.  \n",
    "- Opsiyonel gating/fusion ile daha zengin temsil.  \n",
    "- Residual + LayerNorm + Dropout ile stabil Ã¶ÄŸrenme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f91a8d",
   "metadata": {},
   "source": [
    "## Hedefler:\n",
    "\n",
    "* Dynamic sequence lengths: Girdi boyutlarÄ± deÄŸiÅŸken olabilir (L_q ve L_kv).\n",
    "\n",
    "* Dynamic head / embed_dim: Head sayÄ±sÄ± ve embedding boyutu parametre olarak esnek.\n",
    "\n",
    "* Mask ve causal seÃ§enekleri: Padding ve look-ahead maskeleri otomatik uygulanabilir.\n",
    "\n",
    "* Gating / Fusion opsiyonel: Ä°stenirse context ve Q fusion kullanÄ±labilir.\n",
    "\n",
    "* Residual + LayerNorm + Dropout dahil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a3d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UltimateCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim_q, embed_dim_kv, num_heads=8, dp=0.1, use_gate=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim_q // num_heads\n",
    "        self.use_gate = use_gate\n",
    "        \n",
    "        # Projeksiyonlar\n",
    "        self.q_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        self.k_proj = nn.Linear(embed_dim_kv, embed_dim_q)\n",
    "        self.v_proj = nn.Linear(embed_dim_kv, embed_dim_q)\n",
    "        self.out_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        # Gating mekanizmasÄ± opsiyonel\n",
    "        if use_gate:\n",
    "            self.gate_proj = nn.Linear(2*embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim_q)\n",
    "    \n",
    "    def forward(self, Q_input, KV_input, mask=None, causal=False):\n",
    "        B, L_q, E_q = Q_input.shape\n",
    "        L_kv = KV_input.size(1)\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ± ve head bÃ¶lÃ¼nmesi\n",
    "        Q = self.q_proj(Q_input).view(B, L_q, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Masking\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.ones(L_q, L_kv, device=Q_input.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax ve context\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleÅŸtirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_q, E_q)\n",
    "        \n",
    "        # Fusion / Gating\n",
    "        if self.use_gate:\n",
    "            fusion_input = torch.cat([context, Q_input], dim=-1)\n",
    "            gate = torch.sigmoid(self.gate_proj(fusion_input))\n",
    "            fused_out = gate * context + (1 - gate) * Q_input\n",
    "        else:\n",
    "            fused_out = context\n",
    "        \n",
    "        # Residual + LayerNorm + Dropout\n",
    "        out = self.layernorm(Q_input + self.dropout(self.out_proj(fused_out)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_q, L_kv, E_q, E_kv = 2, 5, 7, 64, 80\n",
    "Q_input = torch.randn(B, L_q, E_q)\n",
    "KV_input = torch.randn(B, L_kv, E_kv)\n",
    "mask = torch.ones(B, L_kv)\n",
    "\n",
    "ultimate_attn = UltimateCrossAttention(E_q, E_kv, num_heads=8, use_gate=True)\n",
    "y, attn_map = ultimate_attn(Q_input, KV_input, mask=mask, causal=True)\n",
    "\n",
    "print(\"Output:\", y.shape)        # (B, L_q, E_q)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_q, L_kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71325282",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Ã–zellikler:\n",
    "\n",
    "* Dinamik sequence uzunluÄŸu ve embedding boyutu\n",
    "\n",
    "* Multi-head paralel dikkat\n",
    "\n",
    "* Padding ve causal mask desteÄŸi\n",
    "\n",
    "* Opsiyonel gating/fusion ile daha zengin baÄŸlam\n",
    "\n",
    "* Residual + LayerNorm + Dropout ile stabil Ã¶ÄŸrenme"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
