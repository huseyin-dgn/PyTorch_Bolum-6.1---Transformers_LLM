{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81f5f39",
   "metadata": {},
   "source": [
    "## Şimdi ilk adım olan temel tek-head Cross-Attention’ı kodlayalım ve adım adım açıklayalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a3f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 8])\n",
      "Attention map: torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Tek-head Cross-Attention\n",
    "    Q: decoder input → (batch, seq_len_dec, d_k)\n",
    "    K: encoder output → (batch, seq_len_enc, d_k)\n",
    "    V: encoder output → (batch, seq_len_enc, d_v)\n",
    "    mask: opsiyonel (batch, seq_len_enc)\n",
    "    \"\"\"\n",
    "    # Skor hesaplama\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)\n",
    "    \n",
    "    # Mask uygulama,\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax → ağırlıklı toplam\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(attn, V)\n",
    "    return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_dec, L_enc, d_k, d_v = 2, 4, 6, 8, 8\n",
    "Q = torch.randn(B, L_dec, d_k)\n",
    "K = torch.randn(B, L_enc, d_k)\n",
    "V = torch.randn(B, L_enc, d_v)\n",
    "\n",
    "out, attn_map = cross_attention(Q, K, V)\n",
    "print(\"Output:\", out.shape)        # (B, L_dec, d_v)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, L_dec, L_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172af3cb",
   "metadata": {},
   "source": [
    "### Bu noktada Cross-Attention tek-head olarak çalışıyor ve mantığı:\n",
    "\n",
    "* Decoder token’ları → Q\n",
    "\n",
    "* Encoder token’ları → K, V\n",
    "\n",
    "* Skor = QKᵀ / √d_k\n",
    "\n",
    "* Softmax + context = attn × V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322db7a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9a3a1",
   "metadata": {},
   "source": [
    "## 🔹 Multi-Head Cross-Attention\n",
    "\n",
    "### Amaç\n",
    "- Tek bir head sadece bir tür ilişkiyi öğrenebilir.  \n",
    "- Multi-head ile farklı “bakış açıları” paralel olarak öğrenilir.  \n",
    "- Decoder token’ları encoder’daki farklı temsillere aynı anda dikkat edebilir.\n",
    "\n",
    "### Adımlar\n",
    "1. Q/K/V projeksiyonları → (B, L, E)  \n",
    "2. Head’lere böl → (B, num_heads, L, head_dim)  \n",
    "3. Her head için skor → softmax → ağırlıklı toplam  \n",
    "4. Head’leri birleştir (concat) → lineer çıkış  \n",
    "5. Opsiyonel: Dropout, Residual, LayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad17974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear projeksiyonlar\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout + LayerNorm\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x_dec, x_enc, mask_enc=None):\n",
    "        B, L_dec, E = x_dec.shape\n",
    "        L_enc = x_enc.size(1)\n",
    "        \n",
    "        # 1️⃣ Q/K/V projeksiyonları\n",
    "        Q = self.q_proj(x_dec).view(B, L_dec, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x_enc).view(B, L_enc, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x_enc).view(B, L_enc, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2️⃣ Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Mask\n",
    "        if mask_enc is not None:\n",
    "            mask = mask_enc.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 3️⃣ Softmax ve ağırlıklı toplam\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)  # (B, num_heads, L_dec, head_dim)\n",
    "        \n",
    "        # 4️⃣ Head birleştirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_dec, E)\n",
    "        out = self.layernorm(x_dec + self.dropout(self.out_proj(context)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_dec, L_enc, E = 2, 5, 8, 64\n",
    "x_dec = torch.randn(B, L_dec, E)\n",
    "x_enc = torch.randn(B, L_enc, E)\n",
    "mask_enc = torch.ones(B, L_enc)\n",
    "\n",
    "cross_attn = MultiHeadCrossAttention(E, num_heads=8)\n",
    "y, attn_map = cross_attn(x_dec, x_enc, mask_enc)\n",
    "print(\"Output:\", y.shape)        # (B, L_dec, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_dec, L_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5d298",
   "metadata": {},
   "source": [
    "### 💡 Bu noktada:\n",
    "\n",
    "* Decoder token’ları, encoder bağlamını multi-head ile paralel olarak öğreniyor.\n",
    "\n",
    "* Residual + LayerNorm + Dropout eklenmiş durumda.\n",
    "\n",
    "* Padding mask desteği var."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a3599",
   "metadata": {},
   "source": [
    "----\n",
    "## 🔹 İleri Düzey Cross-Attention (Bağımsız Blok)\n",
    "\n",
    "### Özellikler:\n",
    "1. **Multi-Head:** Birden fazla head ile paralel dikkat\n",
    "2. **Residual + LayerNorm:** Stabil öğrenme\n",
    "3. **Dropout:** Overfitting’i azaltma\n",
    "4. **Masking:** Padding veya look-ahead maskeleri\n",
    "5. **Bağımsız kullanım:** Artık encoder-decoder zorunluluğu yok, herhangi iki tensor arasında çalışabilir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a972b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdvancedCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout ve LayerNorm\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, Q_input, KV_input, mask=None, causal=False):\n",
    "        \"\"\"\n",
    "        Q_input: (B, L_q, E) → sorgular\n",
    "        KV_input: (B, L_kv, E) → anahtar ve değerler\n",
    "        mask: opsiyonel (B, L_kv)\n",
    "        causal: opsiyonel, look-ahead mask\n",
    "        \"\"\"\n",
    "        B, L_q, E = Q_input.shape\n",
    "        L_kv = KV_input.size(1)\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        Q = self.q_proj(Q_input).view(B, L_q, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Padding mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.ones(L_q, L_kv, device=Q_input.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax ve ağırlıklı toplam\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleştirme ve çıkış\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_q, E)\n",
    "        out = self.layernorm(Q_input + self.dropout(self.out_proj(context)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_q, L_kv, E = 2, 5, 7, 64\n",
    "Q_input = torch.randn(B, L_q, E)\n",
    "KV_input = torch.randn(B, L_kv, E)\n",
    "mask = torch.ones(B, L_kv)\n",
    "\n",
    "adv_cross_attn = AdvancedCrossAttention(E, num_heads=8)\n",
    "y, attn_map = adv_cross_attn(Q_input, KV_input, mask=mask, causal=True)\n",
    "\n",
    "print(\"Output:\", y.shape)        # (B, L_q, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_q, L_kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e403651",
   "metadata": {},
   "source": [
    "---\n",
    "### O zaman şimdi AdvancedCrossAttention’ı bir adım daha ileriye taşıyalım:\n",
    "\n",
    "## Hedefler:\n",
    "\n",
    "* Dynamic Q/K/V boyutları: Girdiler farklı embedding boyutlarına sahip olabilir, otomatik uyarlansın.\n",
    "\n",
    "* Opsiyonel Fusion / Gating Mekanizması: Q ve context çıktısı birleştirilirken gelişmiş bir füzyon ile daha zengin temsil sağlansın.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62440404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DynamicFusionCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim_q, embed_dim_kv, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        # Multi-head parametreleri\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim_q // num_heads\n",
    "        \n",
    "        # Linear projeksiyonlar\n",
    "        self.q_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        self.k_proj = nn.Linear(embed_dim_kv, embed_dim_q)  # farklı embed_dim support\n",
    "        self.v_proj = nn.Linear(embed_dim_kv, embed_dim_q)\n",
    "        self.out_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        # Gating mekanizması\n",
    "        self.gate_proj = nn.Linear(2*embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim_q)\n",
    "        \n",
    "    def forward(self, Q_input, KV_input, mask=None, causal=False):\n",
    "        B, L_q, E_q = Q_input.shape\n",
    "        L_kv = KV_input.size(1)\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        Q = self.q_proj(Q_input).view(B, L_q, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.ones(L_q, L_kv, device=Q_input.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax ve context\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleştirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_q, E_q)\n",
    "        \n",
    "        # Fusion / Gating\n",
    "        fusion_input = torch.cat([context, Q_input], dim=-1)\n",
    "        gate = torch.sigmoid(self.gate_proj(fusion_input))\n",
    "        fused_out = gate * context + (1 - gate) * Q_input\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(Q_input + self.dropout(self.out_proj(fused_out)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_q, L_kv, E_q, E_kv = 2, 5, 7, 64, 80\n",
    "Q_input = torch.randn(B, L_q, E_q)\n",
    "KV_input = torch.randn(B, L_kv, E_kv)\n",
    "mask = torch.ones(B, L_kv)\n",
    "\n",
    "dyn_cross_attn = DynamicFusionCrossAttention(E_q, E_kv, num_heads=8)\n",
    "y, attn_map = dyn_cross_attn(Q_input, KV_input, mask=mask, causal=True)\n",
    "\n",
    "print(\"Output:\", y.shape)        # (B, L_q, E_q)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_q, L_kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da286f80",
   "metadata": {},
   "source": [
    "## 💡 Avantajlar:\n",
    "\n",
    "* Q/K/V farklı embedding boyutlarıyla çalışabilir.\n",
    "\n",
    "* Gating mekanizması sayesinde context ve orijinal Q optimal şekilde birleşir.\n",
    "\n",
    "- Padding ve causal mask desteği mevcut.\n",
    "\n",
    "- Multi-head + residual + layernorm + dropout korunuyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf763fd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77155c0",
   "metadata": {},
   "source": [
    "## 🔹 Ultimate Dynamic Cross-Attention\n",
    "\n",
    "### Özellikler\n",
    "- Herhangi bir batch, sequence uzunluğu ve embedding boyutunda çalışabilir.  \n",
    "- Multi-head ile paralel dikkat.  \n",
    "- Mask ve causal desteği.  \n",
    "- Opsiyonel gating/fusion ile daha zengin temsil.  \n",
    "- Residual + LayerNorm + Dropout ile stabil öğrenme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f91a8d",
   "metadata": {},
   "source": [
    "## Hedefler:\n",
    "\n",
    "* Dynamic sequence lengths: Girdi boyutları değişken olabilir (L_q ve L_kv).\n",
    "\n",
    "* Dynamic head / embed_dim: Head sayısı ve embedding boyutu parametre olarak esnek.\n",
    "\n",
    "* Mask ve causal seçenekleri: Padding ve look-ahead maskeleri otomatik uygulanabilir.\n",
    "\n",
    "* Gating / Fusion opsiyonel: İstenirse context ve Q fusion kullanılabilir.\n",
    "\n",
    "* Residual + LayerNorm + Dropout dahil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a3d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 5, 64])\n",
      "Attention map: torch.Size([2, 8, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UltimateCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim_q, embed_dim_kv, num_heads=8, dp=0.1, use_gate=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim_q // num_heads\n",
    "        self.use_gate = use_gate\n",
    "        \n",
    "        # Projeksiyonlar\n",
    "        self.q_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        self.k_proj = nn.Linear(embed_dim_kv, embed_dim_q)\n",
    "        self.v_proj = nn.Linear(embed_dim_kv, embed_dim_q)\n",
    "        self.out_proj = nn.Linear(embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        # Gating mekanizması opsiyonel\n",
    "        if use_gate:\n",
    "            self.gate_proj = nn.Linear(2*embed_dim_q, embed_dim_q)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim_q)\n",
    "    \n",
    "    def forward(self, Q_input, KV_input, mask=None, causal=False):\n",
    "        B, L_q, E_q = Q_input.shape\n",
    "        L_kv = KV_input.size(1)\n",
    "        \n",
    "        # Q/K/V projeksiyonları ve head bölünmesi\n",
    "        Q = self.q_proj(Q_input).view(B, L_q, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(KV_input).view(B, L_kv, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Masking\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        if causal:\n",
    "            causal_mask = torch.triu(torch.ones(L_q, L_kv, device=Q_input.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax ve context\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleştirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L_q, E_q)\n",
    "        \n",
    "        # Fusion / Gating\n",
    "        if self.use_gate:\n",
    "            fusion_input = torch.cat([context, Q_input], dim=-1)\n",
    "            gate = torch.sigmoid(self.gate_proj(fusion_input))\n",
    "            fused_out = gate * context + (1 - gate) * Q_input\n",
    "        else:\n",
    "            fused_out = context\n",
    "        \n",
    "        # Residual + LayerNorm + Dropout\n",
    "        out = self.layernorm(Q_input + self.dropout(self.out_proj(fused_out)))\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L_q, L_kv, E_q, E_kv = 2, 5, 7, 64, 80\n",
    "Q_input = torch.randn(B, L_q, E_q)\n",
    "KV_input = torch.randn(B, L_kv, E_kv)\n",
    "mask = torch.ones(B, L_kv)\n",
    "\n",
    "ultimate_attn = UltimateCrossAttention(E_q, E_kv, num_heads=8, use_gate=True)\n",
    "y, attn_map = ultimate_attn(Q_input, KV_input, mask=mask, causal=True)\n",
    "\n",
    "print(\"Output:\", y.shape)        # (B, L_q, E_q)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L_q, L_kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71325282",
   "metadata": {},
   "source": [
    "### 💡 Özellikler:\n",
    "\n",
    "* Dinamik sequence uzunluğu ve embedding boyutu\n",
    "\n",
    "* Multi-head paralel dikkat\n",
    "\n",
    "* Padding ve causal mask desteği\n",
    "\n",
    "* Opsiyonel gating/fusion ile daha zengin bağlam\n",
    "\n",
    "* Residual + LayerNorm + Dropout ile stabil öğrenme"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
