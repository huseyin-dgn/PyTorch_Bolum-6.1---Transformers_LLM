{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af15ac2",
   "metadata": {},
   "source": [
    "### Mini Multi-Head Dot-Product Attention’dan başlıyoruz.\n",
    "\n",
    "Bu küçük versiyon:\n",
    "\n",
    "* Multi-head ama minimal parametre ile\n",
    "\n",
    "* Residual, LayerNorm ve dropout yok\n",
    "\n",
    "* Mask ve causal opsiyonel değil, sadece temel mantığı gösteriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1️⃣ Q/K/V projeksiyonları ve head bölünmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2️⃣ Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 3️⃣ Ağırlıklı toplam\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # 4️⃣ Head birleştirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "\n",
    "mini_mha = MiniMultiHeadAttention(E, num_heads=2)\n",
    "y, attn_map = mini_mha(x)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f73624",
   "metadata": {},
   "source": [
    "### Bu mini versiyon:\n",
    "\n",
    "* Minimal multi-head attention mantığını gösteriyor\n",
    "\n",
    "* Her head kendi attention’ını öğreniyor\n",
    "\n",
    "* Henüz residual, layernorm, dropout veya mask yok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d1626",
   "metadata": {},
   "source": [
    "---\n",
    "## İlk olarak mini Multi-Head Attention’ı residual ve layernorm ile güçlendireceğiz.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3a9cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Residual(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor ve ağırlıklı toplam\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleştirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "\n",
    "mha_res = MiniMHA_Residual(E, num_heads=2)\n",
    "y, attn_map = mha_res(x)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cccd53",
   "metadata": {},
   "source": [
    "## 💡 Açıklama:\n",
    "\n",
    "* Residual: Girdiyi çıktıya ekleyerek öğrenmeyi kolaylaştırır\n",
    "\n",
    "* LayerNorm: Stabil ve hızlı öğrenme sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61eafa",
   "metadata": {},
   "source": [
    "---\n",
    "##  Adım — Dropout ve Padding Mask\n",
    "\n",
    "### 🔹 Amaç\n",
    "- **Dropout:** Overfitting’i azaltmak için attention ağırlıklarını ve çıkışı rastgele sıfırlamak.  \n",
    "- **Padding Mask:** Seq2Seq veya değişken uzunluklu dizilerde geçersiz token’lara dikkat edilmemesi.\n",
    "\n",
    "### 🔹 Mantık\n",
    "1. **Scores Hesaplama:**  \n",
    "\\[\n",
    "\\text{scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "\\]\n",
    "\n",
    "2. **Mask Uygulama:**  \n",
    "Padding token’ları `-inf` ile softmax’ten önce devre dışı bırakılır.\n",
    "\n",
    "3. **Softmax → attn:**  \n",
    "Skorlar normalize edilir.\n",
    "\n",
    "4. **Dropout:**  \n",
    "- attn üzerinde  \n",
    "- final output üzerinde  \n",
    "overfitting’i azaltmak için uygulanır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbcdb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Residual_Dropout(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout + LayerNorm\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1️⃣ Q/K/V projeksiyonları ve head bölünmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2️⃣ Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # 3️⃣ Padding mask\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(1)  # (B,1,1,L)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        \n",
    "        # 4️⃣ Softmax ve dropout\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # 5️⃣ Ağırlıklı toplam ve head birleştirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # 6️⃣ Residual + LayerNorm\n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0]])  # padding mask\n",
    "\n",
    "mha_dp = MiniMHA_Residual_Dropout(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_dp(x, mask=mask)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169a726",
   "metadata": {},
   "source": [
    "## Multi-Head Dot-Product Attention + Residual + LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5521f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DotProductMHA_Residual(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1️⃣ Q/K/V projeksiyonları ve head bölünmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2️⃣ Skor hesaplama (Dot-Product)\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 3️⃣ Ağırlıklı toplam ve head birleştirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        \n",
    "        # 4️⃣ Residual + LayerNorm\n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "\n",
    "dot_mha = DotProductMHA_Residual(E, num_heads=2)\n",
    "y, attn_map = dot_mha(x)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f518195",
   "metadata": {},
   "source": [
    "##  Adım — Causal / Look-Ahead Mask ve Autoregressive Destek\n",
    "\n",
    "### 🔹 Amaç\n",
    "- Autoregressive görevlerde her token yalnızca kendisinden önceki token’lara bakabilir.  \n",
    "- Gelecek token’lar maskelenir (`-inf` ile softmax öncesi).  \n",
    "- Böylece dil modelleme ve seq2seq görevleri için uygun hale gelir.\n",
    "\n",
    "### 🔹 Mantık\n",
    "1. Q/K/V projeksiyonları → head’lere bölünür  \n",
    "2. Skor:  \n",
    "\\[\n",
    "\\text{scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "\\]\n",
    "3. **Padding Mask** uygulanır (opsiyonel)  \n",
    "4. **Causal Mask** uygulanır  \n",
    "5. Softmax → attn  \n",
    "6. Dropout → attn ve output  \n",
    "7. Head’ler birleştirilir  \n",
    "8. Residual + LayerNorm → son çıktı\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce28f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class DotProductMHA_Causal(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, L, E = x.shape\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Padding mask\n",
    "        if padding_mask is not None:\n",
    "            mask_exp = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = torch.tril(torch.ones(L, L, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "padding_mask = torch.tensor([[1,1,1,0],[1,1,0,0]])\n",
    "mha_causal = DotProductMHA_Causal(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_causal(x, padding_mask=padding_mask)\n",
    "print(\"Output:\", y.shape)\n",
    "print(\"Attention map:\", attn_map.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb9b6f",
   "metadata": {},
   "source": [
    "##  Adım — Opsiyonel Gating / Fusion\n",
    "\n",
    "### 🔹 Amaç\n",
    "- Attention çıktısı ile orijinal input’u akıllıca birleştirmek.  \n",
    "- Model, hangi bilgiyi daha çok kullanacağını öğrenir.  \n",
    "- Özellikle çok katmanlı veya encoder-decoder yapılarında faydalıdır.\n",
    "\n",
    "### 🔹 Mantık\n",
    "1. Attention çıktısı `context` ve orijinal input `x` alınır  \n",
    "2. Gating ağı (sigmoid aktivasyonlu) ile hangi kısımların kullanılacağı belirlenir:\n",
    "\n",
    "\\[\n",
    "g = \\sigma(W_g [context; x])\n",
    "\\]\n",
    "\n",
    "3. Fusion uygulanır:\n",
    "\n",
    "\\[\n",
    "y = g * context + (1 - g) * x\n",
    "\\]\n",
    "\n",
    "4. Residual + LayerNorm uygulanır (opsiyonel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd5edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Gating(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm ve Dropout\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        \n",
    "        # Gating ağı\n",
    "        self.gate = nn.Linear(embed_dim*2, embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # Q/K/V projeksiyonları ve head bölünmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor ve attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Weighted sum ve head birleştirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        context = self.out_proj(context)\n",
    "        context = self.dropout(context)\n",
    "        \n",
    "        # Gating / Fusion\n",
    "        combined = torch.cat([context, x], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(combined))\n",
    "        out = g * context + (1 - g) * x\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0]])\n",
    "\n",
    "mha_gating = MiniMHA_Gating(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_gating(x, mask=mask)\n",
    "\n",
    "print(\"Output:\", y.shape)         # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620f0d0",
   "metadata": {},
   "source": [
    "## 💡 Açıklama:\n",
    "\n",
    "* context ve x concat edilip gating ağı ile fusion uygulanıyor\n",
    "\n",
    "* Sigmoid ile hangi bilgi ne kadar kullanılacak öğreniliyor\n",
    "\n",
    "* Dropout + LayerNorm ile stabil ve overfitting’e karşı dayanıklı hâle getiriliyor\n",
    "\n",
    "* Artık Mini Multi-Head Dot-Product Attention tam esnek ve güçlü hâlde 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cdc5a4",
   "metadata": {},
   "source": [
    "## Mini Multi-Head Dot-Product Attention + Causal Mask + Padding Mask + Gating/Fusion kombinasyonunu birleştirip tam üretime hazır hâle getirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb80de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Full(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonları\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm ve Dropout\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        \n",
    "        # Gating ağı\n",
    "        self.gate = nn.Linear(embed_dim*2, embed_dim)\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # Q/K/V projeksiyonları ve head bölünmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor ve attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Padding mask\n",
    "        if padding_mask is not None:\n",
    "            mask_exp = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = torch.tril(torch.ones(L, L, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax ve dropout\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Weighted sum ve head birleştirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        context = self.out_proj(context)\n",
    "        context = self.dropout(context)\n",
    "        \n",
    "        # Gating / Fusion\n",
    "        combined = torch.cat([context, x], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(combined))\n",
    "        out = g * context + (1 - g) * x\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "padding_mask = torch.tensor([[1,1,1,0],[1,1,0,0]])\n",
    "\n",
    "mha_full = MiniMHA_Full(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_full(x, padding_mask=padding_mask)\n",
    "\n",
    "print(\"Output:\", y.shape)         # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5778d",
   "metadata": {},
   "source": [
    "## 💡 Açıklama:\n",
    "\n",
    "* Q/K/V projeksiyonları → Multi-head Dot-Product Attention\n",
    "\n",
    "* Padding Mask → geçersiz token’ları devre dışı bırakır\n",
    "\n",
    "* Causal Mask → autoregressive görevler için geleceğe bakışı engeller\n",
    "\n",
    "* Dropout → attn ve output üzerinde overfitting’i önler\n",
    "\n",
    "* Gating / Fusion → context ve x akıllıca birleştirilir\n",
    "\n",
    "* Residual + LayerNorm → stabil ve hızlı öğrenme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6884041",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
