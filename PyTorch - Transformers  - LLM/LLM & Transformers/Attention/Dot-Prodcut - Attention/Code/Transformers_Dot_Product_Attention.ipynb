{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af15ac2",
   "metadata": {},
   "source": [
    "### Mini Multi-Head Dot-Product Attentionâ€™dan baÅŸlÄ±yoruz.\n",
    "\n",
    "Bu kÃ¼Ã§Ã¼k versiyon:\n",
    "\n",
    "* Multi-head ama minimal parametre ile\n",
    "\n",
    "* Residual, LayerNorm ve dropout yok\n",
    "\n",
    "* Mask ve causal opsiyonel deÄŸil, sadece temel mantÄ±ÄŸÄ± gÃ¶steriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1ï¸âƒ£ Q/K/V projeksiyonlarÄ± ve head bÃ¶lÃ¼nmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2ï¸âƒ£ Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 3ï¸âƒ£ AÄŸÄ±rlÄ±klÄ± toplam\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # 4ï¸âƒ£ Head birleÅŸtirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "\n",
    "mini_mha = MiniMultiHeadAttention(E, num_heads=2)\n",
    "y, attn_map = mini_mha(x)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f73624",
   "metadata": {},
   "source": [
    "### Bu mini versiyon:\n",
    "\n",
    "* Minimal multi-head attention mantÄ±ÄŸÄ±nÄ± gÃ¶steriyor\n",
    "\n",
    "* Her head kendi attentionâ€™Ä±nÄ± Ã¶ÄŸreniyor\n",
    "\n",
    "* HenÃ¼z residual, layernorm, dropout veya mask yok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d1626",
   "metadata": {},
   "source": [
    "---\n",
    "## Ä°lk olarak mini Multi-Head Attentionâ€™Ä± residual ve layernorm ile gÃ¼Ã§lendireceÄŸiz.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3a9cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Residual(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor ve aÄŸÄ±rlÄ±klÄ± toplam\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Head birleÅŸtirme\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "\n",
    "mha_res = MiniMHA_Residual(E, num_heads=2)\n",
    "y, attn_map = mha_res(x)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cccd53",
   "metadata": {},
   "source": [
    "## ğŸ’¡ AÃ§Ä±klama:\n",
    "\n",
    "* Residual: Girdiyi Ã§Ä±ktÄ±ya ekleyerek Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r\n",
    "\n",
    "* LayerNorm: Stabil ve hÄ±zlÄ± Ã¶ÄŸrenme saÄŸlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61eafa",
   "metadata": {},
   "source": [
    "---\n",
    "##  AdÄ±m â€” Dropout ve Padding Mask\n",
    "\n",
    "### ğŸ”¹ AmaÃ§\n",
    "- **Dropout:** Overfittingâ€™i azaltmak iÃ§in attention aÄŸÄ±rlÄ±klarÄ±nÄ± ve Ã§Ä±kÄ±ÅŸÄ± rastgele sÄ±fÄ±rlamak.  \n",
    "- **Padding Mask:** Seq2Seq veya deÄŸiÅŸken uzunluklu dizilerde geÃ§ersiz tokenâ€™lara dikkat edilmemesi.\n",
    "\n",
    "### ğŸ”¹ MantÄ±k\n",
    "1. **Scores Hesaplama:**  \n",
    "\\[\n",
    "\\text{scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "\\]\n",
    "\n",
    "2. **Mask Uygulama:**  \n",
    "Padding tokenâ€™larÄ± `-inf` ile softmaxâ€™ten Ã¶nce devre dÄ±ÅŸÄ± bÄ±rakÄ±lÄ±r.\n",
    "\n",
    "3. **Softmax â†’ attn:**  \n",
    "Skorlar normalize edilir.\n",
    "\n",
    "4. **Dropout:**  \n",
    "- attn Ã¼zerinde  \n",
    "- final output Ã¼zerinde  \n",
    "overfittingâ€™i azaltmak iÃ§in uygulanÄ±r.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbcdb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Residual_Dropout(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout + LayerNorm\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1ï¸âƒ£ Q/K/V projeksiyonlarÄ± ve head bÃ¶lÃ¼nmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2ï¸âƒ£ Skor hesaplama\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # 3ï¸âƒ£ Padding mask\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(1)  # (B,1,1,L)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        \n",
    "        # 4ï¸âƒ£ Softmax ve dropout\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # 5ï¸âƒ£ AÄŸÄ±rlÄ±klÄ± toplam ve head birleÅŸtirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # 6ï¸âƒ£ Residual + LayerNorm\n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0]])  # padding mask\n",
    "\n",
    "mha_dp = MiniMHA_Residual_Dropout(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_dp(x, mask=mask)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169a726",
   "metadata": {},
   "source": [
    "## Multi-Head Dot-Product Attention + Residual + LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5521f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DotProductMHA_Residual(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # 1ï¸âƒ£ Q/K/V projeksiyonlarÄ± ve head bÃ¶lÃ¼nmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # 2ï¸âƒ£ Skor hesaplama (Dot-Product)\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 3ï¸âƒ£ AÄŸÄ±rlÄ±klÄ± toplam ve head birleÅŸtirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        \n",
    "        # 4ï¸âƒ£ Residual + LayerNorm\n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "\n",
    "dot_mha = DotProductMHA_Residual(E, num_heads=2)\n",
    "y, attn_map = dot_mha(x)\n",
    "print(\"Output:\", y.shape)        # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f518195",
   "metadata": {},
   "source": [
    "##  AdÄ±m â€” Causal / Look-Ahead Mask ve Autoregressive Destek\n",
    "\n",
    "### ğŸ”¹ AmaÃ§\n",
    "- Autoregressive gÃ¶revlerde her token yalnÄ±zca kendisinden Ã¶nceki tokenâ€™lara bakabilir.  \n",
    "- Gelecek tokenâ€™lar maskelenir (`-inf` ile softmax Ã¶ncesi).  \n",
    "- BÃ¶ylece dil modelleme ve seq2seq gÃ¶revleri iÃ§in uygun hale gelir.\n",
    "\n",
    "### ğŸ”¹ MantÄ±k\n",
    "1. Q/K/V projeksiyonlarÄ± â†’ headâ€™lere bÃ¶lÃ¼nÃ¼r  \n",
    "2. Skor:  \n",
    "\\[\n",
    "\\text{scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "\\]\n",
    "3. **Padding Mask** uygulanÄ±r (opsiyonel)  \n",
    "4. **Causal Mask** uygulanÄ±r  \n",
    "5. Softmax â†’ attn  \n",
    "6. Dropout â†’ attn ve output  \n",
    "7. Headâ€™ler birleÅŸtirilir  \n",
    "8. Residual + LayerNorm â†’ son Ã§Ä±ktÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce28f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class DotProductMHA_Causal(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, L, E = x.shape\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Padding mask\n",
    "        if padding_mask is not None:\n",
    "            mask_exp = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = torch.tril(torch.ones(L, L, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        out = self.out_proj(context)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.layernorm(x + out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "padding_mask = torch.tensor([[1,1,1,0],[1,1,0,0]])\n",
    "mha_causal = DotProductMHA_Causal(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_causal(x, padding_mask=padding_mask)\n",
    "print(\"Output:\", y.shape)\n",
    "print(\"Attention map:\", attn_map.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb9b6f",
   "metadata": {},
   "source": [
    "##  AdÄ±m â€” Opsiyonel Gating / Fusion\n",
    "\n",
    "### ğŸ”¹ AmaÃ§\n",
    "- Attention Ã§Ä±ktÄ±sÄ± ile orijinal inputâ€™u akÄ±llÄ±ca birleÅŸtirmek.  \n",
    "- Model, hangi bilgiyi daha Ã§ok kullanacaÄŸÄ±nÄ± Ã¶ÄŸrenir.  \n",
    "- Ã–zellikle Ã§ok katmanlÄ± veya encoder-decoder yapÄ±larÄ±nda faydalÄ±dÄ±r.\n",
    "\n",
    "### ğŸ”¹ MantÄ±k\n",
    "1. Attention Ã§Ä±ktÄ±sÄ± `context` ve orijinal input `x` alÄ±nÄ±r  \n",
    "2. Gating aÄŸÄ± (sigmoid aktivasyonlu) ile hangi kÄ±sÄ±mlarÄ±n kullanÄ±lacaÄŸÄ± belirlenir:\n",
    "\n",
    "\\[\n",
    "g = \\sigma(W_g [context; x])\n",
    "\\]\n",
    "\n",
    "3. Fusion uygulanÄ±r:\n",
    "\n",
    "\\[\n",
    "y = g * context + (1 - g) * x\n",
    "\\]\n",
    "\n",
    "4. Residual + LayerNorm uygulanÄ±r (opsiyonel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd5edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Gating(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm ve Dropout\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        \n",
    "        # Gating aÄŸÄ±\n",
    "        self.gate = nn.Linear(embed_dim*2, embed_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ± ve head bÃ¶lÃ¼nmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor ve attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Weighted sum ve head birleÅŸtirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        context = self.out_proj(context)\n",
    "        context = self.dropout(context)\n",
    "        \n",
    "        # Gating / Fusion\n",
    "        combined = torch.cat([context, x], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(combined))\n",
    "        out = g * context + (1 - g) * x\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0]])\n",
    "\n",
    "mha_gating = MiniMHA_Gating(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_gating(x, mask=mask)\n",
    "\n",
    "print(\"Output:\", y.shape)         # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620f0d0",
   "metadata": {},
   "source": [
    "## ğŸ’¡ AÃ§Ä±klama:\n",
    "\n",
    "* context ve x concat edilip gating aÄŸÄ± ile fusion uygulanÄ±yor\n",
    "\n",
    "* Sigmoid ile hangi bilgi ne kadar kullanÄ±lacak Ã¶ÄŸreniliyor\n",
    "\n",
    "* Dropout + LayerNorm ile stabil ve overfittingâ€™e karÅŸÄ± dayanÄ±klÄ± hÃ¢le getiriliyor\n",
    "\n",
    "* ArtÄ±k Mini Multi-Head Dot-Product Attention tam esnek ve gÃ¼Ã§lÃ¼ hÃ¢lde ğŸ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cdc5a4",
   "metadata": {},
   "source": [
    "## Mini Multi-Head Dot-Product Attention + Causal Mask + Padding Mask + Gating/Fusion kombinasyonunu birleÅŸtirip tam Ã¼retime hazÄ±r hÃ¢le getirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb80de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 4, 16])\n",
      "Attention map: torch.Size([2, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniMHA_Full(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=2, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ±\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # LayerNorm ve Dropout\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        \n",
    "        # Gating aÄŸÄ±\n",
    "        self.gate = nn.Linear(embed_dim*2, embed_dim)\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        B, L, E = x.shape\n",
    "        \n",
    "        # Q/K/V projeksiyonlarÄ± ve head bÃ¶lÃ¼nmesi\n",
    "        Q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # Skor ve attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Padding mask\n",
    "        if padding_mask is not None:\n",
    "            mask_exp = padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = torch.tril(torch.ones(L, L, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax ve dropout\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Weighted sum ve head birleÅŸtirme\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1,2).contiguous().view(B, L, E)\n",
    "        context = self.out_proj(context)\n",
    "        context = self.dropout(context)\n",
    "        \n",
    "        # Gating / Fusion\n",
    "        combined = torch.cat([context, x], dim=-1)\n",
    "        g = torch.sigmoid(self.gate(combined))\n",
    "        out = g * context + (1 - g) * x\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layernorm(out)\n",
    "        return out, attn\n",
    "\n",
    "# Test\n",
    "B, L, E = 2, 4, 16\n",
    "x = torch.randn(B, L, E)\n",
    "padding_mask = torch.tensor([[1,1,1,0],[1,1,0,0]])\n",
    "\n",
    "mha_full = MiniMHA_Full(E, num_heads=2, dp=0.1)\n",
    "y, attn_map = mha_full(x, padding_mask=padding_mask)\n",
    "\n",
    "print(\"Output:\", y.shape)         # (B, L, E)\n",
    "print(\"Attention map:\", attn_map.shape)  # (B, num_heads, L, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec5778d",
   "metadata": {},
   "source": [
    "## ğŸ’¡ AÃ§Ä±klama:\n",
    "\n",
    "* Q/K/V projeksiyonlarÄ± â†’ Multi-head Dot-Product Attention\n",
    "\n",
    "* Padding Mask â†’ geÃ§ersiz tokenâ€™larÄ± devre dÄ±ÅŸÄ± bÄ±rakÄ±r\n",
    "\n",
    "* Causal Mask â†’ autoregressive gÃ¶revler iÃ§in geleceÄŸe bakÄ±ÅŸÄ± engeller\n",
    "\n",
    "* Dropout â†’ attn ve output Ã¼zerinde overfittingâ€™i Ã¶nler\n",
    "\n",
    "* Gating / Fusion â†’ context ve x akÄ±llÄ±ca birleÅŸtirilir\n",
    "\n",
    "* Residual + LayerNorm â†’ stabil ve hÄ±zlÄ± Ã¶ÄŸrenme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6884041",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
