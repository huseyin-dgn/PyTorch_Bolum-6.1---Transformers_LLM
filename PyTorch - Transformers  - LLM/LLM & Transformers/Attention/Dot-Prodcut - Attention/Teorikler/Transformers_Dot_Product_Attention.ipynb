{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ff06b2",
   "metadata": {},
   "source": [
    "## 🔹 Dot-Product Attention — Detaylı Anlatım\n",
    "\n",
    "### 1️⃣ Amaç\n",
    "- Dot-Product Attention, sorgu (Q) ve anahtar (K) vektörleri arasındaki benzerlik üzerinden değerler (V) ile ağırlıklı çıktılar üretir.  \n",
    "- Transformer’da hem **Self-Attention** hem **Cross-Attention** temelinde kullanılır.  \n",
    "\n",
    "### 2️⃣ Girdi ve Çıkış\n",
    "**Inputs:**  \n",
    "- `Q` (Query) → (seq_len_q, d_k)  \n",
    "- `K` (Key) → (seq_len_k, d_k)  \n",
    "- `V` (Value) → (seq_len_k, d_v)  \n",
    "- `mask` (opsiyonel) → padding veya look-ahead maskesi  \n",
    "\n",
    "**Outputs:**  \n",
    "- `out` → (seq_len_q, d_v)  \n",
    "- `attn` → (seq_len_q, seq_len_k) → hangi Q’nun hangi K’ye ne kadar dikkat ettiği\n",
    "\n",
    "\n",
    "\n",
    "### 3️⃣ Adım Adım İşleyiş\n",
    "\n",
    "#### 🔹 3.1 Skor Hesaplama\n",
    "- Q ve K arasındaki benzerlik, **dot product** ile hesaplanır ve ölçeklenir:\n",
    "\n",
    "\\[\n",
    "\\text{scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "\\]\n",
    "\n",
    "- Ölçekleme (`√d_k`) softmax sonrası gradyanların stabil olmasını sağlar.\n",
    "\n",
    "#### 🔹 3.2 Mask Uygulama (Opsiyonel)\n",
    "- Padding veya causal mask uygulanır:  \n",
    "\\[\n",
    "\\text{scores}_{ij} = -\\infty \\quad \\text{if mask=0}\n",
    "\\]\n",
    "\n",
    "#### 🔹 3.3 Softmax ve Ağırlıklı Toplama\n",
    "- Softmax ile skorlar normalize edilir → ağırlıklı değer hesaplanır:\n",
    "\n",
    "\\[\n",
    "\\text{attn} = \\text{Softmax(scores)}\n",
    "\\]\n",
    "\\[\n",
    "\\text{out} = \\text{attn} \\cdot V\n",
    "\\]\n",
    "\n",
    "- Her sorgu, değerlerin **ağırlıklı ortalaması** ile temsil edilir.\n",
    "\n",
    "#### 🔹 3.4 Multi-Head (Opsiyonel)\n",
    "- Farklı başlıklar (head) ile paralel dikkat hesaplanabilir.  \n",
    "- Her head farklı bir “bakış açısı” öğrenir.  \n",
    "- Head’ler birleştirilip lineer projeksiyon uygulanır:\n",
    "\n",
    "\\[\n",
    "\\text{MultiHeadOut} = W_O [head_1; head_2; ... ; head_h]\n",
    "\\]\n",
    "\n",
    "#### 🔹 3.5 Residual + LayerNorm + Dropout\n",
    "- Çıktı stabil hale getirilir:  \n",
    "\n",
    "\\[\n",
    "\\text{final\\_out} = \\text{LayerNorm}(X + \\text{Dropout(MultiHeadOut)})\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### 4️⃣ Özellikler ve Avantajlar\n",
    "- Basit ama güçlü: **Q-K benzerliği → V ağırlıklı toplam**  \n",
    "- Multi-head ile paralel öğrenme  \n",
    "- Mask ile padding ve autoregressive görevler desteklenir  \n",
    "- Residual + LayerNorm ile derin ağlarda stabilite  \n",
    "\n",
    "\n",
    "\n",
    "### 5️⃣ Özet Akış Şeması\n",
    "1. Q, K, V tensor’ları → linear projeksiyon  \n",
    "2. Skor = QKᵀ / √d_k  \n",
    "3. Mask uygulanır (varsa)  \n",
    "4. Softmax → ağırlıklı toplam: attn × V  \n",
    "5. Multi-head ise head’ler birleştirilir  \n",
    "6. Residual + LayerNorm + Dropout → son çıktı\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
