{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ff06b2",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Dot-Product Attention â€” DetaylÄ± AnlatÄ±m\n",
    "\n",
    "### 1ï¸âƒ£ AmaÃ§\n",
    "- Dot-Product Attention, sorgu (Q) ve anahtar (K) vektÃ¶rleri arasÄ±ndaki benzerlik Ã¼zerinden deÄŸerler (V) ile aÄŸÄ±rlÄ±klÄ± Ã§Ä±ktÄ±lar Ã¼retir.  \n",
    "- Transformerâ€™da hem **Self-Attention** hem **Cross-Attention** temelinde kullanÄ±lÄ±r.  \n",
    "\n",
    "### 2ï¸âƒ£ Girdi ve Ã‡Ä±kÄ±ÅŸ\n",
    "**Inputs:**  \n",
    "- `Q` (Query) â†’ (seq_len_q, d_k)  \n",
    "- `K` (Key) â†’ (seq_len_k, d_k)  \n",
    "- `V` (Value) â†’ (seq_len_k, d_v)  \n",
    "- `mask` (opsiyonel) â†’ padding veya look-ahead maskesi  \n",
    "\n",
    "**Outputs:**  \n",
    "- `out` â†’ (seq_len_q, d_v)  \n",
    "- `attn` â†’ (seq_len_q, seq_len_k) â†’ hangi Qâ€™nun hangi Kâ€™ye ne kadar dikkat ettiÄŸi\n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ AdÄ±m AdÄ±m Ä°ÅŸleyiÅŸ\n",
    "\n",
    "#### ğŸ”¹ 3.1 Skor Hesaplama\n",
    "- Q ve K arasÄ±ndaki benzerlik, **dot product** ile hesaplanÄ±r ve Ã¶lÃ§eklenir:\n",
    "\n",
    "\\[\n",
    "\\text{scores} = \\frac{Q K^T}{\\sqrt{d_k}}\n",
    "\\]\n",
    "\n",
    "- Ã–lÃ§ekleme (`âˆšd_k`) softmax sonrasÄ± gradyanlarÄ±n stabil olmasÄ±nÄ± saÄŸlar.\n",
    "\n",
    "#### ğŸ”¹ 3.2 Mask Uygulama (Opsiyonel)\n",
    "- Padding veya causal mask uygulanÄ±r:  \n",
    "\\[\n",
    "\\text{scores}_{ij} = -\\infty \\quad \\text{if mask=0}\n",
    "\\]\n",
    "\n",
    "#### ğŸ”¹ 3.3 Softmax ve AÄŸÄ±rlÄ±klÄ± Toplama\n",
    "- Softmax ile skorlar normalize edilir â†’ aÄŸÄ±rlÄ±klÄ± deÄŸer hesaplanÄ±r:\n",
    "\n",
    "\\[\n",
    "\\text{attn} = \\text{Softmax(scores)}\n",
    "\\]\n",
    "\\[\n",
    "\\text{out} = \\text{attn} \\cdot V\n",
    "\\]\n",
    "\n",
    "- Her sorgu, deÄŸerlerin **aÄŸÄ±rlÄ±klÄ± ortalamasÄ±** ile temsil edilir.\n",
    "\n",
    "#### ğŸ”¹ 3.4 Multi-Head (Opsiyonel)\n",
    "- FarklÄ± baÅŸlÄ±klar (head) ile paralel dikkat hesaplanabilir.  \n",
    "- Her head farklÄ± bir â€œbakÄ±ÅŸ aÃ§Ä±sÄ±â€ Ã¶ÄŸrenir.  \n",
    "- Headâ€™ler birleÅŸtirilip lineer projeksiyon uygulanÄ±r:\n",
    "\n",
    "\\[\n",
    "\\text{MultiHeadOut} = W_O [head_1; head_2; ... ; head_h]\n",
    "\\]\n",
    "\n",
    "#### ğŸ”¹ 3.5 Residual + LayerNorm + Dropout\n",
    "- Ã‡Ä±ktÄ± stabil hale getirilir:  \n",
    "\n",
    "\\[\n",
    "\\text{final\\_out} = \\text{LayerNorm}(X + \\text{Dropout(MultiHeadOut)})\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ Ã–zellikler ve Avantajlar\n",
    "- Basit ama gÃ¼Ã§lÃ¼: **Q-K benzerliÄŸi â†’ V aÄŸÄ±rlÄ±klÄ± toplam**  \n",
    "- Multi-head ile paralel Ã¶ÄŸrenme  \n",
    "- Mask ile padding ve autoregressive gÃ¶revler desteklenir  \n",
    "- Residual + LayerNorm ile derin aÄŸlarda stabilite  \n",
    "\n",
    "\n",
    "\n",
    "### 5ï¸âƒ£ Ã–zet AkÄ±ÅŸ ÅemasÄ±\n",
    "1. Q, K, V tensorâ€™larÄ± â†’ linear projeksiyon  \n",
    "2. Skor = QKáµ€ / âˆšd_k  \n",
    "3. Mask uygulanÄ±r (varsa)  \n",
    "4. Softmax â†’ aÄŸÄ±rlÄ±klÄ± toplam: attn Ã— V  \n",
    "5. Multi-head ise headâ€™ler birleÅŸtirilir  \n",
    "6. Residual + LayerNorm + Dropout â†’ son Ã§Ä±ktÄ±\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
