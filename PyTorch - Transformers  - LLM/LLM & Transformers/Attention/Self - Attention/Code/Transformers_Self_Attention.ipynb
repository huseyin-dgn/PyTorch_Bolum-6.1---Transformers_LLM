{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a052e358",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556d1fa",
   "metadata": {},
   "source": [
    "> ## Bu, Transformer mimarisinin en temel yapı taşıdır.\n",
    "## Aşağıya sade ama açıklamalı bir PyTorch örneği yazalım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483cb5ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9e6d1",
   "metadata": {},
   "source": [
    "# 🧠 Self-Attention (Scaled Dot-Product) — Kod Yapısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b8508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e936e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # 3 ayrı lineer katman: Query, Key, Value\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embed_dim]\n",
    "        Q = self.query(x)   # [B, L, D]\n",
    "        K = self.key(x)     # [B, L, D]\n",
    "        V = self.value(x)   # [B, L, D]\n",
    "        \n",
    "        # 1️⃣ Skor hesaplama (benzerlik): Q * K^T\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale   # [B, L, L]\n",
    "        \n",
    "        # 2️⃣ Softmax ile normalize et (her kelime diğerlerine ne kadar dikkat edecek)\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # [B, L, L]\n",
    "        \n",
    "        # 3️⃣ Ağırlıklı toplam: dikkat değerleri * Value\n",
    "        output = torch.bmm(attn_weights, V)  # [B, L, D]\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08565a04",
   "metadata": {},
   "source": [
    "# 🔍 Adım Adım Açıklama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8356e4b",
   "metadata": {},
   "source": [
    "| Adım | İşlem              | Matematiksel Gösterim           | Açıklama                                                   |\n",
    "| ---- | ------------------ | ------------------------------- | ---------------------------------------------------------- |\n",
    "| 1️⃣  | Lineer dönüşüm     | Q = XW_Q, K = XW_K, V = XW_V    | Girdi (X), 3 farklı uzaya projekte edilir                  |\n",
    "| 2️⃣  | Benzerlik hesapla  | Attention = softmax(QKᵀ / √d_k) | Her kelimenin diğer kelimelere benzerliğini bulur          |\n",
    "| 3️⃣  | Ağırlıklı ortalama | Output = Attention * V          | “Hangi kelimelere dikkat edilmesi gerektiğini” uygular     |\n",
    "| 4️⃣  | Sonuç              | Y = Output                      | Artık her kelime bağlama duyarlı bir şekilde temsil edilir |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1141f82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Şimdi elimizdeki basit Self-Attention modülünü gerçek Transformer standardına daha yakın, performans ve esneklik odaklı bir yapıya çevirelim.\n",
    "---\n",
    "### Birkaç iyileştirme yapabiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6bee9",
   "metadata": {},
   "source": [
    "## ⚙️ İyileştirme Alanları\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "* Çıkışı normalize ederek gradyanların stabil kalmasını sağlar.\n",
    "\n",
    "#### Residual Connection (Skip Connection)\n",
    "\n",
    "* Self-Attention çıktısını girdiye ekleyerek öğrenmeyi kolaylaştırır.\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "* Aşırı öğrenmeyi (overfitting) engeller.\n",
    "\n",
    "#### Opsiyonel Masking\n",
    "\n",
    "* Decoder’da geleceğe bakmayı engeller, Encoder’da mask kullanımı opsiyoneldir.\n",
    "\n",
    "#### Sade ve okunabilir yapı\n",
    "\n",
    "* Kodun hem eğitim hem debugging sırasında anlaşılır olması."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f7d76",
   "metadata": {},
   "source": [
    "# 🔥 Geliştirilmiş Self-Attention — PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51dbb566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeada8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Query, Key, Value lineer katmanları\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Skor ölçekleme\n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, embed_dim]\n",
    "        mask: opsiyonel, [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 1️⃣ Q, K, V hesapla\n",
    "        Q = self.query(x)  # [B, L, D]\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # 2️⃣ Skorları hesapla\n",
    "        scores = torch.bmm(Q, K.transpose(1,2)) / self.scale  # [B, L, L]\n",
    "        \n",
    "        # 3️⃣ Mask varsa uygula\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 4️⃣ Softmax ve Dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 5️⃣ Ağırlıklı toplam\n",
    "        attn_output = torch.bmm(attn_weights, V)  # [B, L, D]\n",
    "        \n",
    "        # 6️⃣ Residual + LayerNorm\n",
    "        out = self.norm(attn_output + x)\n",
    "        \n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418488fe",
   "metadata": {},
   "source": [
    "## 🔍 İyileştirmelerin Açıklaması"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10b60e",
   "metadata": {},
   "source": [
    "| Özellik                 | Amaç                                                 |\n",
    "| ----------------------- | ---------------------------------------------------- |\n",
    "| **Residual Connection** | Öğrenmeyi kolaylaştırır, gradyan kaybını önler       |\n",
    "| **Layer Normalization** | Daha stabil ve hızlı eğitim sağlar                   |\n",
    "| **Dropout**             | Overfitting’i azaltır                                |\n",
    "| **Masking (opsiyonel)** | Decoder için geleceğe bakmayı engeller               |\n",
    "| **Sade yapı**           | Multi-Head eklemeden temel mekanizmayı optimize eder |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cd180",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25c709",
   "metadata": {},
   "source": [
    "## ⚙️ Son İyileştirmeler\n",
    "\n",
    "#### LayerNorm ve Residual – Eğitim stabilitesi ve gradyan akışı\n",
    "\n",
    "#### Dropout – Overfitting’e karşı\n",
    "\n",
    "#### Optional Masking – Decoder için hazır\n",
    "\n",
    "#### Sade ve anlaşılır yapı – Multi-Head eklemeden temel mekanizmayı optimize ediyoruz\n",
    "\n",
    "#### Matris çarpım optimizasyonu – bmm ve transpose ile GPU dostu\n",
    "\n",
    "#### Dikkat ağırlıklarının geri döndürülmesi – Görselleştirme ve debugging için"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71672d",
   "metadata": {},
   "source": [
    "# 🔥 Son Hâl: Improved Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce294fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class FinalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Query, Key, Value lineer katmanları\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Skor ölçekleme\n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, embed_dim]\n",
    "        mask: opsiyonel, [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 1️⃣ Q, K, V hesapla\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # 2️⃣ Skorları hesapla ve ölçekle\n",
    "        scores = torch.bmm(Q, K.transpose(1,2)) / self.scale\n",
    "        \n",
    "        # 3️⃣ Mask varsa uygula\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 4️⃣ Softmax + Dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 5️⃣ Ağırlıklı toplam\n",
    "        attn_output = torch.bmm(attn_weights, V)\n",
    "        \n",
    "        # 6️⃣ Residual + LayerNorm\n",
    "        out = self.norm(attn_output + x)\n",
    "        \n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020749d6",
   "metadata": {},
   "source": [
    "## 🔍 Bu Son Hâlin Avantajları"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ac04c",
   "metadata": {},
   "source": [
    "| Özellik                 | Amaç                                                      |\n",
    "| ----------------------- | --------------------------------------------------------- |\n",
    "| **Residual Connection** | Gradyan kaybını önler, öğrenmeyi kolaylaştırır            |\n",
    "| **Layer Normalization** | Eğitim stabilitesi ve hızlı öğrenme sağlar                |\n",
    "| **Dropout**             | Overfitting’e karşı                                       |\n",
    "| **Optional Masking**    | Decoder için geleceğe bakmayı engeller                    |\n",
    "| **Sade yapı**           | Multi-Head eklemeden temel Self-Attention’ı optimize eder |\n",
    "| **Attention Weights**   | Görselleştirme ve debugging için geri döndürülür          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f220ab8",
   "metadata": {},
   "source": [
    "## 💡 Özet:\n",
    "\n",
    "* Artık elimizde en stabil ve optimize Self-Attention modülü var.\n",
    "\n",
    "* Bu modül, tek başına veya bir Encoder/Decoder bloğunun içinde kullanılabilir.\n",
    "\n",
    "* Multi-Head eklemeden önce temel yapıyı son derece sağlamlaştırdık."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a80ffc",
   "metadata": {},
   "source": [
    "| Özellik                    | Önceki Sade Self-Attention          | Final / Geliştirilmiş Self-Attention           | Avantajı                                                                  |\n",
    "| -------------------------- | ----------------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| **Residual Connection**    | Yok                                 | Var (`out = LayerNorm(attn_output + x)`)       | Gradyan kaybını önler, öğrenmeyi kolaylaştırır. Eğitim stabilitesi artar. |\n",
    "| **Layer Normalization**    | Yok                                 | Var (`nn.LayerNorm`)                           | Çıkışları normalize eder, hızlı ve stabil eğitim sağlar.                  |\n",
    "| **Dropout**                | Var, sadece attention ağırlıklarına | Var, attention ağırlıkları ve softmax sonrası  | Overfitting’i azaltır, model daha genelleyici olur.                       |\n",
    "| **Masking**                | Opsiyonel ama temel yapı basit      | Opsiyonel ve kodda entegre                     | Decoder’da geleceğe bakmayı engeller, üretim görevlerinde kritik.         |\n",
    "| **Kod Düzeni / GPU Dostu** | Basit `bmm` ile çarpım              | Aynı `bmm` ama Residual+LayerNorm ile birleşik | Hem okunabilir hem performanslı                                           |\n",
    "| **Görselleştirme / Debug** | `attn_weights` döndürülüyor         | `attn_weights` döndürülüyor                    | Aynı, ama artık daha stabil.                                              |\n",
    "| **Matematiksel Stabilite** | Skorlar softmax öncesi direkt       | Skorlar ölçekleniyor (`/√d`) + softmax         | Büyük embedding boyutlarında softmax overflow riskini azaltır             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f211c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
