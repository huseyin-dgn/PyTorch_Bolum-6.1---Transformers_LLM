{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a052e358",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556d1fa",
   "metadata": {},
   "source": [
    "> ## Bu, Transformer mimarisinin en temel yapÄ± taÅŸÄ±dÄ±r.\n",
    "## AÅŸaÄŸÄ±ya sade ama aÃ§Ä±klamalÄ± bir PyTorch Ã¶rneÄŸi yazalÄ±m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483cb5ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9e6d1",
   "metadata": {},
   "source": [
    "# ğŸ§  Self-Attention (Scaled Dot-Product) â€” Kod YapÄ±sÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b8508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e936e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # 3 ayrÄ± lineer katman: Query, Key, Value\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embed_dim]\n",
    "        Q = self.query(x)   # [B, L, D]\n",
    "        K = self.key(x)     # [B, L, D]\n",
    "        V = self.value(x)   # [B, L, D]\n",
    "        \n",
    "        # 1ï¸âƒ£ Skor hesaplama (benzerlik): Q * K^T\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale   # [B, L, L]\n",
    "        \n",
    "        # 2ï¸âƒ£ Softmax ile normalize et (her kelime diÄŸerlerine ne kadar dikkat edecek)\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # [B, L, L]\n",
    "        \n",
    "        # 3ï¸âƒ£ AÄŸÄ±rlÄ±klÄ± toplam: dikkat deÄŸerleri * Value\n",
    "        output = torch.bmm(attn_weights, V)  # [B, L, D]\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08565a04",
   "metadata": {},
   "source": [
    "# ğŸ” AdÄ±m AdÄ±m AÃ§Ä±klama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8356e4b",
   "metadata": {},
   "source": [
    "| AdÄ±m | Ä°ÅŸlem              | Matematiksel GÃ¶sterim           | AÃ§Ä±klama                                                   |\n",
    "| ---- | ------------------ | ------------------------------- | ---------------------------------------------------------- |\n",
    "| 1ï¸âƒ£  | Lineer dÃ¶nÃ¼ÅŸÃ¼m     | Q = XW_Q, K = XW_K, V = XW_V    | Girdi (X), 3 farklÄ± uzaya projekte edilir                  |\n",
    "| 2ï¸âƒ£  | Benzerlik hesapla  | Attention = softmax(QKáµ€ / âˆšd_k) | Her kelimenin diÄŸer kelimelere benzerliÄŸini bulur          |\n",
    "| 3ï¸âƒ£  | AÄŸÄ±rlÄ±klÄ± ortalama | Output = Attention * V          | â€œHangi kelimelere dikkat edilmesi gerektiÄŸiniâ€ uygular     |\n",
    "| 4ï¸âƒ£  | SonuÃ§              | Y = Output                      | ArtÄ±k her kelime baÄŸlama duyarlÄ± bir ÅŸekilde temsil edilir |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1141f82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Åimdi elimizdeki basit Self-Attention modÃ¼lÃ¼nÃ¼ gerÃ§ek Transformer standardÄ±na daha yakÄ±n, performans ve esneklik odaklÄ± bir yapÄ±ya Ã§evirelim.\n",
    "---\n",
    "### BirkaÃ§ iyileÅŸtirme yapabiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6bee9",
   "metadata": {},
   "source": [
    "## âš™ï¸ Ä°yileÅŸtirme AlanlarÄ±\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "* Ã‡Ä±kÄ±ÅŸÄ± normalize ederek gradyanlarÄ±n stabil kalmasÄ±nÄ± saÄŸlar.\n",
    "\n",
    "#### Residual Connection (Skip Connection)\n",
    "\n",
    "* Self-Attention Ã§Ä±ktÄ±sÄ±nÄ± girdiye ekleyerek Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r.\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "* AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) engeller.\n",
    "\n",
    "#### Opsiyonel Masking\n",
    "\n",
    "* Decoderâ€™da geleceÄŸe bakmayÄ± engeller, Encoderâ€™da mask kullanÄ±mÄ± opsiyoneldir.\n",
    "\n",
    "#### Sade ve okunabilir yapÄ±\n",
    "\n",
    "* Kodun hem eÄŸitim hem debugging sÄ±rasÄ±nda anlaÅŸÄ±lÄ±r olmasÄ±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f7d76",
   "metadata": {},
   "source": [
    "# ğŸ”¥ GeliÅŸtirilmiÅŸ Self-Attention â€” PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51dbb566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeada8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Query, Key, Value lineer katmanlarÄ±\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Skor Ã¶lÃ§ekleme\n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, embed_dim]\n",
    "        mask: opsiyonel, [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 1ï¸âƒ£ Q, K, V hesapla\n",
    "        Q = self.query(x)  # [B, L, D]\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # 2ï¸âƒ£ SkorlarÄ± hesapla\n",
    "        scores = torch.bmm(Q, K.transpose(1,2)) / self.scale  # [B, L, L]\n",
    "        \n",
    "        # 3ï¸âƒ£ Mask varsa uygula\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 4ï¸âƒ£ Softmax ve Dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 5ï¸âƒ£ AÄŸÄ±rlÄ±klÄ± toplam\n",
    "        attn_output = torch.bmm(attn_weights, V)  # [B, L, D]\n",
    "        \n",
    "        # 6ï¸âƒ£ Residual + LayerNorm\n",
    "        out = self.norm(attn_output + x)\n",
    "        \n",
    "        return out, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418488fe",
   "metadata": {},
   "source": [
    "## ğŸ” Ä°yileÅŸtirmelerin AÃ§Ä±klamasÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10b60e",
   "metadata": {},
   "source": [
    "| Ã–zellik                 | AmaÃ§                                                 |\n",
    "| ----------------------- | ---------------------------------------------------- |\n",
    "| **Residual Connection** | Ã–ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r, gradyan kaybÄ±nÄ± Ã¶nler       |\n",
    "| **Layer Normalization** | Daha stabil ve hÄ±zlÄ± eÄŸitim saÄŸlar                   |\n",
    "| **Dropout**             | Overfittingâ€™i azaltÄ±r                                |\n",
    "| **Masking (opsiyonel)** | Decoder iÃ§in geleceÄŸe bakmayÄ± engeller               |\n",
    "| **Sade yapÄ±**           | Multi-Head eklemeden temel mekanizmayÄ± optimize eder |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cd180",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25c709",
   "metadata": {},
   "source": [
    "## âš™ï¸ Son Ä°yileÅŸtirmeler\n",
    "\n",
    "#### LayerNorm ve Residual â€“ EÄŸitim stabilitesi ve gradyan akÄ±ÅŸÄ±\n",
    "\n",
    "#### Dropout â€“ Overfittingâ€™e karÅŸÄ±\n",
    "\n",
    "#### Optional Masking â€“ Decoder iÃ§in hazÄ±r\n",
    "\n",
    "#### Sade ve anlaÅŸÄ±lÄ±r yapÄ± â€“ Multi-Head eklemeden temel mekanizmayÄ± optimize ediyoruz\n",
    "\n",
    "#### Matris Ã§arpÄ±m optimizasyonu â€“ bmm ve transpose ile GPU dostu\n",
    "\n",
    "#### Dikkat aÄŸÄ±rlÄ±klarÄ±nÄ±n geri dÃ¶ndÃ¼rÃ¼lmesi â€“ GÃ¶rselleÅŸtirme ve debugging iÃ§in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71672d",
   "metadata": {},
   "source": [
    "# ğŸ”¥ Son HÃ¢l: Improved Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce294fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class FinalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Query, Key, Value lineer katmanlarÄ±\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Skor Ã¶lÃ§ekleme\n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, embed_dim]\n",
    "        mask: opsiyonel, [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # 1ï¸âƒ£ Q, K, V hesapla\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # 2ï¸âƒ£ SkorlarÄ± hesapla ve Ã¶lÃ§ekle\n",
    "        scores = torch.bmm(Q, K.transpose(1,2)) / self.scale\n",
    "        \n",
    "        # 3ï¸âƒ£ Mask varsa uygula\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # 4ï¸âƒ£ Softmax + Dropout\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # 5ï¸âƒ£ AÄŸÄ±rlÄ±klÄ± toplam\n",
    "        attn_output = torch.bmm(attn_weights, V)\n",
    "        \n",
    "        # 6ï¸âƒ£ Residual + LayerNorm\n",
    "        out = self.norm(attn_output + x)\n",
    "        \n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020749d6",
   "metadata": {},
   "source": [
    "## ğŸ” Bu Son HÃ¢lin AvantajlarÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ac04c",
   "metadata": {},
   "source": [
    "| Ã–zellik                 | AmaÃ§                                                      |\n",
    "| ----------------------- | --------------------------------------------------------- |\n",
    "| **Residual Connection** | Gradyan kaybÄ±nÄ± Ã¶nler, Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r            |\n",
    "| **Layer Normalization** | EÄŸitim stabilitesi ve hÄ±zlÄ± Ã¶ÄŸrenme saÄŸlar                |\n",
    "| **Dropout**             | Overfittingâ€™e karÅŸÄ±                                       |\n",
    "| **Optional Masking**    | Decoder iÃ§in geleceÄŸe bakmayÄ± engeller                    |\n",
    "| **Sade yapÄ±**           | Multi-Head eklemeden temel Self-Attentionâ€™Ä± optimize eder |\n",
    "| **Attention Weights**   | GÃ¶rselleÅŸtirme ve debugging iÃ§in geri dÃ¶ndÃ¼rÃ¼lÃ¼r          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f220ab8",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Ã–zet:\n",
    "\n",
    "* ArtÄ±k elimizde en stabil ve optimize Self-Attention modÃ¼lÃ¼ var.\n",
    "\n",
    "* Bu modÃ¼l, tek baÅŸÄ±na veya bir Encoder/Decoder bloÄŸunun iÃ§inde kullanÄ±labilir.\n",
    "\n",
    "* Multi-Head eklemeden Ã¶nce temel yapÄ±yÄ± son derece saÄŸlamlaÅŸtÄ±rdÄ±k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a80ffc",
   "metadata": {},
   "source": [
    "| Ã–zellik                    | Ã–nceki Sade Self-Attention          | Final / GeliÅŸtirilmiÅŸ Self-Attention           | AvantajÄ±                                                                  |\n",
    "| -------------------------- | ----------------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| **Residual Connection**    | Yok                                 | Var (`out = LayerNorm(attn_output + x)`)       | Gradyan kaybÄ±nÄ± Ã¶nler, Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±r. EÄŸitim stabilitesi artar. |\n",
    "| **Layer Normalization**    | Yok                                 | Var (`nn.LayerNorm`)                           | Ã‡Ä±kÄ±ÅŸlarÄ± normalize eder, hÄ±zlÄ± ve stabil eÄŸitim saÄŸlar.                  |\n",
    "| **Dropout**                | Var, sadece attention aÄŸÄ±rlÄ±klarÄ±na | Var, attention aÄŸÄ±rlÄ±klarÄ± ve softmax sonrasÄ±  | Overfittingâ€™i azaltÄ±r, model daha genelleyici olur.                       |\n",
    "| **Masking**                | Opsiyonel ama temel yapÄ± basit      | Opsiyonel ve kodda entegre                     | Decoderâ€™da geleceÄŸe bakmayÄ± engeller, Ã¼retim gÃ¶revlerinde kritik.         |\n",
    "| **Kod DÃ¼zeni / GPU Dostu** | Basit `bmm` ile Ã§arpÄ±m              | AynÄ± `bmm` ama Residual+LayerNorm ile birleÅŸik | Hem okunabilir hem performanslÄ±                                           |\n",
    "| **GÃ¶rselleÅŸtirme / Debug** | `attn_weights` dÃ¶ndÃ¼rÃ¼lÃ¼yor         | `attn_weights` dÃ¶ndÃ¼rÃ¼lÃ¼yor                    | AynÄ±, ama artÄ±k daha stabil.                                              |\n",
    "| **Matematiksel Stabilite** | Skorlar softmax Ã¶ncesi direkt       | Skorlar Ã¶lÃ§ekleniyor (`/âˆšd`) + softmax         | BÃ¼yÃ¼k embedding boyutlarÄ±nda softmax overflow riskini azaltÄ±r             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f211c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
