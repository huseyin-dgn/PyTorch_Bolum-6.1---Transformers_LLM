{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72a096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5006de4",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b2f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: torch.Size([2, 16])\n",
      "Encoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Input IDs: torch.Size([2, 16])\n",
      "Decoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Target IDs: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 1️⃣ Seq2Seq Tokenization Dataset (T5/BART uyumlu)\n",
    "# =====================================\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Tokenization pipeline for seq2seq LLMs (T5, BART, mBART)\n",
    "    Includes encoder/decoder tokenization, attention masks, shifted decoder input.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources, targets, tokenizer_name=\"t5-small\", max_length=64):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Seq2Seq modellerinde decoder_start_token_id önemli\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # zorunlu pad token\n",
    "        self.decoder_start_token_id = self.tokenizer.pad_token_id if self.tokenizer.bos_token_id is None else self.tokenizer.bos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # -------------------\n",
    "        # Encoder tokenization\n",
    "        # -------------------\n",
    "        enc = self.tokenizer(\n",
    "            self.sources[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder tokenization\n",
    "        # -------------------\n",
    "        dec = self.tokenizer(\n",
    "            self.targets[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder input (shifted right)\n",
    "        # -------------------\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [torch.full((1,1), self.decoder_start_token_id), dec['input_ids'][:, :-1]], dim=1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': enc['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'decoder_target_ids': dec['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# =====================================\n",
    "# 2️⃣ Collate function\n",
    "# =====================================\n",
    "def collate_fn(batch):\n",
    "    enc_ids = torch.stack([item['encoder_input_ids'] for item in batch])\n",
    "    enc_mask = torch.stack([item['encoder_attention_mask'] for item in batch])\n",
    "    dec_ids = torch.stack([item['decoder_input_ids'] for item in batch])\n",
    "    dec_mask = torch.stack([item['decoder_attention_mask'] for item in batch])\n",
    "    dec_target = torch.stack([item['decoder_target_ids'] for item in batch])\n",
    "    return {\n",
    "        'encoder_input_ids': enc_ids,\n",
    "        'encoder_attention_mask': enc_mask,\n",
    "        'decoder_input_ids': dec_ids,\n",
    "        'decoder_attention_mask': dec_mask,\n",
    "        'decoder_target_ids': dec_target\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# 3️⃣ Örnek kullanım\n",
    "# =====================================\n",
    "input_texts = [\n",
    "    \"Merhaba dünya!\",\n",
    "    \"Transformers çok güçlü.\",\n",
    "    \"Memory-efficient pipeline.\"\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are powerful.\",\n",
    "    \"Super useful pipeline.\"\n",
    "]\n",
    "\n",
    "dataset = Seq2SeqDataset(input_texts, target_texts, tokenizer_name=\"t5-small\", max_length=16)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# 4️⃣ Test bir batch\n",
    "# =====================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    print(\"Encoder Input IDs:\", batch['encoder_input_ids'].shape)\n",
    "    print(\"Encoder Attention Mask:\", batch['encoder_attention_mask'].shape)\n",
    "    print(\"Decoder Input IDs:\", batch['decoder_input_ids'].shape)\n",
    "    print(\"Decoder Attention Mask:\", batch['decoder_attention_mask'].shape)\n",
    "    print(\"Decoder Target IDs:\", batch['decoder_target_ids'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb76f80",
   "metadata": {},
   "source": [
    "# ENCODER & DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55e1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0) , ) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape , dtype=x.dtype , device = x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) *  random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cca8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f12aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09fed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b610cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714b9b5",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac03161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa7aa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersEncoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size , embed_dim = 1024 , num_layers = 12 , dp = 0.1 ,num_heads=16 ,  expansion = 8 , max_len= 5000 , drop_path = 0.1 , use_swiglu =False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = TokenEmbedding(vocab_size,embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim , max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerEncoderBlockLLM(embed_dim , num_heads , dp , drop_path , expansion , use_swiglu) for _ in range(num_layers)]\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self,src_tokens , src_mask =None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask = src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464b4f0",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8e3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a11865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca86074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000    # kelime dağarcığı\n",
    "embed_dim = 1024\n",
    "num_layers = 12\n",
    "num_heads = 16\n",
    "dp = 0.1\n",
    "drop_path = 0.1\n",
    "expansion = 8\n",
    "max_len = 512\n",
    "use_swiglu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "510eabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformersEncoderLLM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    dp=dp,\n",
    "    drop_path=drop_path,\n",
    "    expansion=expansion,\n",
    "    max_len=max_len,\n",
    "    use_swiglu=use_swiglu\n",
    ")\n",
    "\n",
    "decoder = TransformerDecoderLLM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    dp=dp,\n",
    "    drop_path=drop_path,\n",
    "    expansion=expansion,\n",
    "    max_len=max_len,\n",
    "    use_swiglu=use_swiglu\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d86a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLLM(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src_tokens, tgt_tokens, src_mask=None, tgt_mask=None):\n",
    "        enc_out = self.encoder(src_tokens, src_mask)\n",
    "        logits = self.decoder(tgt_tokens, enc_out, self_mask=tgt_mask, enc_mask=src_mask)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ac42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqLLM(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6105f02",
   "metadata": {},
   "source": [
    "# TRAİN_LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7834eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecc8277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True\n",
    "accumulation_steps = 2  \n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c89d85",
   "metadata": {},
   "source": [
    "### TEST İÇİNDİR ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e652ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([2, 16]) torch.Size([2, 16]) torch.Size([2, 16])\n",
      "Max target id: 12045\n",
      "Vocab size: 20000\n",
      "Batch shapes: torch.Size([1, 16]) torch.Size([1, 16]) torch.Size([1, 16])\n",
      "Max target id: 31220\n",
      "Vocab size: 20000\n",
      "⚠️ Target id > vocab_size!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  # önce CPU’da çalıştır\n",
    "\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    batch = {k: v for k,v in batch.items()}  \n",
    "    input_ids = batch['encoder_input_ids']\n",
    "    dec_input = batch['decoder_input_ids']\n",
    "    dec_target = batch['decoder_target_ids']\n",
    "\n",
    "    print(\"Batch shapes:\", input_ids.shape, dec_input.shape, dec_target.shape)\n",
    "    print(\"Max target id:\", dec_target.max().item())\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "    # target id vocab_size’dan büyükse sorun burada\n",
    "    if dec_target.max().item() >= vocab_size:\n",
    "        print(\"⚠️ Target id > vocab_size!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1851f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_seq2seq(model, tokenizer, src_texts, max_len=32, top_k=50, top_p=0.9, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Seq2Seq için Top-K + Top-P (nucleus) sampling tabanlı generation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # -----------------------------\n",
    "    # Encoder tokenization\n",
    "    # -----------------------------\n",
    "    batch_enc = tokenizer(\n",
    "        src_texts,\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    enc_ids = batch_enc['input_ids'].to(device)\n",
    "    enc_mask = batch_enc['attention_mask'].to(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Decoder başlangıcı (bos/pad token)\n",
    "    # -----------------------------\n",
    "    dec_input_ids = torch.full(\n",
    "        (enc_ids.size(0), 1),\n",
    "        tokenizer.pad_token_id,\n",
    "        dtype=torch.long,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # Adım adım generation\n",
    "    # -----------------------------\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_len):\n",
    "            logits = model(enc_ids, dec_input_ids, src_mask=enc_mask, tgt_mask=None)\n",
    "            next_token_logits = logits[:, -1, :]  # sadece son token\n",
    "            # -----------------------------\n",
    "            # Top-K sampling\n",
    "            # -----------------------------\n",
    "            if top_k > 0:\n",
    "                topk_vals, topk_idx = torch.topk(next_token_logits, top_k)\n",
    "                next_token_logits_filtered = torch.full_like(next_token_logits, -float('Inf'))\n",
    "                next_token_logits_filtered.scatter_(1, topk_idx, topk_vals)\n",
    "                next_token_logits = next_token_logits_filtered\n",
    "\n",
    "            # -----------------------------\n",
    "            # Top-P (nucleus) sampling\n",
    "            # -----------------------------\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                mask = cumulative_probs > top_p\n",
    "                mask[..., 1:] = mask[..., :-1].clone()\n",
    "                mask[..., 0] = 0\n",
    "                next_token_logits[sorted_indices] = next_token_logits[sorted_indices].masked_fill(mask, -float('Inf'))\n",
    "\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            dec_input_ids = torch.cat([dec_input_ids, next_tokens], dim=1)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Decode token IDs -> string\n",
    "    # -----------------------------\n",
    "    for seq in dec_input_ids:\n",
    "        text = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        outputs.append(text)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = dataset.tokenizer\n",
    "src_texts = [\"Merhaba dünya!\", \"Transformers çok güçlü.\"]\n",
    "\n",
    "outputs = generate_seq2seq(model, tokenizer, src_texts, max_len=16, top_k=50, top_p=0.9, device=device)\n",
    "\n",
    "for src, out in zip(src_texts, outputs):\n",
    "    print(f\"{src} -> {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5661b6",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Merhaba dünya! -> Hello world!\n",
    "Transformers çok güçlü. -> Transformers are powerful.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
