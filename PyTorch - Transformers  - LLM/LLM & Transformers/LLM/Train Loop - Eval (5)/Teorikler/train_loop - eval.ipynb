{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a91386",
   "metadata": {},
   "source": [
    "# ðŸ”¹ LLM Train Loop Teorisi\n",
    "\n",
    "LLM'lerde **train loop**, modelin veriyi Ã¶ÄŸrenmesini saÄŸlayan Ã§ekirdek mekanizmadÄ±r. Burada **compile**, **metric** veya **validation** kÄ±smÄ±na girmeden sadece **forward, loss ve backward** adÄ±mlarÄ±nÄ± ele alÄ±yoruz.\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ Veri HazÄ±rlÄ±ÄŸÄ± ve Batch Ä°ÅŸleme\n",
    "- Veriler **mini-batch** hÃ¢linde iÅŸlenir:\n",
    "  - `input_ids` â†’ Encoder/Decoder giriÅŸleri\n",
    "  - `target_ids` â†’ Ã–ÄŸrenilecek hedef token dizileri\n",
    "- **AmaÃ§:** GPU hafÄ±zasÄ±nÄ± korumak ve Ã¶ÄŸrenmeyi stabil hÃ¢le getirmek.\n",
    "\n",
    "> Mini-batch, hem hesaplama verimliliÄŸi saÄŸlar hem de gradyanlarÄ±n daha stabil olmasÄ±na yardÄ±mcÄ± olur.\n",
    "\n",
    "\n",
    "## 2ï¸âƒ£ Forward Pass\n",
    "- Model, aÄŸÄ±rlÄ±klar Ã¼zerinden veri geÃ§irir ve **tahminler (logits)** Ã¼retir.\n",
    "- **Decoder** iÃ§in genellikle **teacher forcing** uygulanÄ±r:\n",
    "  - Decoderâ€™a bir adÄ±m kaydÄ±rÄ±lmÄ±ÅŸ hedef tokenlar verilir.\n",
    "  - HatalarÄ±n birikmesi engellenir, Ã¶ÄŸrenme hÄ±zlanÄ±r.\n",
    "- Ã‡Ä±kÄ±ÅŸ boyutu: `[batch_size, seq_len, vocab_size]`\n",
    "\n",
    "\n",
    "## 3ï¸âƒ£ Loss Hesaplama\n",
    "- Tahminler ile gerÃ§ek hedefler arasÄ±ndaki fark Ã¶lÃ§Ã¼lÃ¼r.\n",
    "- Genellikle **CrossEntropyLoss** kullanÄ±lÄ±r:\n",
    "  - Padding tokenlar loss hesabÄ±na dahil edilmez (`ignore_index=PAD_TOKEN`)\n",
    "- Tensor boyutlarÄ± reshape edilir: `[B*T, V]` ve `[B*T]`\n",
    "\n",
    "> Model her token iÃ§in doÄŸru kelimeyi tahmin etmeye Ã§alÄ±ÅŸÄ±r. Loss, tokenâ€™larÄ±n ortalamasÄ± olarak hesaplanÄ±r.\n",
    "\n",
    "\n",
    "## 4ï¸âƒ£ Backward Pass ve Optimize\n",
    "- `loss.backward()` â†’ Gradyanlar hesaplanÄ±r (chain rule)\n",
    "- **Gradient clipping** â†’ Ã‡ok derin aÄŸlarda gradyan patlamasÄ±nÄ± Ã¶nler\n",
    "- `optimizer.step()` â†’ Model aÄŸÄ±rlÄ±klarÄ± gÃ¼ncellenir\n",
    "\n",
    "> Forwardâ€™da hesaplanan hatayÄ± geriye yayarak model daha doÄŸru tahminler yapmayÄ± Ã¶ÄŸrenir.\n",
    "\n",
    "\n",
    "## 5ï¸âƒ£ Epoch DÃ¶ngÃ¼sÃ¼\n",
    "- Her **epoch**, tÃ¼m veri Ã¼zerinden bir geÃ§iÅŸtir.\n",
    "- Genellikle epoch sonunda:\n",
    "  - Ortalama loss hesaplanÄ±r\n",
    "  - Modelin Ã¶ÄŸrenme durumu gÃ¶zlemlenir\n",
    "\n",
    "> Validation veya metric hesaplama burada iÅŸlenmez; sadece **train loop Ã§ekirdeÄŸi** odaklanÄ±lÄ±r.\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”‘ Ã–zet MantÄ±k\n",
    "1. **Batch al** â†’ Veri kÃ¼Ã§Ã¼k parÃ§alara bÃ¶lÃ¼nÃ¼r  \n",
    "2. **Forward** â†’ Model tahmin yapar (teacher forcing)  \n",
    "3. **Loss** â†’ Tahmin ile gerÃ§ek hedef karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r  \n",
    "4. **Backward** â†’ Gradyanlar hesaplanÄ±r  \n",
    "5. **Optimize** â†’ AÄŸÄ±rlÄ±klar gÃ¼ncellenir  \n",
    "\n",
    "> Bu 5 adÄ±m, tÃ¼m LLM train loopâ€™larÄ±nÄ±n temelini oluÅŸturur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5140e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fd53b",
   "metadata": {},
   "source": [
    "# LLM Train Loop:\n",
    "```bash\n",
    "\n",
    "Start Epoch\n",
    "   â”‚\n",
    "   â–¼\n",
    "Load Mini-Batch\n",
    "   â”‚\n",
    "   â–¼\n",
    "Move input_ids & target_ids to Device\n",
    "   â”‚\n",
    "   â–¼\n",
    "Forward Pass (model(input_ids, target_ids))\n",
    "   â”‚\n",
    "   â–¼\n",
    "Compute Loss (CrossEntropyLoss)\n",
    "   â”‚\n",
    "   â–¼\n",
    "Backward Pass (loss.backward())\n",
    "   â”‚\n",
    "   â–¼\n",
    "Gradient Clipping (optional)\n",
    "   â”‚\n",
    "   â–¼\n",
    "Optimizer Step (update model weights)\n",
    "   â”‚\n",
    "   â–¼\n",
    "Accumulate total_loss\n",
    "   â”‚\n",
    "   â–¼\n",
    "More Batches? â”€â”€ Yes â”€â”€â–¶ Back to Load Mini-Batch\n",
    "      â”‚\n",
    "      No\n",
    "      â”‚\n",
    "      â–¼\n",
    "Compute Average Loss for Epoch\n",
    "      â”‚\n",
    "      â–¼\n",
    "More Epochs? â”€â”€ Yes â”€â”€â–¶ Back to Load Mini-Batch\n",
    "      â”‚\n",
    "      No\n",
    "      â”‚\n",
    "      â–¼\n",
    "End Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603eb4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
