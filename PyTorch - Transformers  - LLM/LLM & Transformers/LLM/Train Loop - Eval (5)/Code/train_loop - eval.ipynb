{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23803e2",
   "metadata": {},
   "source": [
    "# üîπ LLM Train Loop: Adƒ±m Adƒ±m Kod Akƒ±≈üƒ±\n",
    "\n",
    "LLM train loop‚Äôu adƒ±m adƒ±m in≈üa edeceƒüiz ve her adƒ±mda iyile≈ütirmeler ekleyeceƒüiz. Teorideki 5 temel adƒ±mƒ± kod seviyesinde ele alacaƒüƒ±z.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Veri ve Batch Hazƒ±rlƒ±ƒüƒ±\n",
    "- Dataset ve DataLoader olu≈üturma\n",
    "- Veriyi GPU/CPU cihazƒ±na ta≈üƒ±ma\n",
    "- Padding ve attention mask kontrolleri\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Forward Pass\n",
    "- Modeli √ßaƒüƒ±rma\n",
    "- Teacher forcing uygulanmasƒ± (decoder kullanƒ±lƒ±yorsa)\n",
    "- √áƒ±kƒ±≈ü (`logits`) boyut kontrol√º\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Loss Hesaplama\n",
    "- CrossEntropyLoss (veya label smoothing)\n",
    "- Padding tokenlarƒ±nƒ± ignore etme\n",
    "- Token boyutlarƒ±nƒ± reshape etme (`[B*T, V]` ve `[B*T]`)\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Backward Pass & Optimize\n",
    "- `loss.backward()`\n",
    "- Gradient clipping\n",
    "- `optimizer.step()`\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Epoch D√∂ng√ºs√º ve Logging\n",
    "- Toplam loss biriktirme\n",
    "- Ortalama loss hesaplama\n",
    "- Opsiyonel: basit progress bar veya logging\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ ƒ∞yile≈ütirmeler (Opsiyonel)\n",
    "- Mixed precision (AMP) kullanƒ±mƒ±\n",
    "- Gradient accumulation\n",
    "- Learning rate scheduler\n",
    "\n",
    "---\n",
    "\n",
    "> üí° Not: √ñnce temel d√∂ng√ºy√º √ßalƒ±≈üƒ±r h√¢le getireceƒüiz, ardƒ±ndan isteƒüe baƒülƒ± iyile≈ütirmeleri ekleyeceƒüiz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e4540",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5896071",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£: Veri ve Batch Hazƒ±rlƒ±ƒüƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a836981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[5, 6, 7, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 0, 0]], device='cuda:0')\n",
      "Target IDs: tensor([[1, 2, 3, 4, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# -----------------------------\n",
    "# √ñrnek Dataset\n",
    "# -----------------------------\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, enc_inputs, dec_targets):\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_targets = dec_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enc_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.enc_inputs[idx], dtype=torch.long),\n",
    "            'target_ids': torch.tensor(self.dec_targets[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# √ñrnek veri\n",
    "# -----------------------------\n",
    "enc_inputs = [[1,2,3,4,0,0],[5,6,7,0,0,0]]   # 0 = PAD token\n",
    "dec_targets = [[1,2,3,4,5,0],[1,2,3,4,0,0]]\n",
    "\n",
    "# Dataset & DataLoader\n",
    "dataset = MyDataset(enc_inputs, dec_targets)\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# -----------------------------\n",
    "# GPU/CPU cihazƒ±\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# √ñrnek batch kullanƒ±mƒ±\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)       # [B, seq_len]\n",
    "    target_ids = batch['target_ids'].to(device)     # [B, seq_len]\n",
    "    \n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Target IDs:\", target_ids)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddeb73",
   "metadata": {},
   "source": [
    "### üîπ A√ßƒ±klamalar\n",
    "\n",
    "* Dataset ve DataLoader:\n",
    "\n",
    "MyDataset sƒ±nƒ±fƒ±, encoder giri≈üleri (input_ids) ve decoder hedeflerini (target_ids) alƒ±yor.\n",
    "\n",
    "DataLoader mini-batch olu≈üturur ve shuffle ile rastgele sƒ±rayla verir.\n",
    "\n",
    "* Cihaz (device) se√ßimi:\n",
    "\n",
    "torch.device ile GPU varsa oraya, yoksa CPU‚Äôya ta≈üƒ±r.\n",
    "\n",
    "* Batch √ßekme:\n",
    "\n",
    "Her iterasyonda input_ids ve target_ids batch olarak alƒ±nƒ±r.\n",
    "\n",
    "Modelin forward pass‚Äôine hazƒ±r h√¢le gelir.\n",
    "\n",
    "* Padding kontrol√º:\n",
    "\n",
    "√ñrnek veride 0 PAD token olarak kullanƒ±ldƒ±.\n",
    "\n",
    "Daha sonra loss hesaplamada ignore edilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3a4da",
   "metadata": {},
   "source": [
    "---\n",
    "# 2Ô∏è‚É£: Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806f3d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# √ñrnek Model (Encoder-Decoder)\n",
    "# -----------------------------\n",
    "class SimpleSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size=10, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # Encoder\n",
    "        enc_emb = self.embedding(enc_input)\n",
    "        _, (h, c) = self.encoder(enc_emb)\n",
    "        \n",
    "        # Decoder (Teacher Forcing)\n",
    "        dec_emb = self.embedding(dec_input)\n",
    "        dec_out, _ = self.decoder(dec_emb, (h, c))\n",
    "        \n",
    "        # Logits\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# Model ve cihaz\n",
    "# -----------------------------\n",
    "vocab_size = 10\n",
    "model = SimpleSeq2Seq(vocab_size=vocab_size).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# √ñrnek Forward Pass\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    \n",
    "    # Teacher forcing i√ßin decoder input: target_ids kaydƒ±rƒ±lmƒ±≈ü\n",
    "    dec_input = target_ids[:, :-1]\n",
    "    \n",
    "    # Forward\n",
    "    logits = model(input_ids, dec_input)  # [B, seq_len-1, vocab_size]\n",
    "    \n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f747f",
   "metadata": {},
   "source": [
    "### üîπ A√ßƒ±klamalar\n",
    "\n",
    "* Embedding & Encoder:\n",
    "\n",
    "input_ids √∂nce embedding katmanƒ±ndan ge√ßer.\n",
    "\n",
    "Encoder LSTM, hidden ve cell state √ºretir.\n",
    "\n",
    "* Decoder & Teacher Forcing:\n",
    "\n",
    "dec_input = target_ids[:, :-1] ‚Üí hedef diziyi bir adƒ±m kaydƒ±rarak veriyoruz.\n",
    "\n",
    "Decoder, hidden state‚Äôi encoder‚Äôdan alƒ±r.\n",
    "\n",
    "Bu ≈üekilde model, bir adƒ±m ileriyi tahmin etmeyi √∂ƒürenir.\n",
    "\n",
    "* Logits:\n",
    "\n",
    "Decoder √ßƒ±kƒ±≈üƒ± Linear ile vocab boyutuna d√∂n√º≈üt√ºr√ºl√ºr.\n",
    "\n",
    "Shape: [batch_size, seq_len-1, vocab_size]\n",
    "\n",
    "Loss hesaplamaya hazƒ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5199c13",
   "metadata": {},
   "source": [
    "---\n",
    "# 3Ô∏è‚É£: Loss Hesaplama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c32dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2987773418426514\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Loss fonksiyonu\n",
    "# -----------------------------\n",
    "PAD_TOKEN = 0  # padding token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# -----------------------------\n",
    "# √ñrnek batch ve logits\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    \n",
    "    dec_input = target_ids[:, :-1]  # decoder input (teacher forcing)\n",
    "    dec_target = target_ids[:, 1:]  # ger√ßek hedef\n",
    "    \n",
    "    # Forward Pass\n",
    "    logits = model(input_ids, dec_input)  # [B, seq_len-1, vocab_size]\n",
    "    \n",
    "    # Reshape logits ve target: [B*T, V] ve [B*T]\n",
    "    B, T, V = logits.shape\n",
    "    logits_flat = logits.reshape(B*T, V)\n",
    "    target_flat = dec_target.reshape(B*T)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(logits_flat, target_flat)\n",
    "    \n",
    "    print(\"Loss:\", loss.item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33e01e",
   "metadata": {},
   "source": [
    "### üîπ A√ßƒ±klamalar\n",
    "\n",
    "* Decoder Target:\n",
    "\n",
    "dec_target = target_ids[:, 1:] ‚Üí Teacher forcing i√ßin bir adƒ±m kaydƒ±rƒ±lmƒ±≈ü hedef.\n",
    "\n",
    "- Shape D√∂n√º≈ü√ºm√º:\n",
    "\n",
    "CrossEntropyLoss [N, C] ve [N] boyutunda bekler.\n",
    "\n",
    "[B, T, V] ‚Üí [B*T, V] ve [B, T] ‚Üí [B*T]\n",
    "\n",
    "- Padding Tokenlarƒ±:\n",
    "\n",
    "ignore_index=PAD_TOKEN sayesinde paddingler loss hesabƒ±na dahil edilmez.\n",
    "\n",
    "B√∂ylece model yalnƒ±zca ger√ßek tokenlar √ºzerinden √∂ƒürenir.\n",
    "\n",
    "- Loss Hazƒ±r:\n",
    "\n",
    "Bu loss artƒ±k backward pass i√ßin kullanƒ±labilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5408780",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce9336",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£: Backward Pass & Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314f6cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights for this batch, Loss: 2.2987773418426514\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Optimizer\n",
    "# -----------------------------\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# √ñrnek Backward Pass\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    \n",
    "    dec_input = target_ids[:, :-1]\n",
    "    dec_target = target_ids[:, 1:]\n",
    "    \n",
    "    # Forward\n",
    "    logits = model(input_ids, dec_input)\n",
    "    B, T, V = logits.shape\n",
    "    logits_flat = logits.reshape(B*T, V)\n",
    "    target_flat = dec_target.reshape(B*T)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(logits_flat, target_flat)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Backward & Optimize\n",
    "    # -----------------------------\n",
    "    optimizer.zero_grad()            # gradyanlarƒ± sƒ±fƒ±rla\n",
    "    loss.backward()                  # backward pass\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "    optimizer.step()                 # aƒüƒ±rlƒ±klarƒ± g√ºncelle\n",
    "    \n",
    "    print(\"Updated weights for this batch, Loss:\", loss.item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd24e7",
   "metadata": {},
   "source": [
    "### üîπ A√ßƒ±klamalar\n",
    "\n",
    "* Gradyan Sƒ±fƒ±rlama\n",
    "\n",
    "optimizer.zero_grad() ‚Üí √ñnceki batch‚Äôin gradyanlarƒ± temizlenir.\n",
    "\n",
    "* Backward Pass\n",
    "\n",
    "loss.backward() ‚Üí Model aƒüƒ± boyunca gradyanlar hesaplanƒ±r.\n",
    "\n",
    "* Gradient Clipping\n",
    "\n",
    "clip_grad_norm_ ile gradyan patlamasƒ± √∂nlenir.\n",
    "\n",
    "√ñzellikle LLM ve derin aƒülarda kritik.\n",
    "\n",
    "* Optimizer Step\n",
    "\n",
    "optimizer.step() ‚Üí Hesaplanan gradyanlar ile aƒüƒ±rlƒ±klar g√ºncellenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591a188",
   "metadata": {},
   "source": [
    "----\n",
    "# 5Ô∏è‚É£: Epoch D√∂ng√ºs√º ve Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d98a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Average Loss: 2.2899\n",
      "Epoch [2/3] Average Loss: 2.2810\n",
      "Epoch [3/3] Average Loss: 2.2722\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()                     # train moduna al\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids, dec_input)\n",
    "        B, T, V = logits.shape\n",
    "        logits_flat = logits.reshape(B*T, V)\n",
    "        target_flat = dec_target.reshape(B*T)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # Backward & Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3a3ec",
   "metadata": {},
   "source": [
    "### üîπ A√ßƒ±klamalar\n",
    "\n",
    "* Epoch D√∂ng√ºs√º\n",
    "\n",
    "for epoch in range(epochs) ‚Üí Model veriyi ka√ß kez g√∂recek.\n",
    "\n",
    "model.train() ‚Üí dropout, batchnorm gibi katmanlarƒ± training moduna alƒ±r.\n",
    "\n",
    "* Batch D√∂ng√ºs√º\n",
    "\n",
    "train_loader ile mini-batch‚Äôler i≈ülenir.\n",
    "\n",
    "Forward, loss, backward ve optimize adƒ±mlarƒ± batch ba≈üƒ±na uygulanƒ±r.\n",
    "\n",
    "* Loss Biriktirme & Ortalama\n",
    "\n",
    "total_loss += loss.item() ‚Üí Batch‚Äôten batch‚Äôe toplam loss birikir.\n",
    "\n",
    "avg_loss = total_loss / len(train_loader) ‚Üí epoch sonunda ortalama loss yazdƒ±rƒ±lƒ±r.\n",
    "\n",
    "* Logging\n",
    "\n",
    "Basit print ile loss g√∂zlemlenebilir.\n",
    "\n",
    "ƒ∞leri seviye: tqdm veya tensorboard ile g√∂rselle≈ütirilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec68b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade6cf9",
   "metadata": {},
   "source": [
    "## KODUN D√úZENLENMƒ∞≈û VE UYARLANMI≈û HALƒ∞ ;\n",
    "```python\n",
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop (D√ºzenli)\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()                     # Training moduna al\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # -------------------------\n",
    "        # 1Ô∏è‚É£ Veri ve batch hazƒ±rlƒ±ƒüƒ±\n",
    "        # -------------------------\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]   # teacher forcing i√ßin\n",
    "        dec_target = target_ids[:, 1:]   # ger√ßek hedef\n",
    "        \n",
    "        # -------------------------\n",
    "        # 2Ô∏è‚É£ Forward Pass\n",
    "        # -------------------------\n",
    "        logits = model(input_ids, dec_input)  # [B, seq_len-1, vocab_size]\n",
    "        \n",
    "        # -------------------------\n",
    "        # 3Ô∏è‚É£ Loss Hesaplama\n",
    "        # -------------------------\n",
    "        B, T, V = logits.shape\n",
    "        logits_flat = logits.reshape(B*T, V)\n",
    "        target_flat = dec_target.reshape(B*T)\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # -------------------------\n",
    "        # 4Ô∏è‚É£ Backward Pass & Optimize\n",
    "        # -------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # -------------------------\n",
    "        # 5Ô∏è‚É£ Logging\n",
    "        # -------------------------\n",
    "        total_loss += loss.item()\n",
    "        if (batch_idx + 1) % 1 == 0:  # Batch bazlƒ± print\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56489f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd98fa1",
   "metadata": {},
   "source": [
    "## Teacher Forcing'i hi√ß duydunuz mu ? Nedir Teacher Forcing ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed58b20",
   "metadata": {},
   "source": [
    "----\n",
    "# üîπ Teacher Forcing Nedir?\n",
    "\n",
    "### Tanƒ±m:\n",
    "* Decoder‚Äôƒ± eƒüitirken, modelin kendi √∂nceki tahminlerini kullanmak yerine ger√ßek hedef tokenlarƒ± bir sonraki adƒ±m i√ßin girdi olarak vermek y√∂ntemidir.\n",
    "\n",
    "### Ama√ß:\n",
    "\n",
    "* Uzun dizilerde hata birikimini √∂nlemek\n",
    "\n",
    "* √ñƒürenmeyi hƒ±zlandƒ±rmak\n",
    "\n",
    "* Modelin doƒüru diziyi daha hƒ±zlƒ± √∂ƒürenmesini saƒülamak\n",
    "\n",
    "## üîπ Nasƒ±l √áalƒ±≈üƒ±r?\n",
    "\n",
    "**Normal seq2seq tahmininde:**\n",
    "\n",
    "> Tahmin_0 -> Tahmin_1 -> Tahmin_2 -> ...\n",
    "\n",
    "\n",
    "* Her adƒ±mda decoder, √∂nceki tahminini kullanƒ±r.\n",
    "\n",
    "* Eƒüer model yanlƒ±≈ü bir tahmin yaparsa, hata birikir ve sonraki adƒ±mlar da yanlƒ±≈ü olur.\n",
    "\n",
    "**Teacher forcing kullanƒ±ldƒ±ƒüƒ±nda:**\n",
    "\n",
    " > Ger√ßek_0 -> Ger√ßek_1 -> Ger√ßek_2 -> ...\n",
    "\n",
    "\n",
    "* Decoder‚Äôa bir adƒ±m kaydƒ±rƒ±lmƒ±≈ü ger√ßek hedef token verilir.\n",
    "\n",
    "* B√∂ylece model her adƒ±mda doƒüru baƒülamƒ± g√∂r√ºr."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13bdb4",
   "metadata": {},
   "source": [
    "## üîπ Kod √ñrneƒüi\n",
    "```python\n",
    "dec_input = target_ids[:, :-1]  # Teacher forcing: bir adƒ±m kaydƒ±rƒ±lmƒ±≈ü hedef\n",
    "dec_target = target_ids[:, 1:]  # Loss hesaplamak i√ßin ger√ßek hedef\n",
    "\n",
    "logits = model(input_ids, dec_input)\n",
    "```\n",
    "\n",
    "\n",
    "* dec_input ‚Üí Decoder‚Äôa verilecek giri≈ü (teacher forcing uygulanmƒ±≈ü)\n",
    "\n",
    "* dec_target ‚Üí Loss hesaplamak i√ßin ger√ßek hedef tokenlar\n",
    "\n",
    "* Model, dec_input √ºzerinden tahmin yapar ve dec_target ile kar≈üƒ±la≈ütƒ±rƒ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7afad2",
   "metadata": {},
   "source": [
    "## üîπ Avantajlarƒ± ve Dezavantajlarƒ±\n",
    "### Avantajlar:\n",
    "\n",
    "* Hata birikimi azalƒ±r\n",
    "\n",
    "* Eƒüitim daha hƒ±zlƒ± ve stabil\n",
    "\n",
    "* √ñzellikle uzun dizilerde √ßok i≈üe yarar\n",
    "\n",
    "### Dezavantajlar:\n",
    "\n",
    "* Train-test farkƒ± yaratabilir:\n",
    "\n",
    "* Test sƒ±rasƒ±nda model kendi tahminlerini kullanƒ±r.\n",
    "\n",
    "* Bu nedenle eƒüitimde g√∂rd√ºƒü√º baƒülam ile testteki baƒülam farklƒ± olabilir.\n",
    "\n",
    "#### **√á√∂z√ºm: Scheduled sampling veya kƒ±smi teacher forcing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53777bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True  # True = teacher forcing, False = kendi tahminini kullan\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # 1Ô∏è‚É£ Veri & batch hazƒ±rlƒ±ƒüƒ±\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        dec_input = target_ids[:, :-1]    # teacher forcing i√ßin\n",
    "        dec_target = target_ids[:, 1:]    # ger√ßek hedef\n",
    "        \n",
    "        # 2Ô∏è‚É£ Forward Pass (teacher forcing opsiyonel)\n",
    "        logits = model(input_ids, dec_input=dec_input, target_ids=target_ids,\n",
    "                       teacher_forcing=teacher_forcing)\n",
    "        \n",
    "        # 3Ô∏è‚É£ Loss Hesaplama\n",
    "        B, T, V = logits.shape\n",
    "        logits_flat = logits.reshape(B*T, V)\n",
    "        target_flat = dec_target.reshape(B*T)\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # 4Ô∏è‚É£ Backward & Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5Ô∏è‚É£ Logging\n",
    "        total_loss += loss.item()\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3ba50",
   "metadata": {},
   "source": [
    "----\n",
    "# ≈ûu ana kadar bir train i≈üleminin a≈üamalarƒ±nƒ± ve temelden i≈üleyi≈üini g√∂rd√ºk.Sƒ±ra bu i≈üleyi≈üi daha optimize ve daha maliyetsiz hale getirmek.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5befd",
   "metadata": {},
   "source": [
    "## üîπ AMP (Automatic Mixed Precision) Nedir?\n",
    "\n",
    "Ama√ß:\n",
    "\n",
    "* Modelin bazƒ± hesaplamalarƒ±nƒ± float16 (half precision) ile yapƒ±p, diƒüerlerini float32 olarak tutmak\n",
    "\n",
    "* GPU belleƒüini ve hesaplama s√ºresini optimize etmek\n",
    "\n",
    "Nasƒ±l √ßalƒ±≈üƒ±r:\n",
    "\n",
    "* Forward ve backward pass‚Äôte torch.cuda.amp.autocast() kullanƒ±lƒ±r\n",
    "\n",
    "* Gradient update sƒ±rasƒ±nda torch.cuda.amp.GradScaler ile gradyanlar √∂l√ßeklenir\n",
    "\n",
    "## üîπ GPU Optimizasyonu\n",
    "\n",
    "* Cihaz se√ßimi: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "* T√ºm tensor ve model GPU‚Äôya ta≈üƒ±nƒ±r: .to(device)\n",
    "\n",
    "* Batch size ve precision GPU‚Äôya uygun ayarlanƒ±r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06370ab4",
   "metadata": {},
   "source": [
    "# üîπ AMP ile D√ºzenlenmi≈ü Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()  # Gradient scaler\n",
    "\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Forward + Loss (AMP ile)\n",
    "        # -----------------------------\n",
    "        with autocast():  # mixed precision context\n",
    "            logits = model(input_ids, dec_input=dec_input,\n",
    "                           target_ids=target_ids,\n",
    "                           teacher_forcing=teacher_forcing)\n",
    "            B, T, V = logits.shape\n",
    "            logits_flat = logits.reshape(B*T, V)\n",
    "            target_flat = dec_target.reshape(B*T)\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Backward + Optimize (AMP ile)\n",
    "        # -----------------------------\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecf290",
   "metadata": {},
   "source": [
    "### üîπ A√ßƒ±klamalar\n",
    "\n",
    "#### **autocast()**\n",
    "\n",
    "* Forward ve loss hesaplamasƒ±nƒ± float16/32 karma olarak yapar\n",
    "\n",
    "* Hƒ±z ve VRAM tasarrufu saƒülar\n",
    "\n",
    "#### **GradScaler**\n",
    "\n",
    "* K√º√ß√ºk gradyanlarƒ± √∂l√ßekleyerek underflow‚Äôu √∂nler\n",
    "\n",
    "- scaler.scale(loss).backward() ve scaler.step(optimizer) ile birlikte √ßalƒ±≈üƒ±r\n",
    "\n",
    "#### **GPU kullanƒ±mƒ±**\n",
    "\n",
    "* Model ve t√ºm tensorlar .to(device) ile GPU‚Äôya ta≈üƒ±nƒ±r\n",
    "\n",
    "* B√ºy√ºk batch‚Äôlerde eƒüitim verimli olur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd4d4a",
   "metadata": {},
   "source": [
    "---\n",
    "# Bir sonraki adƒ±m olarak gradient accumulation ve scheduler ekleyip, b√ºy√ºk batch‚Äôleri k√º√ß√ºk GPU belleƒüi ile √ßalƒ±≈ütƒ±rabilecek ≈üekilde loop‚Äôu geli≈ütireceƒüiz.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143542a2",
   "metadata": {},
   "source": [
    "## üîπ Gradient Accumulation Nedir?\n",
    "\n",
    "#### **Ama√ß:**\n",
    "\n",
    "* K√º√ß√ºk GPU belleƒüi olan makinelerde, b√ºy√ºk batch‚Äôi par√ßalara b√∂lerek i≈ülem yapmak\n",
    "\n",
    "* Her mini-batch i√ßin backward yapƒ±p, birka√ß mini-batch sonra optimizer step atmak\n",
    "\n",
    "#### **Nasƒ±l √ßalƒ±≈üƒ±r:**\n",
    "\n",
    "* accumulation_steps = N ‚Üí N mini-batch biriktirilir\n",
    "\n",
    "* loss.backward() her mini-batch i√ßin yapƒ±lƒ±r\n",
    "\n",
    "* optimizer.step() ve scaler.update() sadece N adƒ±mda bir yapƒ±lƒ±r\n",
    "\n",
    "## üîπ Scheduler Nedir?\n",
    "\n",
    "* √ñƒürenme oranƒ±nƒ± dinamik olarak ayarlayan mekanizma\n",
    "\n",
    "* √ñrnek: StepLR, CosineAnnealingLR, OneCycleLR\n",
    "\n",
    "* √ñzellikle LLM‚Äôlerde stabil eƒüitim i√ßin √∂nemlidir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee11d9d",
   "metadata": {},
   "source": [
    "# üîπ Optimized Train Loop (AMP + Gradient Accumulation + Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2698f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# -----------------------------\n",
    "# Parametreler\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True\n",
    "accumulation_steps = 2  # 2 mini-batch biriktir\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)  # √∂rnek\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(input_ids, dec_input=dec_input,\n",
    "                           target_ids=target_ids,\n",
    "                           teacher_forcing=teacher_forcing)\n",
    "            B, T, V = logits.shape\n",
    "            logits_flat = logits.reshape(B*T, V)\n",
    "            target_flat = dec_target.reshape(B*T)\n",
    "            loss = criterion(logits_flat, target_flat) / accumulation_steps  # loss scaling\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item() * accumulation_steps  # orijinal loss\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()  # LR update\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item()*accumulation_steps:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4f570",
   "metadata": {},
   "source": [
    "## üîπ A√ßƒ±klamalar\n",
    "\n",
    "### **Loss scaling**\n",
    "\n",
    "* loss / accumulation_steps ‚Üí gradyanlar birikince toplam loss doƒüru olur\n",
    "\n",
    "### **Accumulation**\n",
    "\n",
    "* if (batch_idx + 1) % accumulation_steps == 0 ‚Üí optimizer ve scaler update\n",
    "\n",
    "### **Scheduler**\n",
    "\n",
    "* scheduler.step() ‚Üí learning rate her accumulation step veya batch sonunda g√ºncellenebilir\n",
    "\n",
    "* AMP + GPU\n",
    "\n",
    "* autocast() ve GradScaler() ile VRAM ve hƒ±z optimizasyonu devam eder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb449b7",
   "metadata": {},
   "source": [
    "---\n",
    "# O zaman ≈üimdi Top-K ve Top-P sampling destekli inference kƒ±smƒ±nƒ± ekleyelim. Bu, modelin eƒüitildikten sonra √ße≈üitli ve mantƒ±klƒ± √ßƒ±ktƒ±lar √ºretmesini saƒülar.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcaf9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Minik Seq2Seq Model (Test i√ßin)\n",
    "# -----------------------------\n",
    "class MiniSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, enc_input, dec_input=None, target_ids=None, teacher_forcing=True, max_len=20):\n",
    "        enc_emb = self.embedding(enc_input)\n",
    "        _, (h, c) = self.encoder(enc_emb)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Teacher forcing a√ßƒ±k\n",
    "        # -----------------------------\n",
    "        if teacher_forcing and dec_input is not None:\n",
    "            dec_emb = self.embedding(dec_input)\n",
    "            dec_out, _ = self.decoder(dec_emb, (h, c))\n",
    "            logits = self.fc(dec_out)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Teacher forcing kapalƒ± (inference)\n",
    "        # -----------------------------\n",
    "        else:\n",
    "            if target_ids is not None:\n",
    "                start_token = target_ids[:, 0].unsqueeze(1)\n",
    "            else:\n",
    "                start_token = enc_input[:, -1].unsqueeze(1)  # son token veya <BOS>\n",
    "            \n",
    "            inputs = start_token\n",
    "            outputs = []\n",
    "            hidden = (h, c)\n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                emb = self.embedding(inputs)\n",
    "                out, hidden = self.decoder(emb, hidden)\n",
    "                logit = self.fc(out)\n",
    "                outputs.append(logit)\n",
    "                inputs = logit.argmax(-1)  # kendi tahminini input olarak kullan\n",
    "            \n",
    "            logits = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model, Device ve Test\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MiniSeq2Seq(vocab_size=20).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c8651",
   "metadata": {},
   "source": [
    "## üîπ Sampling (Inference) Fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3795cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, input_ids, max_len=20, top_k=50, top_p=0.9, temperature=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Top-K / Top-P sampling ile token √ºretme\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    generated = input_ids  # ba≈ülangƒ±√ß token\n",
    "    B = input_ids.size(0)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # decoder kendi tahminini input olarak kullanacak, teacher forcing kapalƒ±\n",
    "        logits = model(generated, dec_input=None, target_ids=None, teacher_forcing=False, max_len=1)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Top-K Sampling\n",
    "        # -----------------------------\n",
    "        if top_k > 0:\n",
    "            top_k_vals, top_k_idx = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.zeros_like(next_token_logits).scatter_(-1, top_k_idx, F.softmax(top_k_vals, dim=-1))\n",
    "        else:\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Top-P Sampling\n",
    "        # -----------------------------\n",
    "        if top_p < 1.0:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            sorted_idx_to_remove = cumulative_probs > top_p\n",
    "            sorted_probs[sorted_idx_to_remove] = 0\n",
    "            sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            probs = torch.zeros_like(probs).scatter_(-1, sorted_idx, sorted_probs)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Token se√ßimi\n",
    "        # -----------------------------\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fdc7b",
   "metadata": {},
   "source": [
    "## üîπ A√ßƒ±klamalar\n",
    "\n",
    "### > teacher_forcing=False\n",
    "\n",
    "* Inference sƒ±rasƒ±nda model kendi tahminini bir sonraki input olarak kullanƒ±r\n",
    "\n",
    "### > Temperature\n",
    "\n",
    "* temperature < 1 ‚Üí daƒüƒ±lƒ±m keskinle≈üir ‚Üí deterministik\n",
    "\n",
    "* temperature > 1 ‚Üí daƒüƒ±lƒ±m d√ºzle≈üir ‚Üí daha √ße≈üitli √ßƒ±ktƒ±lar\n",
    "\n",
    "### > Top-K Sampling\n",
    "\n",
    "* En y√ºksek K olasƒ±lƒ±klƒ± tokenlar arasƒ±ndan rastgele se√ßim\n",
    "\n",
    "### > Top-P (Nucleus) Sampling\n",
    "\n",
    "* K√ºm√ºlatif olasƒ±lƒ±ƒüƒ± P olan token setinden rastgele se√ßim\n",
    "\n",
    "* Dinamik ve mantƒ±klƒ± token se√ßimi saƒülar\n",
    "\n",
    "### > Multinomial\n",
    "\n",
    "* Token olasƒ±lƒ±klarƒ±na g√∂re rastgele se√ßim yapƒ±lƒ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df122cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence (Top-K): [[1, 2, 3, 10, 2, 18, 3, 10, 3, 10, 12]]\n",
      "Generated sequence (Top-P): [[1, 4, 5, 4, 12, 18, 7, 7, 3, 18, 6]]\n"
     ]
    }
   ],
   "source": [
    "start_token = torch.tensor([[1]])  # batch size=1, start token\n",
    "\n",
    "generated_topk = generate(model, start_token, max_len=10, top_k=5, top_p=1.0, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-K):\", generated_topk.tolist())\n",
    "\n",
    "generated_topp = generate(model, start_token, max_len=10, top_k=0, top_p=0.9, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-P):\", generated_topp.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a780b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05229351",
   "metadata": {},
   "source": [
    "# üîπ Minik Tokenizer √ñrneƒüi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a787b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Mini tokenizer\n",
    "# -----------------------------\n",
    "id2token = {i: f\"tok{i}\" for i in range(20)}  # token ID ‚Üí string\n",
    "token2id = {v:k for k,v in id2token.items()}\n",
    "\n",
    "def decode(token_ids):\n",
    "    \"\"\"\n",
    "    Token ID listesini string h√¢line √ßevirir\n",
    "    \"\"\"\n",
    "    if isinstance(token_ids, torch.Tensor):\n",
    "        token_ids = token_ids.tolist()\n",
    "    # batch support: batch_size x seq_len\n",
    "    if isinstance(token_ids[0], list):\n",
    "        return [\" \".join(id2token[tok] for tok in seq) for seq in token_ids]\n",
    "    else:\n",
    "        return \" \".join(id2token[tok] for tok in token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4c77f",
   "metadata": {},
   "source": [
    "## üîπ Test: Top-K ve Top-P Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b0a085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence (Top-K IDs): [[1, 3, 18, 2, 3, 7, 16, 18, 10, 18, 3]]\n",
      "Generated sequence (Top-K Decoded): ['tok1 tok3 tok18 tok2 tok3 tok7 tok16 tok18 tok10 tok18 tok3']\n",
      "Generated sequence (Top-P IDs): [[1, 8, 17, 18, 12, 12, 10, 8, 12, 3, 6]]\n",
      "Generated sequence (Top-P Decoded): ['tok1 tok8 tok17 tok18 tok12 tok12 tok10 tok8 tok12 tok3 tok6']\n"
     ]
    }
   ],
   "source": [
    "# √ñrnek start token\n",
    "start_token = torch.tensor([[1]])\n",
    "\n",
    "# Top-K sampling\n",
    "generated_topk = generate(model, start_token, max_len=10, top_k=5, top_p=1.0, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-K IDs):\", generated_topk.tolist())\n",
    "print(\"Generated sequence (Top-K Decoded):\", decode(generated_topk))\n",
    "\n",
    "# Top-P sampling\n",
    "generated_topp = generate(model, start_token, max_len=10, top_k=0, top_p=0.9, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-P IDs):\", generated_topp.tolist())\n",
    "print(\"Generated sequence (Top-P Decoded):\", decode(generated_topp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc95ae9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9947d",
   "metadata": {},
   "source": [
    "## Yukarƒ±da anlatƒ±lan i≈ülemler doƒürultusunda olu≈üturulan \"TRAƒ∞N_LOOP\" a≈üaƒüƒ±daki gibidir ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9274de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# -----------------------------\n",
    "# Parametreler\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True\n",
    "accumulation_steps = 2  # b√ºy√ºk batch sim√ºlasyonu\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# -----------------------------\n",
    "# Model, Optimizer, Scaler, Scheduler\n",
    "# -----------------------------\n",
    "model = MiniSeq2Seq(vocab_size=20).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)  # √∂rnek scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):  # train_loader = DataLoader objesi\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Forward + Loss (AMP)\n",
    "        # -----------------------------\n",
    "        with autocast():\n",
    "            logits = model(input_ids, dec_input=dec_input,\n",
    "                           target_ids=target_ids,\n",
    "                           teacher_forcing=teacher_forcing)\n",
    "            \n",
    "            B, T, V = logits.shape\n",
    "            logits_flat = logits.reshape(B*T, V)\n",
    "            target_flat = dec_target.reshape(B*T)\n",
    "            loss = criterion(logits_flat, target_flat) / accumulation_steps  # scaled for accumulation\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Backward (AMP + Grad Accumulation)\n",
    "        # -----------------------------\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item()*accumulation_steps:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ccbd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence (Top-K IDs): [[1, 2, 4, 7, 7, 0, 19, 7, 12, 16, 3]]\n",
      "Generated sequence (Top-K Decoded): ['tok1 tok2 tok4 tok7 tok7 tok0 tok19 tok7 tok12 tok16 tok3']\n",
      "Generated sequence (Top-P IDs): [[1, 19, 5, 2, 11, 9, 18, 10, 8, 12, 12]]\n",
      "Generated sequence (Top-P Decoded): ['tok1 tok19 tok5 tok2 tok11 tok9 tok18 tok10 tok8 tok12 tok12']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# √ñrnek start token\n",
    "# -----------------------------\n",
    "start_token = torch.tensor([[1]]).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Top-K Sampling\n",
    "# -----------------------------\n",
    "generated_topk = generate(model, start_token, max_len=10, top_k=5, top_p=1.0, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-K IDs):\", generated_topk.tolist())\n",
    "print(\"Generated sequence (Top-K Decoded):\", decode(generated_topk))\n",
    "\n",
    "# -----------------------------\n",
    "# Top-P (Nucleus) Sampling\n",
    "# -----------------------------\n",
    "generated_topp = generate(model, start_token, max_len=10, top_k=0, top_p=0.9, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-P IDs):\", generated_topp.tolist())\n",
    "print(\"Generated sequence (Top-P Decoded):\", decode(generated_topp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f481045",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
