{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23803e2",
   "metadata": {},
   "source": [
    "# 🔹 LLM Train Loop: Adım Adım Kod Akışı\n",
    "\n",
    "LLM train loop’u adım adım inşa edeceğiz ve her adımda iyileştirmeler ekleyeceğiz. Teorideki 5 temel adımı kod seviyesinde ele alacağız.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Veri ve Batch Hazırlığı\n",
    "- Dataset ve DataLoader oluşturma\n",
    "- Veriyi GPU/CPU cihazına taşıma\n",
    "- Padding ve attention mask kontrolleri\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Forward Pass\n",
    "- Modeli çağırma\n",
    "- Teacher forcing uygulanması (decoder kullanılıyorsa)\n",
    "- Çıkış (`logits`) boyut kontrolü\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Loss Hesaplama\n",
    "- CrossEntropyLoss (veya label smoothing)\n",
    "- Padding tokenlarını ignore etme\n",
    "- Token boyutlarını reshape etme (`[B*T, V]` ve `[B*T]`)\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Backward Pass & Optimize\n",
    "- `loss.backward()`\n",
    "- Gradient clipping\n",
    "- `optimizer.step()`\n",
    "\n",
    "---\n",
    "\n",
    "## 5️⃣ Epoch Döngüsü ve Logging\n",
    "- Toplam loss biriktirme\n",
    "- Ortalama loss hesaplama\n",
    "- Opsiyonel: basit progress bar veya logging\n",
    "\n",
    "---\n",
    "\n",
    "## 6️⃣ İyileştirmeler (Opsiyonel)\n",
    "- Mixed precision (AMP) kullanımı\n",
    "- Gradient accumulation\n",
    "- Learning rate scheduler\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 Not: Önce temel döngüyü çalışır hâle getireceğiz, ardından isteğe bağlı iyileştirmeleri ekleyeceğiz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e4540",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5896071",
   "metadata": {},
   "source": [
    "# 1️⃣: Veri ve Batch Hazırlığı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a836981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[5, 6, 7, 0, 0, 0],\n",
      "        [1, 2, 3, 4, 0, 0]], device='cuda:0')\n",
      "Target IDs: tensor([[1, 2, 3, 4, 0, 0],\n",
      "        [1, 2, 3, 4, 5, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# -----------------------------\n",
    "# Örnek Dataset\n",
    "# -----------------------------\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, enc_inputs, dec_targets):\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_targets = dec_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.enc_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.enc_inputs[idx], dtype=torch.long),\n",
    "            'target_ids': torch.tensor(self.dec_targets[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# Örnek veri\n",
    "# -----------------------------\n",
    "enc_inputs = [[1,2,3,4,0,0],[5,6,7,0,0,0]]   # 0 = PAD token\n",
    "dec_targets = [[1,2,3,4,5,0],[1,2,3,4,0,0]]\n",
    "\n",
    "# Dataset & DataLoader\n",
    "dataset = MyDataset(enc_inputs, dec_targets)\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# -----------------------------\n",
    "# GPU/CPU cihazı\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Örnek batch kullanımı\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)       # [B, seq_len]\n",
    "    target_ids = batch['target_ids'].to(device)     # [B, seq_len]\n",
    "    \n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Target IDs:\", target_ids)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddeb73",
   "metadata": {},
   "source": [
    "### 🔹 Açıklamalar\n",
    "\n",
    "* Dataset ve DataLoader:\n",
    "\n",
    "MyDataset sınıfı, encoder girişleri (input_ids) ve decoder hedeflerini (target_ids) alıyor.\n",
    "\n",
    "DataLoader mini-batch oluşturur ve shuffle ile rastgele sırayla verir.\n",
    "\n",
    "* Cihaz (device) seçimi:\n",
    "\n",
    "torch.device ile GPU varsa oraya, yoksa CPU’ya taşır.\n",
    "\n",
    "* Batch çekme:\n",
    "\n",
    "Her iterasyonda input_ids ve target_ids batch olarak alınır.\n",
    "\n",
    "Modelin forward pass’ine hazır hâle gelir.\n",
    "\n",
    "* Padding kontrolü:\n",
    "\n",
    "Örnek veride 0 PAD token olarak kullanıldı.\n",
    "\n",
    "Daha sonra loss hesaplamada ignore edilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3a4da",
   "metadata": {},
   "source": [
    "---\n",
    "# 2️⃣: Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806f3d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Örnek Model (Encoder-Decoder)\n",
    "# -----------------------------\n",
    "class SimpleSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size=10, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # Encoder\n",
    "        enc_emb = self.embedding(enc_input)\n",
    "        _, (h, c) = self.encoder(enc_emb)\n",
    "        \n",
    "        # Decoder (Teacher Forcing)\n",
    "        dec_emb = self.embedding(dec_input)\n",
    "        dec_out, _ = self.decoder(dec_emb, (h, c))\n",
    "        \n",
    "        # Logits\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# Model ve cihaz\n",
    "# -----------------------------\n",
    "vocab_size = 10\n",
    "model = SimpleSeq2Seq(vocab_size=vocab_size).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Örnek Forward Pass\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    \n",
    "    # Teacher forcing için decoder input: target_ids kaydırılmış\n",
    "    dec_input = target_ids[:, :-1]\n",
    "    \n",
    "    # Forward\n",
    "    logits = model(input_ids, dec_input)  # [B, seq_len-1, vocab_size]\n",
    "    \n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f747f",
   "metadata": {},
   "source": [
    "### 🔹 Açıklamalar\n",
    "\n",
    "* Embedding & Encoder:\n",
    "\n",
    "input_ids önce embedding katmanından geçer.\n",
    "\n",
    "Encoder LSTM, hidden ve cell state üretir.\n",
    "\n",
    "* Decoder & Teacher Forcing:\n",
    "\n",
    "dec_input = target_ids[:, :-1] → hedef diziyi bir adım kaydırarak veriyoruz.\n",
    "\n",
    "Decoder, hidden state’i encoder’dan alır.\n",
    "\n",
    "Bu şekilde model, bir adım ileriyi tahmin etmeyi öğrenir.\n",
    "\n",
    "* Logits:\n",
    "\n",
    "Decoder çıkışı Linear ile vocab boyutuna dönüştürülür.\n",
    "\n",
    "Shape: [batch_size, seq_len-1, vocab_size]\n",
    "\n",
    "Loss hesaplamaya hazır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5199c13",
   "metadata": {},
   "source": [
    "---\n",
    "# 3️⃣: Loss Hesaplama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c32dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2987773418426514\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Loss fonksiyonu\n",
    "# -----------------------------\n",
    "PAD_TOKEN = 0  # padding token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# -----------------------------\n",
    "# Örnek batch ve logits\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    \n",
    "    dec_input = target_ids[:, :-1]  # decoder input (teacher forcing)\n",
    "    dec_target = target_ids[:, 1:]  # gerçek hedef\n",
    "    \n",
    "    # Forward Pass\n",
    "    logits = model(input_ids, dec_input)  # [B, seq_len-1, vocab_size]\n",
    "    \n",
    "    # Reshape logits ve target: [B*T, V] ve [B*T]\n",
    "    B, T, V = logits.shape\n",
    "    logits_flat = logits.reshape(B*T, V)\n",
    "    target_flat = dec_target.reshape(B*T)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(logits_flat, target_flat)\n",
    "    \n",
    "    print(\"Loss:\", loss.item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f33e01e",
   "metadata": {},
   "source": [
    "### 🔹 Açıklamalar\n",
    "\n",
    "* Decoder Target:\n",
    "\n",
    "dec_target = target_ids[:, 1:] → Teacher forcing için bir adım kaydırılmış hedef.\n",
    "\n",
    "- Shape Dönüşümü:\n",
    "\n",
    "CrossEntropyLoss [N, C] ve [N] boyutunda bekler.\n",
    "\n",
    "[B, T, V] → [B*T, V] ve [B, T] → [B*T]\n",
    "\n",
    "- Padding Tokenları:\n",
    "\n",
    "ignore_index=PAD_TOKEN sayesinde paddingler loss hesabına dahil edilmez.\n",
    "\n",
    "Böylece model yalnızca gerçek tokenlar üzerinden öğrenir.\n",
    "\n",
    "- Loss Hazır:\n",
    "\n",
    "Bu loss artık backward pass için kullanılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5408780",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce9336",
   "metadata": {},
   "source": [
    "# 4️⃣: Backward Pass & Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314f6cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights for this batch, Loss: 2.2987773418426514\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Optimizer\n",
    "# -----------------------------\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# Örnek Backward Pass\n",
    "# -----------------------------\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "    \n",
    "    dec_input = target_ids[:, :-1]\n",
    "    dec_target = target_ids[:, 1:]\n",
    "    \n",
    "    # Forward\n",
    "    logits = model(input_ids, dec_input)\n",
    "    B, T, V = logits.shape\n",
    "    logits_flat = logits.reshape(B*T, V)\n",
    "    target_flat = dec_target.reshape(B*T)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(logits_flat, target_flat)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Backward & Optimize\n",
    "    # -----------------------------\n",
    "    optimizer.zero_grad()            # gradyanları sıfırla\n",
    "    loss.backward()                  # backward pass\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "    optimizer.step()                 # ağırlıkları güncelle\n",
    "    \n",
    "    print(\"Updated weights for this batch, Loss:\", loss.item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd24e7",
   "metadata": {},
   "source": [
    "### 🔹 Açıklamalar\n",
    "\n",
    "* Gradyan Sıfırlama\n",
    "\n",
    "optimizer.zero_grad() → Önceki batch’in gradyanları temizlenir.\n",
    "\n",
    "* Backward Pass\n",
    "\n",
    "loss.backward() → Model ağı boyunca gradyanlar hesaplanır.\n",
    "\n",
    "* Gradient Clipping\n",
    "\n",
    "clip_grad_norm_ ile gradyan patlaması önlenir.\n",
    "\n",
    "Özellikle LLM ve derin ağlarda kritik.\n",
    "\n",
    "* Optimizer Step\n",
    "\n",
    "optimizer.step() → Hesaplanan gradyanlar ile ağırlıklar güncellenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591a188",
   "metadata": {},
   "source": [
    "----\n",
    "# 5️⃣: Epoch Döngüsü ve Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d98a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Average Loss: 2.2899\n",
      "Epoch [2/3] Average Loss: 2.2810\n",
      "Epoch [3/3] Average Loss: 2.2722\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()                     # train moduna al\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids, dec_input)\n",
    "        B, T, V = logits.shape\n",
    "        logits_flat = logits.reshape(B*T, V)\n",
    "        target_flat = dec_target.reshape(B*T)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # Backward & Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3a3ec",
   "metadata": {},
   "source": [
    "### 🔹 Açıklamalar\n",
    "\n",
    "* Epoch Döngüsü\n",
    "\n",
    "for epoch in range(epochs) → Model veriyi kaç kez görecek.\n",
    "\n",
    "model.train() → dropout, batchnorm gibi katmanları training moduna alır.\n",
    "\n",
    "* Batch Döngüsü\n",
    "\n",
    "train_loader ile mini-batch’ler işlenir.\n",
    "\n",
    "Forward, loss, backward ve optimize adımları batch başına uygulanır.\n",
    "\n",
    "* Loss Biriktirme & Ortalama\n",
    "\n",
    "total_loss += loss.item() → Batch’ten batch’e toplam loss birikir.\n",
    "\n",
    "avg_loss = total_loss / len(train_loader) → epoch sonunda ortalama loss yazdırılır.\n",
    "\n",
    "* Logging\n",
    "\n",
    "Basit print ile loss gözlemlenebilir.\n",
    "\n",
    "İleri seviye: tqdm veya tensorboard ile görselleştirilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec68b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade6cf9",
   "metadata": {},
   "source": [
    "## KODUN DÜZENLENMİŞ VE UYARLANMIŞ HALİ ;\n",
    "```python\n",
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop (Düzenli)\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()                     # Training moduna al\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # -------------------------\n",
    "        # 1️⃣ Veri ve batch hazırlığı\n",
    "        # -------------------------\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]   # teacher forcing için\n",
    "        dec_target = target_ids[:, 1:]   # gerçek hedef\n",
    "        \n",
    "        # -------------------------\n",
    "        # 2️⃣ Forward Pass\n",
    "        # -------------------------\n",
    "        logits = model(input_ids, dec_input)  # [B, seq_len-1, vocab_size]\n",
    "        \n",
    "        # -------------------------\n",
    "        # 3️⃣ Loss Hesaplama\n",
    "        # -------------------------\n",
    "        B, T, V = logits.shape\n",
    "        logits_flat = logits.reshape(B*T, V)\n",
    "        target_flat = dec_target.reshape(B*T)\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # -------------------------\n",
    "        # 4️⃣ Backward Pass & Optimize\n",
    "        # -------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # -------------------------\n",
    "        # 5️⃣ Logging\n",
    "        # -------------------------\n",
    "        total_loss += loss.item()\n",
    "        if (batch_idx + 1) % 1 == 0:  # Batch bazlı print\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56489f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd98fa1",
   "metadata": {},
   "source": [
    "## Teacher Forcing'i hiç duydunuz mu ? Nedir Teacher Forcing ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed58b20",
   "metadata": {},
   "source": [
    "----\n",
    "# 🔹 Teacher Forcing Nedir?\n",
    "\n",
    "### Tanım:\n",
    "* Decoder’ı eğitirken, modelin kendi önceki tahminlerini kullanmak yerine gerçek hedef tokenları bir sonraki adım için girdi olarak vermek yöntemidir.\n",
    "\n",
    "### Amaç:\n",
    "\n",
    "* Uzun dizilerde hata birikimini önlemek\n",
    "\n",
    "* Öğrenmeyi hızlandırmak\n",
    "\n",
    "* Modelin doğru diziyi daha hızlı öğrenmesini sağlamak\n",
    "\n",
    "## 🔹 Nasıl Çalışır?\n",
    "\n",
    "**Normal seq2seq tahmininde:**\n",
    "\n",
    "> Tahmin_0 -> Tahmin_1 -> Tahmin_2 -> ...\n",
    "\n",
    "\n",
    "* Her adımda decoder, önceki tahminini kullanır.\n",
    "\n",
    "* Eğer model yanlış bir tahmin yaparsa, hata birikir ve sonraki adımlar da yanlış olur.\n",
    "\n",
    "**Teacher forcing kullanıldığında:**\n",
    "\n",
    " > Gerçek_0 -> Gerçek_1 -> Gerçek_2 -> ...\n",
    "\n",
    "\n",
    "* Decoder’a bir adım kaydırılmış gerçek hedef token verilir.\n",
    "\n",
    "* Böylece model her adımda doğru bağlamı görür."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13bdb4",
   "metadata": {},
   "source": [
    "## 🔹 Kod Örneği\n",
    "```python\n",
    "dec_input = target_ids[:, :-1]  # Teacher forcing: bir adım kaydırılmış hedef\n",
    "dec_target = target_ids[:, 1:]  # Loss hesaplamak için gerçek hedef\n",
    "\n",
    "logits = model(input_ids, dec_input)\n",
    "```\n",
    "\n",
    "\n",
    "* dec_input → Decoder’a verilecek giriş (teacher forcing uygulanmış)\n",
    "\n",
    "* dec_target → Loss hesaplamak için gerçek hedef tokenlar\n",
    "\n",
    "* Model, dec_input üzerinden tahmin yapar ve dec_target ile karşılaştırır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7afad2",
   "metadata": {},
   "source": [
    "## 🔹 Avantajları ve Dezavantajları\n",
    "### Avantajlar:\n",
    "\n",
    "* Hata birikimi azalır\n",
    "\n",
    "* Eğitim daha hızlı ve stabil\n",
    "\n",
    "* Özellikle uzun dizilerde çok işe yarar\n",
    "\n",
    "### Dezavantajlar:\n",
    "\n",
    "* Train-test farkı yaratabilir:\n",
    "\n",
    "* Test sırasında model kendi tahminlerini kullanır.\n",
    "\n",
    "* Bu nedenle eğitimde gördüğü bağlam ile testteki bağlam farklı olabilir.\n",
    "\n",
    "#### **Çözüm: Scheduled sampling veya kısmi teacher forcing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53777bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Hyperparameters\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True  # True = teacher forcing, False = kendi tahminini kullan\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # 1️⃣ Veri & batch hazırlığı\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        \n",
    "        dec_input = target_ids[:, :-1]    # teacher forcing için\n",
    "        dec_target = target_ids[:, 1:]    # gerçek hedef\n",
    "        \n",
    "        # 2️⃣ Forward Pass (teacher forcing opsiyonel)\n",
    "        logits = model(input_ids, dec_input=dec_input, target_ids=target_ids,\n",
    "                       teacher_forcing=teacher_forcing)\n",
    "        \n",
    "        # 3️⃣ Loss Hesaplama\n",
    "        B, T, V = logits.shape\n",
    "        logits_flat = logits.reshape(B*T, V)\n",
    "        target_flat = dec_target.reshape(B*T)\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # 4️⃣ Backward & Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5️⃣ Logging\n",
    "        total_loss += loss.item()\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3ba50",
   "metadata": {},
   "source": [
    "----\n",
    "# Şu ana kadar bir train işleminin aşamalarını ve temelden işleyişini gördük.Sıra bu işleyişi daha optimize ve daha maliyetsiz hale getirmek.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5befd",
   "metadata": {},
   "source": [
    "## 🔹 AMP (Automatic Mixed Precision) Nedir?\n",
    "\n",
    "Amaç:\n",
    "\n",
    "* Modelin bazı hesaplamalarını float16 (half precision) ile yapıp, diğerlerini float32 olarak tutmak\n",
    "\n",
    "* GPU belleğini ve hesaplama süresini optimize etmek\n",
    "\n",
    "Nasıl çalışır:\n",
    "\n",
    "* Forward ve backward pass’te torch.cuda.amp.autocast() kullanılır\n",
    "\n",
    "* Gradient update sırasında torch.cuda.amp.GradScaler ile gradyanlar ölçeklenir\n",
    "\n",
    "## 🔹 GPU Optimizasyonu\n",
    "\n",
    "* Cihaz seçimi: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "* Tüm tensor ve model GPU’ya taşınır: .to(device)\n",
    "\n",
    "* Batch size ve precision GPU’ya uygun ayarlanır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06370ab4",
   "metadata": {},
   "source": [
    "# 🔹 AMP ile Düzenlenmiş Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()  # Gradient scaler\n",
    "\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Forward + Loss (AMP ile)\n",
    "        # -----------------------------\n",
    "        with autocast():  # mixed precision context\n",
    "            logits = model(input_ids, dec_input=dec_input,\n",
    "                           target_ids=target_ids,\n",
    "                           teacher_forcing=teacher_forcing)\n",
    "            B, T, V = logits.shape\n",
    "            logits_flat = logits.reshape(B*T, V)\n",
    "            target_flat = dec_target.reshape(B*T)\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Backward + Optimize (AMP ile)\n",
    "        # -----------------------------\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecf290",
   "metadata": {},
   "source": [
    "### 🔹 Açıklamalar\n",
    "\n",
    "#### **autocast()**\n",
    "\n",
    "* Forward ve loss hesaplamasını float16/32 karma olarak yapar\n",
    "\n",
    "* Hız ve VRAM tasarrufu sağlar\n",
    "\n",
    "#### **GradScaler**\n",
    "\n",
    "* Küçük gradyanları ölçekleyerek underflow’u önler\n",
    "\n",
    "- scaler.scale(loss).backward() ve scaler.step(optimizer) ile birlikte çalışır\n",
    "\n",
    "#### **GPU kullanımı**\n",
    "\n",
    "* Model ve tüm tensorlar .to(device) ile GPU’ya taşınır\n",
    "\n",
    "* Büyük batch’lerde eğitim verimli olur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd4d4a",
   "metadata": {},
   "source": [
    "---\n",
    "# Bir sonraki adım olarak gradient accumulation ve scheduler ekleyip, büyük batch’leri küçük GPU belleği ile çalıştırabilecek şekilde loop’u geliştireceğiz.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143542a2",
   "metadata": {},
   "source": [
    "## 🔹 Gradient Accumulation Nedir?\n",
    "\n",
    "#### **Amaç:**\n",
    "\n",
    "* Küçük GPU belleği olan makinelerde, büyük batch’i parçalara bölerek işlem yapmak\n",
    "\n",
    "* Her mini-batch için backward yapıp, birkaç mini-batch sonra optimizer step atmak\n",
    "\n",
    "#### **Nasıl çalışır:**\n",
    "\n",
    "* accumulation_steps = N → N mini-batch biriktirilir\n",
    "\n",
    "* loss.backward() her mini-batch için yapılır\n",
    "\n",
    "* optimizer.step() ve scaler.update() sadece N adımda bir yapılır\n",
    "\n",
    "## 🔹 Scheduler Nedir?\n",
    "\n",
    "* Öğrenme oranını dinamik olarak ayarlayan mekanizma\n",
    "\n",
    "* Örnek: StepLR, CosineAnnealingLR, OneCycleLR\n",
    "\n",
    "* Özellikle LLM’lerde stabil eğitim için önemlidir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee11d9d",
   "metadata": {},
   "source": [
    "# 🔹 Optimized Train Loop (AMP + Gradient Accumulation + Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2698f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# -----------------------------\n",
    "# Parametreler\n",
    "# -----------------------------\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True\n",
    "accumulation_steps = 2  # 2 mini-batch biriktir\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)  # örnek\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(input_ids, dec_input=dec_input,\n",
    "                           target_ids=target_ids,\n",
    "                           teacher_forcing=teacher_forcing)\n",
    "            B, T, V = logits.shape\n",
    "            logits_flat = logits.reshape(B*T, V)\n",
    "            target_flat = dec_target.reshape(B*T)\n",
    "            loss = criterion(logits_flat, target_flat) / accumulation_steps  # loss scaling\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item() * accumulation_steps  # orijinal loss\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()  # LR update\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item()*accumulation_steps:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4f570",
   "metadata": {},
   "source": [
    "## 🔹 Açıklamalar\n",
    "\n",
    "### **Loss scaling**\n",
    "\n",
    "* loss / accumulation_steps → gradyanlar birikince toplam loss doğru olur\n",
    "\n",
    "### **Accumulation**\n",
    "\n",
    "* if (batch_idx + 1) % accumulation_steps == 0 → optimizer ve scaler update\n",
    "\n",
    "### **Scheduler**\n",
    "\n",
    "* scheduler.step() → learning rate her accumulation step veya batch sonunda güncellenebilir\n",
    "\n",
    "* AMP + GPU\n",
    "\n",
    "* autocast() ve GradScaler() ile VRAM ve hız optimizasyonu devam eder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb449b7",
   "metadata": {},
   "source": [
    "---\n",
    "# O zaman şimdi Top-K ve Top-P sampling destekli inference kısmını ekleyelim. Bu, modelin eğitildikten sonra çeşitli ve mantıklı çıktılar üretmesini sağlar.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcaf9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Minik Seq2Seq Model (Test için)\n",
    "# -----------------------------\n",
    "class MiniSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size=20, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, enc_input, dec_input=None, target_ids=None, teacher_forcing=True, max_len=20):\n",
    "        enc_emb = self.embedding(enc_input)\n",
    "        _, (h, c) = self.encoder(enc_emb)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Teacher forcing açık\n",
    "        # -----------------------------\n",
    "        if teacher_forcing and dec_input is not None:\n",
    "            dec_emb = self.embedding(dec_input)\n",
    "            dec_out, _ = self.decoder(dec_emb, (h, c))\n",
    "            logits = self.fc(dec_out)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Teacher forcing kapalı (inference)\n",
    "        # -----------------------------\n",
    "        else:\n",
    "            if target_ids is not None:\n",
    "                start_token = target_ids[:, 0].unsqueeze(1)\n",
    "            else:\n",
    "                start_token = enc_input[:, -1].unsqueeze(1)  # son token veya <BOS>\n",
    "            \n",
    "            inputs = start_token\n",
    "            outputs = []\n",
    "            hidden = (h, c)\n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                emb = self.embedding(inputs)\n",
    "                out, hidden = self.decoder(emb, hidden)\n",
    "                logit = self.fc(out)\n",
    "                outputs.append(logit)\n",
    "                inputs = logit.argmax(-1)  # kendi tahminini input olarak kullan\n",
    "            \n",
    "            logits = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model, Device ve Test\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MiniSeq2Seq(vocab_size=20).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c8651",
   "metadata": {},
   "source": [
    "## 🔹 Sampling (Inference) Fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3795cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, input_ids, max_len=20, top_k=50, top_p=0.9, temperature=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    Top-K / Top-P sampling ile token üretme\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    generated = input_ids  # başlangıç token\n",
    "    B = input_ids.size(0)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # decoder kendi tahminini input olarak kullanacak, teacher forcing kapalı\n",
    "        logits = model(generated, dec_input=None, target_ids=None, teacher_forcing=False, max_len=1)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Top-K Sampling\n",
    "        # -----------------------------\n",
    "        if top_k > 0:\n",
    "            top_k_vals, top_k_idx = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.zeros_like(next_token_logits).scatter_(-1, top_k_idx, F.softmax(top_k_vals, dim=-1))\n",
    "        else:\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Top-P Sampling\n",
    "        # -----------------------------\n",
    "        if top_p < 1.0:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            sorted_idx_to_remove = cumulative_probs > top_p\n",
    "            sorted_probs[sorted_idx_to_remove] = 0\n",
    "            sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            probs = torch.zeros_like(probs).scatter_(-1, sorted_idx, sorted_probs)\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Token seçimi\n",
    "        # -----------------------------\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fdc7b",
   "metadata": {},
   "source": [
    "## 🔹 Açıklamalar\n",
    "\n",
    "### > teacher_forcing=False\n",
    "\n",
    "* Inference sırasında model kendi tahminini bir sonraki input olarak kullanır\n",
    "\n",
    "### > Temperature\n",
    "\n",
    "* temperature < 1 → dağılım keskinleşir → deterministik\n",
    "\n",
    "* temperature > 1 → dağılım düzleşir → daha çeşitli çıktılar\n",
    "\n",
    "### > Top-K Sampling\n",
    "\n",
    "* En yüksek K olasılıklı tokenlar arasından rastgele seçim\n",
    "\n",
    "### > Top-P (Nucleus) Sampling\n",
    "\n",
    "* Kümülatif olasılığı P olan token setinden rastgele seçim\n",
    "\n",
    "* Dinamik ve mantıklı token seçimi sağlar\n",
    "\n",
    "### > Multinomial\n",
    "\n",
    "* Token olasılıklarına göre rastgele seçim yapılır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df122cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence (Top-K): [[1, 2, 3, 10, 2, 18, 3, 10, 3, 10, 12]]\n",
      "Generated sequence (Top-P): [[1, 4, 5, 4, 12, 18, 7, 7, 3, 18, 6]]\n"
     ]
    }
   ],
   "source": [
    "start_token = torch.tensor([[1]])  # batch size=1, start token\n",
    "\n",
    "generated_topk = generate(model, start_token, max_len=10, top_k=5, top_p=1.0, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-K):\", generated_topk.tolist())\n",
    "\n",
    "generated_topp = generate(model, start_token, max_len=10, top_k=0, top_p=0.9, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-P):\", generated_topp.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a780b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05229351",
   "metadata": {},
   "source": [
    "# 🔹 Minik Tokenizer Örneği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a787b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Mini tokenizer\n",
    "# -----------------------------\n",
    "id2token = {i: f\"tok{i}\" for i in range(20)}  # token ID → string\n",
    "token2id = {v:k for k,v in id2token.items()}\n",
    "\n",
    "def decode(token_ids):\n",
    "    \"\"\"\n",
    "    Token ID listesini string hâline çevirir\n",
    "    \"\"\"\n",
    "    if isinstance(token_ids, torch.Tensor):\n",
    "        token_ids = token_ids.tolist()\n",
    "    # batch support: batch_size x seq_len\n",
    "    if isinstance(token_ids[0], list):\n",
    "        return [\" \".join(id2token[tok] for tok in seq) for seq in token_ids]\n",
    "    else:\n",
    "        return \" \".join(id2token[tok] for tok in token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4c77f",
   "metadata": {},
   "source": [
    "## 🔹 Test: Top-K ve Top-P Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b0a085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence (Top-K IDs): [[1, 3, 18, 2, 3, 7, 16, 18, 10, 18, 3]]\n",
      "Generated sequence (Top-K Decoded): ['tok1 tok3 tok18 tok2 tok3 tok7 tok16 tok18 tok10 tok18 tok3']\n",
      "Generated sequence (Top-P IDs): [[1, 8, 17, 18, 12, 12, 10, 8, 12, 3, 6]]\n",
      "Generated sequence (Top-P Decoded): ['tok1 tok8 tok17 tok18 tok12 tok12 tok10 tok8 tok12 tok3 tok6']\n"
     ]
    }
   ],
   "source": [
    "# Örnek start token\n",
    "start_token = torch.tensor([[1]])\n",
    "\n",
    "# Top-K sampling\n",
    "generated_topk = generate(model, start_token, max_len=10, top_k=5, top_p=1.0, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-K IDs):\", generated_topk.tolist())\n",
    "print(\"Generated sequence (Top-K Decoded):\", decode(generated_topk))\n",
    "\n",
    "# Top-P sampling\n",
    "generated_topp = generate(model, start_token, max_len=10, top_k=0, top_p=0.9, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-P IDs):\", generated_topp.tolist())\n",
    "print(\"Generated sequence (Top-P Decoded):\", decode(generated_topp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc95ae9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9947d",
   "metadata": {},
   "source": [
    "## Yukarıda anlatılan işlemler doğrultusunda oluşturulan \"TRAİN_LOOP\" aşağıdaki gibidir ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9274de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# -----------------------------\n",
    "# Parametreler\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "teacher_forcing = True\n",
    "accumulation_steps = 2  # büyük batch simülasyonu\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# -----------------------------\n",
    "# Model, Optimizer, Scaler, Scheduler\n",
    "# -----------------------------\n",
    "model = MiniSeq2Seq(vocab_size=20).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scaler = GradScaler()\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)  # örnek scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -----------------------------\n",
    "# Train Loop\n",
    "# -----------------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):  # train_loader = DataLoader objesi\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "        dec_input = target_ids[:, :-1]\n",
    "        dec_target = target_ids[:, 1:]\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Forward + Loss (AMP)\n",
    "        # -----------------------------\n",
    "        with autocast():\n",
    "            logits = model(input_ids, dec_input=dec_input,\n",
    "                           target_ids=target_ids,\n",
    "                           teacher_forcing=teacher_forcing)\n",
    "            \n",
    "            B, T, V = logits.shape\n",
    "            logits_flat = logits.reshape(B*T, V)\n",
    "            target_flat = dec_target.reshape(B*T)\n",
    "            loss = criterion(logits_flat, target_flat) / accumulation_steps  # scaled for accumulation\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Backward (AMP + Grad Accumulation)\n",
    "        # -----------------------------\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Logging\n",
    "        if (batch_idx + 1) % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Batch [{batch_idx+1}/{len(train_loader)}] | Loss: {loss.item()*accumulation_steps:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ccbd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence (Top-K IDs): [[1, 2, 4, 7, 7, 0, 19, 7, 12, 16, 3]]\n",
      "Generated sequence (Top-K Decoded): ['tok1 tok2 tok4 tok7 tok7 tok0 tok19 tok7 tok12 tok16 tok3']\n",
      "Generated sequence (Top-P IDs): [[1, 19, 5, 2, 11, 9, 18, 10, 8, 12, 12]]\n",
      "Generated sequence (Top-P Decoded): ['tok1 tok19 tok5 tok2 tok11 tok9 tok18 tok10 tok8 tok12 tok12']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Örnek start token\n",
    "# -----------------------------\n",
    "start_token = torch.tensor([[1]]).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Top-K Sampling\n",
    "# -----------------------------\n",
    "generated_topk = generate(model, start_token, max_len=10, top_k=5, top_p=1.0, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-K IDs):\", generated_topk.tolist())\n",
    "print(\"Generated sequence (Top-K Decoded):\", decode(generated_topk))\n",
    "\n",
    "# -----------------------------\n",
    "# Top-P (Nucleus) Sampling\n",
    "# -----------------------------\n",
    "generated_topp = generate(model, start_token, max_len=10, top_k=0, top_p=0.9, temperature=1.0, device=device)\n",
    "print(\"Generated sequence (Top-P IDs):\", generated_topp.tolist())\n",
    "print(\"Generated sequence (Top-P Decoded):\", decode(generated_topp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f481045",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
