{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e6925d",
   "metadata": {},
   "source": [
    "# 🧠 Transformer Nedir?\n",
    "\n",
    "**Transformer**, 2017’de Vaswani ve ekibinin *“Attention is All You Need”* makalesiyle tanıttığı bir derin öğrenme mimarisidir.  \n",
    "Bu yapı, **RNN** veya **LSTM** gibi sırayla çalışan (ardışık) yapıları tamamen bırakır ve yerine **attention (dikkat) mekanizması** üzerine kurulu **paralel bir mimari** getirir.\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 Temel Fikir\n",
    "\n",
    "Önceden RNN/LSTM modellerinde cümle şu şekilde işleniyordu:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645af521",
   "metadata": {},
   "source": [
    "* [Ben] → [okula] → [gidiyorum]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbcbfd",
   "metadata": {},
   "source": [
    "\n",
    "Her kelime sırayla işlenir, bilgi “gizli durum (hidden state)” içinde aktarılırdı.  \n",
    "Bu da **zaman kaybı** ve **uzun bağımlılık sorunlarına (long-term dependency)** yol açıyordu.\n",
    "\n",
    "Transformer ise:\n",
    "> “Tüm kelimelere aynı anda bakayım, dikkatimi en önemli yerlere vereyim.” der.\n",
    "\n",
    "Yani paralel işler, **sequence bağımlılığını kaldırır**.\n",
    "\n",
    "\n",
    "## 🧩 Transformer’in Bileşenleri\n",
    "\n",
    "Transformer iki ana kısımdan oluşur:\n",
    "\n",
    "### 1. **Encoder (Kodlayıcı)**\n",
    "- Girdiyi işler (örneğin “Ben okula gidiyorum.”)\n",
    "- Her kelimeye “attention” uygular ve anlam temsilleri üretir.\n",
    "- Encoder çıktısı = gizli bilgi (context)\n",
    "\n",
    "### 2. **Decoder (Çözücü)**\n",
    "- Encoder’ın çıktısını alır.\n",
    "- Çıktı dizisini (örneğin çeviri ya da tahmin) üretir.\n",
    "- Kendi önceki çıktılarından da öğrenir (auto-regressive yapı).\n",
    "\n",
    "\n",
    "## ⚙️ İç Yapı\n",
    "\n",
    "Her encoder/decoder bloğu birkaç temel alt parçadan oluşur:\n",
    "\n",
    "| Katman | İşlev | Açıklama |\n",
    "|--------|--------|----------|\n",
    "| **Multi-Head Self-Attention** | Dikkat hesaplar | Her kelime diğerlerine ne kadar dikkat edecek, bunu öğrenir. |\n",
    "| **Feed Forward Network (FFN)** | Dönüştürme | Attention çıktısını işler. |\n",
    "| **Layer Normalization** | Denge | Katmanlar arası ölçek farklarını dengeler. |\n",
    "| **Residual Connection** | Shortcut | Öğrenmeyi stabilize eder, gradyan kaybını azaltır. |\n",
    "\n",
    "Decoder tarafında bunlara ek olarak:\n",
    "- **Masked Self-Attention** (geleceğe bakmayı engeller)\n",
    "- **Encoder-Decoder Attention** (encoder’dan bilgi çeker)\n",
    "\n",
    "\n",
    "## 🔍 Görsel Olarak\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469861a",
   "metadata": {},
   "source": [
    "```bash \n",
    "Input Sequence\n",
    "↓\n",
    "[Encoder 1]\n",
    "↓\n",
    "[Encoder 2]\n",
    "↓\n",
    "... n kez\n",
    "↓\n",
    "Context Vector\n",
    "↓\n",
    "[Decoder 1]\n",
    "↓\n",
    "[Decoder 2]\n",
    "↓\n",
    "... n kez\n",
    "↓\n",
    "Output Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e39ef3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 💡 Avantajları\n",
    "\n",
    "✅ Paralel işlem yapar → GPU dostu, çok hızlıdır  \n",
    "✅ Uzun cümlelerde bağımlılıkları iyi öğrenir  \n",
    "✅ Büyük veriyle mükemmel performans  \n",
    "✅ Transfer learning’e uygundur (örnek: BERT, GPT, T5, BART...)\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 Örnek Kullanımlar\n",
    "\n",
    "| Model | Taban | Görev |\n",
    "|-------|--------|-------|\n",
    "| **BERT** | Encoder | Anlama (classification, NER, QA) |\n",
    "| **GPT** | Decoder | Üretim (text generation) |\n",
    "| **T5 / BART** | Encoder–Decoder | Çeviri, özetleme, dönüşüm |\n",
    "\n",
    "\n",
    "\n",
    "## 📚 Özet\n",
    "\n",
    "- Transformer, RNN/LSTM yapılarının yerini alan **tamamen attention tabanlı** bir modeldir.  \n",
    "- **Encoder**, girdiyi işler.  \n",
    "- **Decoder**, çıktı üretir.  \n",
    "- **Multi-Head Attention**, kelimeler arasındaki ilişkileri paralel olarak öğrenir.  \n",
    "- Günümüzde **BERT**, **GPT**, **T5** gibi modern modellerin temelini oluşturur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ecfcb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb7f21",
   "metadata": {},
   "source": [
    "# 🎯 Transformer'da Kullanılan Attention Türleri\n",
    "\n",
    "Transformer mimarisinin kalbi **“Attention” (Dikkat) mekanizmasıdır.**  \n",
    "Bu mekanizma, modelin bir kelimeyi işlerken dizideki diğer kelimelere ne kadar dikkat edeceğini öğrenmesini sağlar.  \n",
    "\n",
    "Transformer’da birden fazla attention türü vardır, çünkü **Encoder** ve **Decoder** farklı görevler yapar.  \n",
    "Aşağıda tüm attention çeşitlerini ayrı ayrı açıklayalım 👇\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 1. Self-Attention (Kendine Dikkat)\n",
    "\n",
    "Self-Attention, her kelimenin **aynı cümledeki diğer kelimelere dikkat etmesini** sağlar.\n",
    "\n",
    "Örnek:\n",
    "> “Kedi sütü içti.”  \n",
    "> Burada “kedi” kelimesi, “içti” kelimesiyle anlam ilişkisini kurmak ister.\n",
    "\n",
    "Bu mekanizmada her kelime, diğer tüm kelimelerle benzerliğini ölçer.  \n",
    "Bu benzerlik **Query (Q)**, **Key (K)** ve **Value (V)** matrisleri üzerinden hesaplanır:\n",
    "\n",
    "### 🧩 Matematiksel Olarak\n",
    "\\[\n",
    "Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- **Q (Query)**: Aranan bilgi (kelimenin neye dikkat etmek istediği)\n",
    "- **K (Key)**: Diğer kelimelerin temsil ettiği bilgi\n",
    "- **V (Value)**: Dikkat edilecek bilginin değeri  \n",
    "- **√dₖ**: Ölçekleme faktörü (stabilite için)\n",
    "\n",
    "\n",
    "\n",
    "## ⚙️ 2. Multi-Head Attention\n",
    "\n",
    "Tek bir attention yeterli değildir, çünkü farklı başlıklar (heads) farklı ilişkileri öğrenir.\n",
    "\n",
    "Bu yüzden **Multi-Head Attention**, aynı anda birden fazla “self-attention” işlemi yapar.  \n",
    "Her head farklı bir alt uzayda ilişkiler öğrenir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95adff",
   "metadata": {},
   "source": [
    "* MultiHead(Q, K, V) = Concat(head₁, head₂, ..., headₕ) * Wₒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879c082",
   "metadata": {},
   "source": [
    "\n",
    "Avantajı:\n",
    "- Farklı dikkat türlerini aynı anda öğrenir.\n",
    "- Dilin karmaşık ilişkilerini daha iyi yakalar.\n",
    "\n",
    "\n",
    "\n",
    "## 🕶️ 3. Masked Self-Attention (Decoder'da Kullanılır)\n",
    "\n",
    "Decoder, çıktıyı **adım adım (auto-regressive)** üretir.  \n",
    "Yani model, “gelecekteki kelimelere bakmadan” sıradaki kelimeyi tahmin etmelidir.\n",
    "\n",
    "Bunu sağlamak için “masking” yapılır:\n",
    "- Model, sadece geçmiş kelimelere bakabilir.\n",
    "- Gelecek kelimelere erişimi **mask** (engelleme) ile kapatılır.\n",
    "\n",
    "Örnek:\n",
    "> Çeviri yaparken, “Ben okula” kelimeleri işlendiğinde “gidiyorum” kelimesine henüz bakamaz.\n",
    "\n",
    "\n",
    "\n",
    "## 🔗 4. Encoder–Decoder Attention\n",
    "\n",
    "Bu attention türü **Decoder tarafında** bulunur.  \n",
    "Burada Decoder, **Encoder’ın ürettiği gizli temsillere (context vector)** dikkat eder.\n",
    "\n",
    "Yani:\n",
    "- **Encoder çıktısı** (bilgi)\n",
    "- **Decoder sorgusu (query)** (hangi bilgiye ihtiyaç var)\n",
    "\n",
    "Decoder böylece, girdi cümlesinin anlamlı bölümlerine odaklanarak doğru çıktıyı üretir.\n",
    "\n",
    "Örnek:\n",
    "> İngilizce: “I love apples”  \n",
    "> Türkçe çeviri: “Elma seviyorum”  \n",
    "> Decoder, “apples” kelimesine geldiğinde Encoder’daki “Elma” bilgisini çeker.\n",
    "\n",
    "\n",
    "\n",
    "## 🧩 5. Cross-Attention (Genel Tanım)\n",
    "\n",
    "“Cross-Attention” aslında Encoder–Decoder Attention’ın genel adıdır.  \n",
    "Çünkü **Q** Decoder’dan gelir, **K** ve **V** Encoder’dan gelir.  \n",
    "Yani iki farklı kaynaktan bilgi alışverişi olur.\n",
    "\n",
    "Bu yapı, “bilgiyi kodlayan” (encoder) ile “bilgiyi kullanan” (decoder) arasındaki köprüdür.\n",
    "\n",
    "\n",
    "\n",
    "## 🧮 Özet Tablo\n",
    "\n",
    "| Attention Türü | Nerede Kullanılır | Amaç |\n",
    "|-----------------|------------------|------|\n",
    "| **Self-Attention** | Encoder & Decoder | Aynı dizideki kelimeler arası ilişki |\n",
    "| **Masked Self-Attention** | Decoder | Geleceğe bakmayı engellemek |\n",
    "| **Encoder–Decoder Attention (Cross)** | Decoder | Encoder’dan bilgi almak |\n",
    "| **Multi-Head Attention** | Her yerde | Farklı ilişkileri paralel öğrenmek |\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 Kısa Hatırlatma\n",
    "\n",
    "> Transformer’ı güçlü yapan şey, “her kelimenin diğer tüm kelimelerle bağ kurabilmesi”dir.  \n",
    "> Bu da **Attention mekanizmaları** sayesinde olur.\n",
    "\n",
    "Örneğin:\n",
    "- **Encoder’daki Self-Attention:** Girdinin bağlamını anlar  \n",
    "- **Decoder’daki Masked Self-Attention:** Çıktıyı adım adım üretir  \n",
    "- **Encoder-Decoder Attention:** Girdi ve çıktı arasında anlam köprüsü kurar\n",
    "\n",
    "\n",
    "\n",
    "## 🎓 Sonuç\n",
    "\n",
    "- “Attention” kelimenin tam anlamıyla “dikkat”tir.  \n",
    "- Transformer, dikkatini farklı parçalara bölüp (multi-head), cümledeki anlam ilişkilerini derinlemesine öğrenir.  \n",
    "- Bu sayede çeviri, özetleme, metin üretimi gibi görevlerde olağanüstü sonuçlar verir.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a3e24",
   "metadata": {},
   "source": [
    "---\n",
    "# 📊 Transformer'da Kullanılan Attention Türleri\n",
    "\n",
    "Aşağıdaki tablo, **Transformer** mimarisinde her bileşende (Encoder ve Decoder) hangi **attention türlerinin** kullanıldığını gösterir.\n",
    "\n",
    "\n",
    "\n",
    "| Bileşen | Attention Türü | Açıklama | Mask Kullanımı | Amaç |\n",
    "|----------|----------------|-----------|----------------|-------|\n",
    "| **Encoder (Self-Attention)** | **Self-Attention** | Encoder içindeki her kelime diğerlerine dikkat eder. | ❌ Yok | Girdinin anlam bağlamını öğrenir. |\n",
    "| **Encoder (Tüm Katmanlar)** | **Multi-Head Self-Attention** | Aynı anda birden fazla dikkat başlığı çalışır. | ❌ Yok | Farklı ilişkileri paralel öğrenir. |\n",
    "| **Decoder (İlk Attention Bloğu)** | **Masked Self-Attention** | Decoder sadece geçmiş kelimelere bakar. | ✅ Var | Gelecek kelimelere bakmadan çıktı üretir. |\n",
    "| **Decoder (İkinci Attention Bloğu)** | **Encoder–Decoder Attention (Cross-Attention)** | Decoder, Encoder çıktısına dikkat eder. | ❌ Yok | Girdi–çıktı bağlamını kurar. |\n",
    "| **Decoder (Tüm Katmanlar)** | **Multi-Head Attention (2 Katmanlı)** | 1. Masked, 2. Encoder–Decoder | 🔸 Karışık | Üretim sırasında bağlam ve geçmiş kelimeleri birleştirir. |\n",
    "\n",
    "\n",
    "## 🧠 Görsel Olarak\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db056877",
   "metadata": {},
   "source": [
    "```bash\n",
    " ┌──────────────────────────┐\n",
    "        │        ENCODER           │\n",
    "        │ ┌──────────────────────┐ │\n",
    "Input →────►│ Multi-Head Attention │ │ (Self-Attention)\n",
    "│ └──────────────────────┘ │\n",
    "│ FFN │\n",
    "└──────────────────────────┘\n",
    "│\n",
    "▼\n",
    "┌──────────────────────────┐\n",
    "│ DECODER │\n",
    "│ ┌──────────────────────┐ │\n",
    "│ │ Masked Self-Attention│ │ (Geleceğe bakma engelli)\n",
    "│ ├──────────────────────┤ │\n",
    "│ │ Encoder–Decoder Att. │ │ (Cross-Attention)\n",
    "│ └──────────────────────┘ │\n",
    "│ FFN │\n",
    "└──────────────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37dc89",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 💬 Özet\n",
    "\n",
    "| Bölge | Kullanılan Attention Türü | Amaç |\n",
    "|--------|-----------------------------|-------|\n",
    "| **Encoder** | Self-Attention | Girdiyi anlamlı temsillere dönüştürmek |\n",
    "| **Decoder** | Masked Self-Attention | Çıktıyı adım adım üretmek |\n",
    "| **Decoder** | Encoder–Decoder Attention | Girdi–çıktı arasında ilişki kurmak |\n",
    "| **Her ikisinde** | Multi-Head Attention | Farklı ilişkileri aynı anda öğrenmek |\n",
    "\n",
    "\n",
    "✅ **Sonuç:**  \n",
    "Transformer mimarisi toplamda **3 temel attention mekanizması** kullanır:  \n",
    "1. **Self-Attention (Encoder)**  \n",
    "2. **Masked Self-Attention (Decoder)**  \n",
    "3. **Encoder–Decoder (Cross) Attention (Decoder)**  \n",
    "\n",
    "Tüm bu attention’lar, **Multi-Head Attention** yapısı içinde paralel olarak çalışır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8229e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
