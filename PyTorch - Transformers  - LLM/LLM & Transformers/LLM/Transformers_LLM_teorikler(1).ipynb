{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e6925d",
   "metadata": {},
   "source": [
    "# ğŸ§  Transformer Nedir?\n",
    "\n",
    "**Transformer**, 2017â€™de Vaswani ve ekibinin *â€œAttention is All You Needâ€* makalesiyle tanÄ±ttÄ±ÄŸÄ± bir derin Ã¶ÄŸrenme mimarisidir.  \n",
    "Bu yapÄ±, **RNN** veya **LSTM** gibi sÄ±rayla Ã§alÄ±ÅŸan (ardÄ±ÅŸÄ±k) yapÄ±larÄ± tamamen bÄ±rakÄ±r ve yerine **attention (dikkat) mekanizmasÄ±** Ã¼zerine kurulu **paralel bir mimari** getirir.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ Temel Fikir\n",
    "\n",
    "Ã–nceden RNN/LSTM modellerinde cÃ¼mle ÅŸu ÅŸekilde iÅŸleniyordu:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645af521",
   "metadata": {},
   "source": [
    "* [Ben] â†’ [okula] â†’ [gidiyorum]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbcbfd",
   "metadata": {},
   "source": [
    "\n",
    "Her kelime sÄ±rayla iÅŸlenir, bilgi â€œgizli durum (hidden state)â€ iÃ§inde aktarÄ±lÄ±rdÄ±.  \n",
    "Bu da **zaman kaybÄ±** ve **uzun baÄŸÄ±mlÄ±lÄ±k sorunlarÄ±na (long-term dependency)** yol aÃ§Ä±yordu.\n",
    "\n",
    "Transformer ise:\n",
    "> â€œTÃ¼m kelimelere aynÄ± anda bakayÄ±m, dikkatimi en Ã¶nemli yerlere vereyim.â€ der.\n",
    "\n",
    "Yani paralel iÅŸler, **sequence baÄŸÄ±mlÄ±lÄ±ÄŸÄ±nÄ± kaldÄ±rÄ±r**.\n",
    "\n",
    "\n",
    "## ğŸ§© Transformerâ€™in BileÅŸenleri\n",
    "\n",
    "Transformer iki ana kÄ±sÄ±mdan oluÅŸur:\n",
    "\n",
    "### 1. **Encoder (KodlayÄ±cÄ±)**\n",
    "- Girdiyi iÅŸler (Ã¶rneÄŸin â€œBen okula gidiyorum.â€)\n",
    "- Her kelimeye â€œattentionâ€ uygular ve anlam temsilleri Ã¼retir.\n",
    "- Encoder Ã§Ä±ktÄ±sÄ± = gizli bilgi (context)\n",
    "\n",
    "### 2. **Decoder (Ã‡Ã¶zÃ¼cÃ¼)**\n",
    "- Encoderâ€™Ä±n Ã§Ä±ktÄ±sÄ±nÄ± alÄ±r.\n",
    "- Ã‡Ä±ktÄ± dizisini (Ã¶rneÄŸin Ã§eviri ya da tahmin) Ã¼retir.\n",
    "- Kendi Ã¶nceki Ã§Ä±ktÄ±larÄ±ndan da Ã¶ÄŸrenir (auto-regressive yapÄ±).\n",
    "\n",
    "\n",
    "## âš™ï¸ Ä°Ã§ YapÄ±\n",
    "\n",
    "Her encoder/decoder bloÄŸu birkaÃ§ temel alt parÃ§adan oluÅŸur:\n",
    "\n",
    "| Katman | Ä°ÅŸlev | AÃ§Ä±klama |\n",
    "|--------|--------|----------|\n",
    "| **Multi-Head Self-Attention** | Dikkat hesaplar | Her kelime diÄŸerlerine ne kadar dikkat edecek, bunu Ã¶ÄŸrenir. |\n",
    "| **Feed Forward Network (FFN)** | DÃ¶nÃ¼ÅŸtÃ¼rme | Attention Ã§Ä±ktÄ±sÄ±nÄ± iÅŸler. |\n",
    "| **Layer Normalization** | Denge | Katmanlar arasÄ± Ã¶lÃ§ek farklarÄ±nÄ± dengeler. |\n",
    "| **Residual Connection** | Shortcut | Ã–ÄŸrenmeyi stabilize eder, gradyan kaybÄ±nÄ± azaltÄ±r. |\n",
    "\n",
    "Decoder tarafÄ±nda bunlara ek olarak:\n",
    "- **Masked Self-Attention** (geleceÄŸe bakmayÄ± engeller)\n",
    "- **Encoder-Decoder Attention** (encoderâ€™dan bilgi Ã§eker)\n",
    "\n",
    "\n",
    "## ğŸ” GÃ¶rsel Olarak\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469861a",
   "metadata": {},
   "source": [
    "```bash \n",
    "Input Sequence\n",
    "â†“\n",
    "[Encoder 1]\n",
    "â†“\n",
    "[Encoder 2]\n",
    "â†“\n",
    "... n kez\n",
    "â†“\n",
    "Context Vector\n",
    "â†“\n",
    "[Decoder 1]\n",
    "â†“\n",
    "[Decoder 2]\n",
    "â†“\n",
    "... n kez\n",
    "â†“\n",
    "Output Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e39ef3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ AvantajlarÄ±\n",
    "\n",
    "âœ… Paralel iÅŸlem yapar â†’ GPU dostu, Ã§ok hÄ±zlÄ±dÄ±r  \n",
    "âœ… Uzun cÃ¼mlelerde baÄŸÄ±mlÄ±lÄ±klarÄ± iyi Ã¶ÄŸrenir  \n",
    "âœ… BÃ¼yÃ¼k veriyle mÃ¼kemmel performans  \n",
    "âœ… Transfer learningâ€™e uygundur (Ã¶rnek: BERT, GPT, T5, BART...)\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ Ã–rnek KullanÄ±mlar\n",
    "\n",
    "| Model | Taban | GÃ¶rev |\n",
    "|-------|--------|-------|\n",
    "| **BERT** | Encoder | Anlama (classification, NER, QA) |\n",
    "| **GPT** | Decoder | Ãœretim (text generation) |\n",
    "| **T5 / BART** | Encoderâ€“Decoder | Ã‡eviri, Ã¶zetleme, dÃ¶nÃ¼ÅŸÃ¼m |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“š Ã–zet\n",
    "\n",
    "- Transformer, RNN/LSTM yapÄ±larÄ±nÄ±n yerini alan **tamamen attention tabanlÄ±** bir modeldir.  \n",
    "- **Encoder**, girdiyi iÅŸler.  \n",
    "- **Decoder**, Ã§Ä±ktÄ± Ã¼retir.  \n",
    "- **Multi-Head Attention**, kelimeler arasÄ±ndaki iliÅŸkileri paralel olarak Ã¶ÄŸrenir.  \n",
    "- GÃ¼nÃ¼mÃ¼zde **BERT**, **GPT**, **T5** gibi modern modellerin temelini oluÅŸturur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ecfcb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb7f21",
   "metadata": {},
   "source": [
    "# ğŸ¯ Transformer'da KullanÄ±lan Attention TÃ¼rleri\n",
    "\n",
    "Transformer mimarisinin kalbi **â€œAttentionâ€ (Dikkat) mekanizmasÄ±dÄ±r.**  \n",
    "Bu mekanizma, modelin bir kelimeyi iÅŸlerken dizideki diÄŸer kelimelere ne kadar dikkat edeceÄŸini Ã¶ÄŸrenmesini saÄŸlar.  \n",
    "\n",
    "Transformerâ€™da birden fazla attention tÃ¼rÃ¼ vardÄ±r, Ã§Ã¼nkÃ¼ **Encoder** ve **Decoder** farklÄ± gÃ¶revler yapar.  \n",
    "AÅŸaÄŸÄ±da tÃ¼m attention Ã§eÅŸitlerini ayrÄ± ayrÄ± aÃ§Ä±klayalÄ±m ğŸ‘‡\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§  1. Self-Attention (Kendine Dikkat)\n",
    "\n",
    "Self-Attention, her kelimenin **aynÄ± cÃ¼mledeki diÄŸer kelimelere dikkat etmesini** saÄŸlar.\n",
    "\n",
    "Ã–rnek:\n",
    "> â€œKedi sÃ¼tÃ¼ iÃ§ti.â€  \n",
    "> Burada â€œkediâ€ kelimesi, â€œiÃ§tiâ€ kelimesiyle anlam iliÅŸkisini kurmak ister.\n",
    "\n",
    "Bu mekanizmada her kelime, diÄŸer tÃ¼m kelimelerle benzerliÄŸini Ã¶lÃ§er.  \n",
    "Bu benzerlik **Query (Q)**, **Key (K)** ve **Value (V)** matrisleri Ã¼zerinden hesaplanÄ±r:\n",
    "\n",
    "### ğŸ§© Matematiksel Olarak\n",
    "\\[\n",
    "Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- **Q (Query)**: Aranan bilgi (kelimenin neye dikkat etmek istediÄŸi)\n",
    "- **K (Key)**: DiÄŸer kelimelerin temsil ettiÄŸi bilgi\n",
    "- **V (Value)**: Dikkat edilecek bilginin deÄŸeri  \n",
    "- **âˆšdâ‚–**: Ã–lÃ§ekleme faktÃ¶rÃ¼ (stabilite iÃ§in)\n",
    "\n",
    "\n",
    "\n",
    "## âš™ï¸ 2. Multi-Head Attention\n",
    "\n",
    "Tek bir attention yeterli deÄŸildir, Ã§Ã¼nkÃ¼ farklÄ± baÅŸlÄ±klar (heads) farklÄ± iliÅŸkileri Ã¶ÄŸrenir.\n",
    "\n",
    "Bu yÃ¼zden **Multi-Head Attention**, aynÄ± anda birden fazla â€œself-attentionâ€ iÅŸlemi yapar.  \n",
    "Her head farklÄ± bir alt uzayda iliÅŸkiler Ã¶ÄŸrenir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da95adff",
   "metadata": {},
   "source": [
    "* MultiHead(Q, K, V) = Concat(headâ‚, headâ‚‚, ..., headâ‚•) * Wâ‚’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879c082",
   "metadata": {},
   "source": [
    "\n",
    "AvantajÄ±:\n",
    "- FarklÄ± dikkat tÃ¼rlerini aynÄ± anda Ã¶ÄŸrenir.\n",
    "- Dilin karmaÅŸÄ±k iliÅŸkilerini daha iyi yakalar.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ•¶ï¸ 3. Masked Self-Attention (Decoder'da KullanÄ±lÄ±r)\n",
    "\n",
    "Decoder, Ã§Ä±ktÄ±yÄ± **adÄ±m adÄ±m (auto-regressive)** Ã¼retir.  \n",
    "Yani model, â€œgelecekteki kelimelere bakmadanâ€ sÄ±radaki kelimeyi tahmin etmelidir.\n",
    "\n",
    "Bunu saÄŸlamak iÃ§in â€œmaskingâ€ yapÄ±lÄ±r:\n",
    "- Model, sadece geÃ§miÅŸ kelimelere bakabilir.\n",
    "- Gelecek kelimelere eriÅŸimi **mask** (engelleme) ile kapatÄ±lÄ±r.\n",
    "\n",
    "Ã–rnek:\n",
    "> Ã‡eviri yaparken, â€œBen okulaâ€ kelimeleri iÅŸlendiÄŸinde â€œgidiyorumâ€ kelimesine henÃ¼z bakamaz.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”— 4. Encoderâ€“Decoder Attention\n",
    "\n",
    "Bu attention tÃ¼rÃ¼ **Decoder tarafÄ±nda** bulunur.  \n",
    "Burada Decoder, **Encoderâ€™Ä±n Ã¼rettiÄŸi gizli temsillere (context vector)** dikkat eder.\n",
    "\n",
    "Yani:\n",
    "- **Encoder Ã§Ä±ktÄ±sÄ±** (bilgi)\n",
    "- **Decoder sorgusu (query)** (hangi bilgiye ihtiyaÃ§ var)\n",
    "\n",
    "Decoder bÃ¶ylece, girdi cÃ¼mlesinin anlamlÄ± bÃ¶lÃ¼mlerine odaklanarak doÄŸru Ã§Ä±ktÄ±yÄ± Ã¼retir.\n",
    "\n",
    "Ã–rnek:\n",
    "> Ä°ngilizce: â€œI love applesâ€  \n",
    "> TÃ¼rkÃ§e Ã§eviri: â€œElma seviyorumâ€  \n",
    "> Decoder, â€œapplesâ€ kelimesine geldiÄŸinde Encoderâ€™daki â€œElmaâ€ bilgisini Ã§eker.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§© 5. Cross-Attention (Genel TanÄ±m)\n",
    "\n",
    "â€œCross-Attentionâ€ aslÄ±nda Encoderâ€“Decoder Attentionâ€™Ä±n genel adÄ±dÄ±r.  \n",
    "Ã‡Ã¼nkÃ¼ **Q** Decoderâ€™dan gelir, **K** ve **V** Encoderâ€™dan gelir.  \n",
    "Yani iki farklÄ± kaynaktan bilgi alÄ±ÅŸveriÅŸi olur.\n",
    "\n",
    "Bu yapÄ±, â€œbilgiyi kodlayanâ€ (encoder) ile â€œbilgiyi kullananâ€ (decoder) arasÄ±ndaki kÃ¶prÃ¼dÃ¼r.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§® Ã–zet Tablo\n",
    "\n",
    "| Attention TÃ¼rÃ¼ | Nerede KullanÄ±lÄ±r | AmaÃ§ |\n",
    "|-----------------|------------------|------|\n",
    "| **Self-Attention** | Encoder & Decoder | AynÄ± dizideki kelimeler arasÄ± iliÅŸki |\n",
    "| **Masked Self-Attention** | Decoder | GeleceÄŸe bakmayÄ± engellemek |\n",
    "| **Encoderâ€“Decoder Attention (Cross)** | Decoder | Encoderâ€™dan bilgi almak |\n",
    "| **Multi-Head Attention** | Her yerde | FarklÄ± iliÅŸkileri paralel Ã¶ÄŸrenmek |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§  KÄ±sa HatÄ±rlatma\n",
    "\n",
    "> Transformerâ€™Ä± gÃ¼Ã§lÃ¼ yapan ÅŸey, â€œher kelimenin diÄŸer tÃ¼m kelimelerle baÄŸ kurabilmesiâ€dir.  \n",
    "> Bu da **Attention mekanizmalarÄ±** sayesinde olur.\n",
    "\n",
    "Ã–rneÄŸin:\n",
    "- **Encoderâ€™daki Self-Attention:** Girdinin baÄŸlamÄ±nÄ± anlar  \n",
    "- **Decoderâ€™daki Masked Self-Attention:** Ã‡Ä±ktÄ±yÄ± adÄ±m adÄ±m Ã¼retir  \n",
    "- **Encoder-Decoder Attention:** Girdi ve Ã§Ä±ktÄ± arasÄ±nda anlam kÃ¶prÃ¼sÃ¼ kurar\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“ SonuÃ§\n",
    "\n",
    "- â€œAttentionâ€ kelimenin tam anlamÄ±yla â€œdikkatâ€tir.  \n",
    "- Transformer, dikkatini farklÄ± parÃ§alara bÃ¶lÃ¼p (multi-head), cÃ¼mledeki anlam iliÅŸkilerini derinlemesine Ã¶ÄŸrenir.  \n",
    "- Bu sayede Ã§eviri, Ã¶zetleme, metin Ã¼retimi gibi gÃ¶revlerde olaÄŸanÃ¼stÃ¼ sonuÃ§lar verir.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a3e24",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“Š Transformer'da KullanÄ±lan Attention TÃ¼rleri\n",
    "\n",
    "AÅŸaÄŸÄ±daki tablo, **Transformer** mimarisinde her bileÅŸende (Encoder ve Decoder) hangi **attention tÃ¼rlerinin** kullanÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "\n",
    "\n",
    "| BileÅŸen | Attention TÃ¼rÃ¼ | AÃ§Ä±klama | Mask KullanÄ±mÄ± | AmaÃ§ |\n",
    "|----------|----------------|-----------|----------------|-------|\n",
    "| **Encoder (Self-Attention)** | **Self-Attention** | Encoder iÃ§indeki her kelime diÄŸerlerine dikkat eder. | âŒ Yok | Girdinin anlam baÄŸlamÄ±nÄ± Ã¶ÄŸrenir. |\n",
    "| **Encoder (TÃ¼m Katmanlar)** | **Multi-Head Self-Attention** | AynÄ± anda birden fazla dikkat baÅŸlÄ±ÄŸÄ± Ã§alÄ±ÅŸÄ±r. | âŒ Yok | FarklÄ± iliÅŸkileri paralel Ã¶ÄŸrenir. |\n",
    "| **Decoder (Ä°lk Attention BloÄŸu)** | **Masked Self-Attention** | Decoder sadece geÃ§miÅŸ kelimelere bakar. | âœ… Var | Gelecek kelimelere bakmadan Ã§Ä±ktÄ± Ã¼retir. |\n",
    "| **Decoder (Ä°kinci Attention BloÄŸu)** | **Encoderâ€“Decoder Attention (Cross-Attention)** | Decoder, Encoder Ã§Ä±ktÄ±sÄ±na dikkat eder. | âŒ Yok | Girdiâ€“Ã§Ä±ktÄ± baÄŸlamÄ±nÄ± kurar. |\n",
    "| **Decoder (TÃ¼m Katmanlar)** | **Multi-Head Attention (2 KatmanlÄ±)** | 1. Masked, 2. Encoderâ€“Decoder | ğŸ”¸ KarÄ±ÅŸÄ±k | Ãœretim sÄ±rasÄ±nda baÄŸlam ve geÃ§miÅŸ kelimeleri birleÅŸtirir. |\n",
    "\n",
    "\n",
    "## ğŸ§  GÃ¶rsel Olarak\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db056877",
   "metadata": {},
   "source": [
    "```bash\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚        ENCODER           â”‚\n",
    "        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "Input â†’â”€â”€â”€â”€â–ºâ”‚ Multi-Head Attention â”‚ â”‚ (Self-Attention)\n",
    "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚ FFN â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚\n",
    "â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ DECODER â”‚\n",
    "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚ â”‚ Masked Self-Attentionâ”‚ â”‚ (GeleceÄŸe bakma engelli)\n",
    "â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚\n",
    "â”‚ â”‚ Encoderâ€“Decoder Att. â”‚ â”‚ (Cross-Attention)\n",
    "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚ FFN â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de37dc89",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ğŸ’¬ Ã–zet\n",
    "\n",
    "| BÃ¶lge | KullanÄ±lan Attention TÃ¼rÃ¼ | AmaÃ§ |\n",
    "|--------|-----------------------------|-------|\n",
    "| **Encoder** | Self-Attention | Girdiyi anlamlÄ± temsillere dÃ¶nÃ¼ÅŸtÃ¼rmek |\n",
    "| **Decoder** | Masked Self-Attention | Ã‡Ä±ktÄ±yÄ± adÄ±m adÄ±m Ã¼retmek |\n",
    "| **Decoder** | Encoderâ€“Decoder Attention | Girdiâ€“Ã§Ä±ktÄ± arasÄ±nda iliÅŸki kurmak |\n",
    "| **Her ikisinde** | Multi-Head Attention | FarklÄ± iliÅŸkileri aynÄ± anda Ã¶ÄŸrenmek |\n",
    "\n",
    "\n",
    "âœ… **SonuÃ§:**  \n",
    "Transformer mimarisi toplamda **3 temel attention mekanizmasÄ±** kullanÄ±r:  \n",
    "1. **Self-Attention (Encoder)**  \n",
    "2. **Masked Self-Attention (Decoder)**  \n",
    "3. **Encoderâ€“Decoder (Cross) Attention (Decoder)**  \n",
    "\n",
    "TÃ¼m bu attentionâ€™lar, **Multi-Head Attention** yapÄ±sÄ± iÃ§inde paralel olarak Ã§alÄ±ÅŸÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8229e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
