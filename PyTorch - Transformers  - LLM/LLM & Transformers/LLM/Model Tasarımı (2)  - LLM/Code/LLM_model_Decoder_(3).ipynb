{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccb4194",
   "metadata": {},
   "source": [
    "# ðŸ”¹ SEQ2SEQâ€™den LLMâ€™e DÃ¶nÃ¼ÅŸÃ¼m PlanÄ± ( DECODER )\n",
    "\n",
    "Bu plan, mevcut **SEQ2SEQ modelinizi** (LSTM + Attention) adÄ±m adÄ±m **GPT-style LLM**â€™e Ã§evirmek iÃ§in yol haritasÄ±nÄ± verir.\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ Encoder â†’ Transformer Encoder\n",
    "\n",
    "**Mevcut:** LSTM tabanlÄ±, bidirectional, opsiyonel attention  \n",
    "**DÃ¶nÃ¼ÅŸÃ¼m:**\n",
    "- LSTM katmanlarÄ± kaldÄ±rÄ±lÄ±r\n",
    "- Yerine N adet **Transformer Encoder Block** eklenir:\n",
    "  - Multi-Head Self-Attention\n",
    "  - FeedForward (GELU / ReLU)\n",
    "  - Residual + LayerNorm\n",
    "- Opsiyonel: Pre-Norm yapÄ±sÄ± tercih edilir\n",
    "\n",
    "\n",
    "\n",
    "## 2ï¸âƒ£ Decoder â†’ Transformer Decoder\n",
    "\n",
    "**Mevcut:** LSTM tabanlÄ±, cross-attention (encoder-decoder), Gated FFN  \n",
    "**DÃ¶nÃ¼ÅŸÃ¼m:**\n",
    "- LSTM yerine **masked self-attention** + feedforward\n",
    "- Encoder-Decoder attention â†’ Cross-Attention (attention over encoder output)\n",
    "- Gated FFN yerine klasik **FFN** veya GELU tabanlÄ± FFN kullanÄ±labilir\n",
    "- Positional Encoding ve Residual + LayerNorm korunur\n",
    "\n",
    "\n",
    "\n",
    "## 3ï¸âƒ£ Multi-Head Attention UyarlamasÄ±\n",
    "\n",
    "- Mevcut `MultiHeadAttention` sÄ±nÄ±fÄ± kullanÄ±labilir  \n",
    "- Ufak deÄŸiÅŸikliklerle:\n",
    "  - Masked attention eklenir (decoder iÃ§in)\n",
    "  - Causal mask uygulanÄ±r\n",
    "  - Key padding mask ve batch handling optimize edilir\n",
    "\n",
    "\n",
    "\n",
    "## 4ï¸âƒ£ Positional Encoding\n",
    "\n",
    "- Mevcut `PositionelEncoding` sÄ±nÄ±fÄ± doÄŸrudan kullanÄ±labilir  \n",
    "- Transformer block giriÅŸine eklenir:  \n",
    "  \\[\n",
    "  x = x + PE\n",
    "  \\]\n",
    "\n",
    "\n",
    "\n",
    "## 5ï¸âƒ£ Output / LM Head\n",
    "\n",
    "- Decoder Ã§Ä±kÄ±ÅŸÄ± â†’ Linear Layer â†’ Vocabulary boyutu\n",
    "- Softmax â†’ Token olasÄ±lÄ±klarÄ±\n",
    "- Ã–rnek: `nn.Linear(hidden_dim, vocab_size)`\n",
    "\n",
    "\n",
    "## 6ï¸âƒ£ Model BirleÅŸtirme\n",
    "\n",
    "- Embedding + Positional Encoding\n",
    "- N adet Transformer Block (encoder veya decoder)\n",
    "- Output projection (LM Head)\n",
    "- Causal mask + Cross-Attention mask uygulanarak **autoregressive text generation** yapÄ±lÄ±r\n",
    "\n",
    "\n",
    "## 7ï¸âƒ£ EÄŸitim ve Ä°nference\n",
    "\n",
    "- Loss: CrossEntropy (token-level)\n",
    "- Optimizer: Adam / AdamW\n",
    "- Masking:\n",
    "  - Decoder: causal mask\n",
    "  - Encoder-Decoder: key padding mask\n",
    "- Generation: greedy, top-k, top-p sampling\n",
    "\n",
    "\n",
    "\n",
    "## âœ… Ã–zet Yol HaritasÄ±\n",
    "\n",
    "1. **Encoder** â†’ Transformer bloklarÄ±\n",
    "2. **Decoder** â†’ Masked self-attention + Cross-attention + FFN\n",
    "3. **MultiHeadAttention** uyarlanÄ±r\n",
    "4. **Positional Encoding** eklenir\n",
    "5. **LM Head** eklenir\n",
    "6. **Loss + Masking** ile eÄŸitim yapÄ±lÄ±r\n",
    "7. **Autoregressive Generation** ile inference\n",
    "\n",
    "> Bu plan, mevcut SEQ2SEQ kodunuzu adÄ±m adÄ±m **GPT-style LLM**â€™e dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in eksiksiz bir yol haritasÄ±dÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0febc",
   "metadata": {},
   "source": [
    "----\n",
    "# ðŸ›  AdÄ±m 1: Embedding + Positional Encoding\n",
    "\n",
    "* Mevcut AdaptiveDecoder iÃ§indeki embedding ve PositionelEncoding katmanÄ±nÄ± kullanacaÄŸÄ±z.\n",
    "\n",
    "* Ama LSTM giriÅŸini direkt Transformer bloklarÄ±na vereceÄŸiz.\n",
    "\n",
    "> Bu aÅŸamada embedding boyutunu hidden_size ile eÅŸleÅŸtiriyoruz, bÃ¶ylece residual + attention uyumlu olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49b5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baba0d4",
   "metadata": {},
   "source": [
    "# ðŸ›  AdÄ±m 2: Multi-Head Attention (Self-Attention + Cross-Attention)\n",
    "\n",
    "* Decoder iÃ§in iki tip attention olacak:\n",
    "\n",
    "* Masked Self-Attention â†’ Kendi geÃ§miÅŸ tokenâ€™larÄ±na bakar\n",
    "\n",
    "* Encoder-Decoder Cross-Attention â†’ Encoder Ã§Ä±ktÄ±sÄ±na bakar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, C = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd616f",
   "metadata": {},
   "source": [
    "# ðŸ›  AdÄ±m 3: FeedForward + LayerNorm + Residual\n",
    "\n",
    "* Transformer blockâ€™un temel taÅŸÄ±\n",
    "\n",
    "* Decoderâ€™da hem self-attention hem cross-attention Ã§Ä±kÄ±ÅŸÄ±na uygulanacak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd5ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim  , expansion = 4 , dp =0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim,embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * expansion , embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "            )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        x = x + self.dp(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask))\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            x = x + self.dp(self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask))\n",
    "        # FeedForward\n",
    "        x = x + self.dp(self.ffn(self.norm3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee4ab7",
   "metadata": {},
   "source": [
    "* Bu adÄ±mla birlikte Transformer decoderâ€™un temeli hazÄ±r.\n",
    "---\n",
    "## Tam decoder kodunu aÅŸaÄŸÄ±ya bÄ±rakÄ±yorum\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d05ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884c2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25cb00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570f533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2ï¸âƒ£ Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, C = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b63ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3ï¸âƒ£ FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5822d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4ï¸âƒ£ Transformer Decoder Block\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        x = x + self.dp(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask))\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            x = x + self.dp(self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask))\n",
    "        # FeedForward\n",
    "        x = x + self.dp(self.ffn(self.norm3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67afbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Full Decoder Skeleton\n",
    "# -----------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22cb29",
   "metadata": {},
   "source": [
    "* TokenEmbedding + PositionalEncoding â†’ input tokenâ€™larÄ± vektÃ¶rleÅŸtiriyor\n",
    "\n",
    "* TransformerDecoderBlock â†’ masked self-attention + cross-attention + feedforward + residual + layernorm\n",
    "\n",
    "* nn.ModuleList ile N katman Ã¼st Ã¼ste\n",
    "\n",
    "* LM Head â†’ vocab boyutuna projection\n",
    "\n",
    "* forward fonksiyonu hem causal mask hem encoder attention iÃ§in hazÄ±r\n",
    "\n",
    "* Tek dikkat edilmesi gereken nokta:\n",
    "\n",
    "* self_mask â†’ decoderâ€™Ä±n kendi geÃ§miÅŸ tokenâ€™larÄ±nÄ± gÃ¶rmesi iÃ§in causal mask\n",
    "\n",
    "* enc_mask â†’ encoder padding mask, cross-attention sÄ±rasÄ±nda kullanÄ±lÄ±r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf90973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        x = x.transpose(1, 2).contiguous()  # [B, T, num_heads, head_dim]\n",
    "        B, T, _, _ = x.size()\n",
    "        return x.view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Decoder Block\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # masked self-attention (pre-norm)\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask))\n",
    "        # cross-attention (pre-norm) - enc_out: [B, T_enc, C]\n",
    "        if enc_out is not None:\n",
    "            x = x + self.dropout(self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask))\n",
    "        # feed-forward\n",
    "        x = x + self.dropout(self.ffn(self.norm3(x)))\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Causal mask builder\n",
    "# -----------------------------\n",
    "def build_causal_mask(seq_len, device=None, dtype=torch.uint8):\n",
    "    \"\"\"\n",
    "    Returns a lower-triangular mask of shape [seq_len, seq_len] with 1 for allowed positions.\n",
    "    mask[i, j] == 1  => query position i can attend to key position j (j <= i)\n",
    "    Use the same mask for all batches/heads (it will broadcast).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    mask = torch.tril(torch.ones((seq_len, seq_len), device=device, dtype=torch.uint8))\n",
    "    return mask  # 1/0\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Decoder\n",
    "# -----------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt_tokens, enc_out=None, tgt_mask=None, enc_mask=None):\n",
    "        \"\"\"\n",
    "        tgt_tokens: [B, T_tgt]\n",
    "        enc_out: [B, T_enc, C] or None\n",
    "        tgt_mask: causal mask shape [T_tgt, T_tgt] or [B, T_tgt, T_tgt] with 1 allowed / 0 masked\n",
    "        enc_mask: mask for cross-attn (1 allowed / 0 masked), broadcastable to attention scores\n",
    "        returns logits: [B, T_tgt, vocab_size]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(tgt_tokens)    # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out=enc_out, self_mask=tgt_mask, enc_mask=enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage helper\n",
    "# -----------------------------\n",
    "def make_causal_mask_for_batch(B, T, device):\n",
    "    # returns mask [B, T, T] (1/0) that is lower triangular for each batch\n",
    "    base = torch.tril(torch.ones((T, T), device=device, dtype=torch.uint8))\n",
    "    return base.unsqueeze(0).expand(B, -1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3fd2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Åžimdi Decoder yapÄ±sÄ±nÄ± daha ileriye taÅŸÄ±yalÄ±m.Ve elimizdeki modeli en optimum hale getirelim;\n",
    "> ## Konularla alakalÄ± daha net aÃ§Ä±klamalarÄ± bu dosyada bulabilirsiniz \n",
    "**LLM & Transformers\\LLM\\Model TasarÄ±mÄ±(2) - LLM\\Code\\LLM_model_Encoder_(2).ipynb**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedaf9cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6c63b",
   "metadata": {},
   "source": [
    "## AÅŸaÄŸÄ±ya direkt kod yapÄ±sÄ±nÄ±n iyileÅŸtirilmiÅŸ halini bÄ±rakÄ±yorum.Genel kod anlatÄ±mlarÄ±nÄ± zaten encoder bÃ¶lÃ¼mÃ¼nde yaptÄ±k.Oradan inceleyebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68cc8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding + Positional Encoding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward (SwiGLU/MLP)\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Modern Transformer Decoder Block\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        self_attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * self_attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_attn_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_attn_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Decoder\n",
    "# -----------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000, drop_path=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(embed_dim, num_heads, dp, drop_path) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a38b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4c871",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Full LLM Transformer Decoder\n",
    "\n",
    "Ã–zellikler:\n",
    "\n",
    "* Embed dim 1024, 12 katman (layers), 16 head\n",
    "\n",
    "* PreNorm + LayerNorm + Residual + DropPath + LayerScale\n",
    "\n",
    "* FeedForward geniÅŸletilmiÅŸ (expansion=8, opsiyonel SwiGLU)\n",
    "\n",
    "* Masked Self-Attention + Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf88ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Decoder Block (LLM)\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Decoder (LLM)\n",
    "# -----------------------------\n",
    "class TransformerDecoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b45891",
   "metadata": {},
   "source": [
    "# Transformer Decoder Block (LLM) AkÄ±ÅŸ DiyagramÄ± (Markdown)\n",
    "```bash\n",
    "\n",
    "Input Tokens (`x`)\n",
    "â”‚\n",
    "â”œâ”€> **Token Embedding**\n",
    "â”‚    - nn.Embedding(vocab_size, embed_dim)\n",
    "â”‚\n",
    "â”œâ”€> **Positional Encoding**\n",
    "â”‚    - sin/cos pozisyon vektÃ¶rleri eklenir\n",
    "â”‚\n",
    "â””â”€> **Decoder Layers** (N adet, Ã¶r: 12)\n",
    "     â”‚\n",
    "     â”œâ”€ **LayerNorm 1**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Masked Self-Attention**\n",
    "     â”‚          - MultiHeadAttention(embed_dim, num_heads)\n",
    "     â”‚          - Masked (future tokens ignored)\n",
    "     â”‚          - Dropout\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **DropPath + Î³â‚ scaling**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Residual Add** (x + scaled attn)\n",
    "     â”‚\n",
    "     â”œâ”€ **LayerNorm 2**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Cross-Attention (optional)**\n",
    "     â”‚          - MultiHeadAttention(embed_dim, num_heads)\n",
    "     â”‚          - Encoder output (`enc_out`) kullanÄ±lÄ±r\n",
    "     â”‚          - Mask optional\n",
    "     â”‚          - Dropout\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **DropPath + Î³â‚‚ scaling**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Residual Add**\n",
    "     â”‚\n",
    "     â”œâ”€ **LayerNorm 3**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **FeedForward Network (FFN)**\n",
    "     â”‚          - Linear â†’ GELU/SwiGLU â†’ Linear\n",
    "     â”‚          - Dropout\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **DropPath + Î³â‚ƒ scaling**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Residual Add**\n",
    "     â”‚\n",
    "     â””â”€> **Output of Layer** (`x` updated)\n",
    "â”‚\n",
    "â”œâ”€> **Final LayerNorm**\n",
    "â”‚\n",
    "â””â”€> **LM Head**\n",
    "     - Linear(embed_dim â†’ vocab_size)\n",
    "     - Output logits for next token prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd8b84",
   "metadata": {},
   "source": [
    "## AÃ§Ä±klama:\n",
    "\n",
    "* Input Tokens: Decoderâ€™a gelen token IDâ€™leri.\n",
    "\n",
    "* Embedding + Positional Encoding: Token vektÃ¶rleri ve pozisyon bilgisi.\n",
    "\n",
    "* Masked Self-Attention: Decoderâ€™Ä±n kendi Ã¶nceki tokenlerini dikkate alarak dikkat mekanizmasÄ±.\n",
    "\n",
    "* Cross-Attention: Encoder Ã§Ä±kÄ±ÅŸÄ± varsa ona bakarak dikkat (opsiyonel).\n",
    "\n",
    "* FeedForward: Lineer + aktivasyon + dropout katmanlarÄ±.\n",
    "\n",
    "* DropPath + Î³ scaling + Residual: Her blokta stochastic depth ve learnable scaling uygulanÄ±yor.\n",
    "\n",
    "* Ã‡Ä±kÄ±ÅŸ: Block sonrasÄ± gÃ¼ncellenmiÅŸ token temsil vektÃ¶rleri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46307bdf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
