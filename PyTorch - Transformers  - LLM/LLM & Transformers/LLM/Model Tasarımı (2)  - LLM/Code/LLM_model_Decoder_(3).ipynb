{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccb4194",
   "metadata": {},
   "source": [
    "# 🔹 SEQ2SEQ’den LLM’e Dönüşüm Planı ( DECODER )\n",
    "\n",
    "Bu plan, mevcut **SEQ2SEQ modelinizi** (LSTM + Attention) adım adım **GPT-style LLM**’e çevirmek için yol haritasını verir.\n",
    "\n",
    "\n",
    "## 1️⃣ Encoder → Transformer Encoder\n",
    "\n",
    "**Mevcut:** LSTM tabanlı, bidirectional, opsiyonel attention  \n",
    "**Dönüşüm:**\n",
    "- LSTM katmanları kaldırılır\n",
    "- Yerine N adet **Transformer Encoder Block** eklenir:\n",
    "  - Multi-Head Self-Attention\n",
    "  - FeedForward (GELU / ReLU)\n",
    "  - Residual + LayerNorm\n",
    "- Opsiyonel: Pre-Norm yapısı tercih edilir\n",
    "\n",
    "\n",
    "\n",
    "## 2️⃣ Decoder → Transformer Decoder\n",
    "\n",
    "**Mevcut:** LSTM tabanlı, cross-attention (encoder-decoder), Gated FFN  \n",
    "**Dönüşüm:**\n",
    "- LSTM yerine **masked self-attention** + feedforward\n",
    "- Encoder-Decoder attention → Cross-Attention (attention over encoder output)\n",
    "- Gated FFN yerine klasik **FFN** veya GELU tabanlı FFN kullanılabilir\n",
    "- Positional Encoding ve Residual + LayerNorm korunur\n",
    "\n",
    "\n",
    "\n",
    "## 3️⃣ Multi-Head Attention Uyarlaması\n",
    "\n",
    "- Mevcut `MultiHeadAttention` sınıfı kullanılabilir  \n",
    "- Ufak değişikliklerle:\n",
    "  - Masked attention eklenir (decoder için)\n",
    "  - Causal mask uygulanır\n",
    "  - Key padding mask ve batch handling optimize edilir\n",
    "\n",
    "\n",
    "\n",
    "## 4️⃣ Positional Encoding\n",
    "\n",
    "- Mevcut `PositionelEncoding` sınıfı doğrudan kullanılabilir  \n",
    "- Transformer block girişine eklenir:  \n",
    "  \\[\n",
    "  x = x + PE\n",
    "  \\]\n",
    "\n",
    "\n",
    "\n",
    "## 5️⃣ Output / LM Head\n",
    "\n",
    "- Decoder çıkışı → Linear Layer → Vocabulary boyutu\n",
    "- Softmax → Token olasılıkları\n",
    "- Örnek: `nn.Linear(hidden_dim, vocab_size)`\n",
    "\n",
    "\n",
    "## 6️⃣ Model Birleştirme\n",
    "\n",
    "- Embedding + Positional Encoding\n",
    "- N adet Transformer Block (encoder veya decoder)\n",
    "- Output projection (LM Head)\n",
    "- Causal mask + Cross-Attention mask uygulanarak **autoregressive text generation** yapılır\n",
    "\n",
    "\n",
    "## 7️⃣ Eğitim ve İnference\n",
    "\n",
    "- Loss: CrossEntropy (token-level)\n",
    "- Optimizer: Adam / AdamW\n",
    "- Masking:\n",
    "  - Decoder: causal mask\n",
    "  - Encoder-Decoder: key padding mask\n",
    "- Generation: greedy, top-k, top-p sampling\n",
    "\n",
    "\n",
    "\n",
    "## ✅ Özet Yol Haritası\n",
    "\n",
    "1. **Encoder** → Transformer blokları\n",
    "2. **Decoder** → Masked self-attention + Cross-attention + FFN\n",
    "3. **MultiHeadAttention** uyarlanır\n",
    "4. **Positional Encoding** eklenir\n",
    "5. **LM Head** eklenir\n",
    "6. **Loss + Masking** ile eğitim yapılır\n",
    "7. **Autoregressive Generation** ile inference\n",
    "\n",
    "> Bu plan, mevcut SEQ2SEQ kodunuzu adım adım **GPT-style LLM**’e dönüştürmek için eksiksiz bir yol haritasıdır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0febc",
   "metadata": {},
   "source": [
    "----\n",
    "# 🛠 Adım 1: Embedding + Positional Encoding\n",
    "\n",
    "* Mevcut AdaptiveDecoder içindeki embedding ve PositionelEncoding katmanını kullanacağız.\n",
    "\n",
    "* Ama LSTM girişini direkt Transformer bloklarına vereceğiz.\n",
    "\n",
    "> Bu aşamada embedding boyutunu hidden_size ile eşleştiriyoruz, böylece residual + attention uyumlu olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49b5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baba0d4",
   "metadata": {},
   "source": [
    "# 🛠 Adım 2: Multi-Head Attention (Self-Attention + Cross-Attention)\n",
    "\n",
    "* Decoder için iki tip attention olacak:\n",
    "\n",
    "* Masked Self-Attention → Kendi geçmiş token’larına bakar\n",
    "\n",
    "* Encoder-Decoder Cross-Attention → Encoder çıktısına bakar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, C = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd616f",
   "metadata": {},
   "source": [
    "# 🛠 Adım 3: FeedForward + LayerNorm + Residual\n",
    "\n",
    "* Transformer block’un temel taşı\n",
    "\n",
    "* Decoder’da hem self-attention hem cross-attention çıkışına uygulanacak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd5ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_dim  , expansion = 4 , dp =0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim,embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * expansion , embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "            )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        x = x + self.dp(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask))\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            x = x + self.dp(self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask))\n",
    "        # FeedForward\n",
    "        x = x + self.dp(self.ffn(self.norm3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee4ab7",
   "metadata": {},
   "source": [
    "* Bu adımla birlikte Transformer decoder’un temeli hazır.\n",
    "---\n",
    "## Tam decoder kodunu aşağıya bırakıyorum\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d05ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884c2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25cb00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570f533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2️⃣ Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, C = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b63ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3️⃣ FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5822d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4️⃣ Transformer Decoder Block\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        x = x + self.dp(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask))\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            x = x + self.dp(self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask))\n",
    "        # FeedForward\n",
    "        x = x + self.dp(self.ffn(self.norm3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67afbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Full Decoder Skeleton\n",
    "# -----------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22cb29",
   "metadata": {},
   "source": [
    "* TokenEmbedding + PositionalEncoding → input token’ları vektörleştiriyor\n",
    "\n",
    "* TransformerDecoderBlock → masked self-attention + cross-attention + feedforward + residual + layernorm\n",
    "\n",
    "* nn.ModuleList ile N katman üst üste\n",
    "\n",
    "* LM Head → vocab boyutuna projection\n",
    "\n",
    "* forward fonksiyonu hem causal mask hem encoder attention için hazır\n",
    "\n",
    "* Tek dikkat edilmesi gereken nokta:\n",
    "\n",
    "* self_mask → decoder’ın kendi geçmiş token’larını görmesi için causal mask\n",
    "\n",
    "* enc_mask → encoder padding mask, cross-attention sırasında kullanılır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf90973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        x = x.transpose(1, 2).contiguous()  # [B, T, num_heads, head_dim]\n",
    "        B, T, _, _ = x.size()\n",
    "        return x.view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Decoder Block\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # masked self-attention (pre-norm)\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask))\n",
    "        # cross-attention (pre-norm) - enc_out: [B, T_enc, C]\n",
    "        if enc_out is not None:\n",
    "            x = x + self.dropout(self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask))\n",
    "        # feed-forward\n",
    "        x = x + self.dropout(self.ffn(self.norm3(x)))\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Causal mask builder\n",
    "# -----------------------------\n",
    "def build_causal_mask(seq_len, device=None, dtype=torch.uint8):\n",
    "    \"\"\"\n",
    "    Returns a lower-triangular mask of shape [seq_len, seq_len] with 1 for allowed positions.\n",
    "    mask[i, j] == 1  => query position i can attend to key position j (j <= i)\n",
    "    Use the same mask for all batches/heads (it will broadcast).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    mask = torch.tril(torch.ones((seq_len, seq_len), device=device, dtype=torch.uint8))\n",
    "    return mask  # 1/0\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Decoder\n",
    "# -----------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt_tokens, enc_out=None, tgt_mask=None, enc_mask=None):\n",
    "        \"\"\"\n",
    "        tgt_tokens: [B, T_tgt]\n",
    "        enc_out: [B, T_enc, C] or None\n",
    "        tgt_mask: causal mask shape [T_tgt, T_tgt] or [B, T_tgt, T_tgt] with 1 allowed / 0 masked\n",
    "        enc_mask: mask for cross-attn (1 allowed / 0 masked), broadcastable to attention scores\n",
    "        returns logits: [B, T_tgt, vocab_size]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(tgt_tokens)    # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out=enc_out, self_mask=tgt_mask, enc_mask=enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage helper\n",
    "# -----------------------------\n",
    "def make_causal_mask_for_batch(B, T, device):\n",
    "    # returns mask [B, T, T] (1/0) that is lower triangular for each batch\n",
    "    base = torch.tril(torch.ones((T, T), device=device, dtype=torch.uint8))\n",
    "    return base.unsqueeze(0).expand(B, -1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3fd2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Şimdi Decoder yapısını daha ileriye taşıyalım.Ve elimizdeki modeli en optimum hale getirelim;\n",
    "> ## Konularla alakalı daha net açıklamaları bu dosyada bulabilirsiniz \n",
    "**LLM & Transformers\\LLM\\Model Tasarımı(2) - LLM\\Code\\LLM_model_Encoder_(2).ipynb**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedaf9cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6c63b",
   "metadata": {},
   "source": [
    "## Aşağıya direkt kod yapısının iyileştirilmiş halini bırakıyorum.Genel kod anlatımlarını zaten encoder bölümünde yaptık.Oradan inceleyebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68cc8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding + Positional Encoding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward (SwiGLU/MLP)\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Modern Transformer Decoder Block\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        self_attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * self_attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_attn_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_attn_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Decoder\n",
    "# -----------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000, drop_path=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(embed_dim, num_heads, dp, drop_path) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a38b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4c871",
   "metadata": {},
   "source": [
    "# 🔹 Full LLM Transformer Decoder\n",
    "\n",
    "Özellikler:\n",
    "\n",
    "* Embed dim 1024, 12 katman (layers), 16 head\n",
    "\n",
    "* PreNorm + LayerNorm + Residual + DropPath + LayerScale\n",
    "\n",
    "* FeedForward genişletilmiş (expansion=8, opsiyonel SwiGLU)\n",
    "\n",
    "* Masked Self-Attention + Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf88ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Decoder Block (LLM)\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Decoder (LLM)\n",
    "# -----------------------------\n",
    "class TransformerDecoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b45891",
   "metadata": {},
   "source": [
    "# Transformer Decoder Block (LLM) Akış Diyagramı (Markdown)\n",
    "```bash\n",
    "\n",
    "Input Tokens (`x`)\n",
    "│\n",
    "├─> **Token Embedding**\n",
    "│    - nn.Embedding(vocab_size, embed_dim)\n",
    "│\n",
    "├─> **Positional Encoding**\n",
    "│    - sin/cos pozisyon vektörleri eklenir\n",
    "│\n",
    "└─> **Decoder Layers** (N adet, ör: 12)\n",
    "     │\n",
    "     ├─ **LayerNorm 1**\n",
    "     │    │\n",
    "     │    └─> **Masked Self-Attention**\n",
    "     │          - MultiHeadAttention(embed_dim, num_heads)\n",
    "     │          - Masked (future tokens ignored)\n",
    "     │          - Dropout\n",
    "     │    │\n",
    "     │    └─> **DropPath + γ₁ scaling**\n",
    "     │    │\n",
    "     │    └─> **Residual Add** (x + scaled attn)\n",
    "     │\n",
    "     ├─ **LayerNorm 2**\n",
    "     │    │\n",
    "     │    └─> **Cross-Attention (optional)**\n",
    "     │          - MultiHeadAttention(embed_dim, num_heads)\n",
    "     │          - Encoder output (`enc_out`) kullanılır\n",
    "     │          - Mask optional\n",
    "     │          - Dropout\n",
    "     │    │\n",
    "     │    └─> **DropPath + γ₂ scaling**\n",
    "     │    │\n",
    "     │    └─> **Residual Add**\n",
    "     │\n",
    "     ├─ **LayerNorm 3**\n",
    "     │    │\n",
    "     │    └─> **FeedForward Network (FFN)**\n",
    "     │          - Linear → GELU/SwiGLU → Linear\n",
    "     │          - Dropout\n",
    "     │    │\n",
    "     │    └─> **DropPath + γ₃ scaling**\n",
    "     │    │\n",
    "     │    └─> **Residual Add**\n",
    "     │\n",
    "     └─> **Output of Layer** (`x` updated)\n",
    "│\n",
    "├─> **Final LayerNorm**\n",
    "│\n",
    "└─> **LM Head**\n",
    "     - Linear(embed_dim → vocab_size)\n",
    "     - Output logits for next token prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd8b84",
   "metadata": {},
   "source": [
    "## Açıklama:\n",
    "\n",
    "* Input Tokens: Decoder’a gelen token ID’leri.\n",
    "\n",
    "* Embedding + Positional Encoding: Token vektörleri ve pozisyon bilgisi.\n",
    "\n",
    "* Masked Self-Attention: Decoder’ın kendi önceki tokenlerini dikkate alarak dikkat mekanizması.\n",
    "\n",
    "* Cross-Attention: Encoder çıkışı varsa ona bakarak dikkat (opsiyonel).\n",
    "\n",
    "* FeedForward: Lineer + aktivasyon + dropout katmanları.\n",
    "\n",
    "* DropPath + γ scaling + Residual: Her blokta stochastic depth ve learnable scaling uygulanıyor.\n",
    "\n",
    "* Çıkış: Block sonrası güncellenmiş token temsil vektörleri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46307bdf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
