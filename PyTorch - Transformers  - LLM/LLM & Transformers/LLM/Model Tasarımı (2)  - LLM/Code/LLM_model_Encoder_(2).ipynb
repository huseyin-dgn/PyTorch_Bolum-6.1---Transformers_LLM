{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef930ea",
   "metadata": {},
   "source": [
    "# ðŸ›  Transformer Encoder YapÄ±sÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed6d1e",
   "metadata": {},
   "source": [
    "* Encoder, giriÅŸ tokenâ€™larÄ±nÄ± alÄ±r ve her token iÃ§in baÄŸlamÄ± (context) Ã¶ÄŸrenen bir gizli temsil Ã¼retir.\n",
    "Bu temsil, decoder tarafÄ±ndan kullanÄ±lacak ve cross-attention ile output Ã¼retilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a7ae4",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4966eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6dbe970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b2a9a",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f65d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T] (long)\n",
    "        return self.embedding(x)  # [B, T, embed_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c791139",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228c84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [B, T, C] -> [B, num_heads, T, head_dim]\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # x: [B, num_heads, T, head_dim] -> [B, T, C]\n",
    "        x = x.transpose(1, 2).contiguous()  # -> [B, T, num_heads, head_dim]\n",
    "        B, T, _, _ = x.size()\n",
    "        return x.view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [B, T, C]\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        # scores: [B, num_heads, T_q, T_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # mask: expected shape can be [T_q, T_k] or [B, T_q, T_k] or broadcastable.\n",
    "        if mask is not None:\n",
    "            # mask should be 1 for allowed positions and 0 for masked positions\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)  # [B, num_heads, T_q, head_dim]\n",
    "        out = self.combine_heads(out)  # [B, T_q, C]\n",
    "        return self.out_proj(out)     # [B, T_q, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a67817",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Position-wise FeedForward\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5220b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e14b00",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Transformer Encoder Block\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72606eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm style\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ffn(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b06c52",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Full Transformer Encoder\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d17904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: [B, T] (long)\n",
    "        src_mask: either None or mask with shape broadcastable to [B, num_heads, T, T] OR [T, T] or [B, T, T]\n",
    "                  mask values: 1 -> allowed, 0 -> masked\n",
    "        returns: encoder_outputs [B, T, C]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(src_tokens)       # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00595086",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Helper: Padding mask builder (optional)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1c1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_padding_mask(pad_mask):\n",
    "    \"\"\"\n",
    "    pad_mask: [B, T] where 1 means token is valid, 0 means padding\n",
    "    returns mask [B, 1, T, T] or broadcastable mask of 1/0\n",
    "    \"\"\"\n",
    "    # We want mask of shape [B, T, T] where allowed positions = 1\n",
    "    if pad_mask is None:\n",
    "        return None\n",
    "    B, T = pad_mask.shape\n",
    "    # allowed positions along keys dimension\n",
    "    mask = pad_mask.unsqueeze(1) * pad_mask.unsqueeze(2)  # [B, T, T]\n",
    "    return mask  # 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92689b85",
   "metadata": {},
   "source": [
    "# TAM KOD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087edf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T] (long)\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [B, T, C] -> [B, num_heads, T, head_dim]\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # x: [B, num_heads, T, head_dim] -> [B, T, C]\n",
    "        x = x.transpose(1, 2).contiguous()  # -> [B, T, num_heads, head_dim]\n",
    "        B, T, _, _ = x.size()\n",
    "        return x.view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [B, T, C]\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        # scores: [B, num_heads, T_q, T_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # mask: expected shape can be [T_q, T_k] or [B, T_q, T_k] or broadcastable.\n",
    "        if mask is not None:\n",
    "            # mask should be 1 for allowed positions and 0 for masked positions\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)  # [B, num_heads, T_q, head_dim]\n",
    "        out = self.combine_heads(out)  # [B, T_q, C]\n",
    "        return self.out_proj(out)     # [B, T_q, C]\n",
    "\n",
    "# -----------------------------\n",
    "# Position-wise FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Encoder Block\n",
    "# -----------------------------\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm style\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Encoder\n",
    "# -----------------------------\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: [B, T] (long)\n",
    "        src_mask: either None or mask with shape broadcastable to [B, num_heads, T, T] OR [T, T] or [B, T, T]\n",
    "                  mask values: 1 -> allowed, 0 -> masked\n",
    "        returns: encoder_outputs [B, T, C]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(src_tokens)       # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Padding mask builder (optional)\n",
    "# -----------------------------\n",
    "def build_padding_mask(pad_mask):\n",
    "    \"\"\"\n",
    "    pad_mask: [B, T] where 1 means token is valid, 0 means padding\n",
    "    returns mask [B, 1, T, T] or broadcastable mask of 1/0\n",
    "    \"\"\"\n",
    "    # We want mask of shape [B, T, T] where allowed positions = 1\n",
    "    if pad_mask is None:\n",
    "        return None\n",
    "    B, T = pad_mask.shape\n",
    "    # allowed positions along keys dimension\n",
    "    mask = pad_mask.unsqueeze(1) * pad_mask.unsqueeze(2)  # [B, T, T]\n",
    "    return mask  # 1/0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363a522",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder YapÄ±sÄ±nÄ± daha ileriye taÅŸÄ±yalÄ±m.BazÄ± eklentiler ekleyeceÄŸiz.Bu eklentilerin ne olduÄŸunu ve en sonda modelin tamamÄ±nÄ± daha da ileriye taÅŸÄ±mÄ±ÅŸ olalaÄ±m.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15c843",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ DropPath (Stochastic Depth)\n",
    "\n",
    "* Derin Transformerâ€™larda bazÄ± bloklarÄ± rastgele bypass etmek iÃ§in kullanÄ±lÄ±r.\n",
    "\n",
    "* Training stabilitesini ve genelleme performansÄ±nÄ± artÄ±rÄ±r.\n",
    "\n",
    "Kod: DropPath sÄ±nÄ±fÄ±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76860861",
   "metadata": {},
   "source": [
    "### DropPath (Stochastic Depth) Nedir?\n",
    "\n",
    "* DropPath, derin Transformerâ€™larda bazÄ± bloklarÄ±n Ã§Ä±kÄ±ÅŸÄ±nÄ± rastgele sÄ±fÄ±rlayÄ±p bypass etmek iÃ§in kullanÄ±lÄ±r.\n",
    "\n",
    "* Bu, Ã§ok derin aÄŸlarda training stabilitesini artÄ±rÄ±r ve overfittingâ€™i azaltÄ±r.\n",
    "\n",
    "* FarkÄ± Dropoutâ€™tan: Dropout bir tensor iÃ§indeki elemanlarÄ± rastgele sÄ±fÄ±rlar; DropPath bir bloÄŸu tamamen atlatÄ±r (residual yol Ã¼zerinden)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bd0c7",
   "metadata": {},
   "source": [
    "#### Nereye entegre edeceÄŸiz?\n",
    "\n",
    "Bir Transformer Encoder bloÄŸu tipik olarak ÅŸÃ¶yle gÃ¶rÃ¼nÃ¼r:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5d643",
   "metadata": {},
   "source": [
    "```python \n",
    "x = x + Attention(LayerNorm(x))\n",
    "x = x + FFN(LayerNorm(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca218a2",
   "metadata": {},
   "source": [
    "* Burada Residual + PreNorm mevcut.\n",
    "\n",
    "#### DropPathâ€™i entegre etmek iÃ§in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d3758",
   "metadata": {},
   "source": [
    "```python \n",
    "x = x + drop_path(Attention(LayerNorm(x)))\n",
    "x = x + drop_path(FFN(LayerNorm(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a1d22",
   "metadata": {},
   "source": [
    "## Kod Ã–rneÄŸi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f65374bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670dd1f6",
   "metadata": {},
   "source": [
    "## Encoder Blok Ã–rneÄŸi ile KullanÄ±mÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebe50773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # PreNorm + DropPath\n",
    "        x = x + self.drop_path(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask))\n",
    "        x = x + self.drop_path(self.ffn(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb79cf5",
   "metadata": {},
   "source": [
    "### âœ… Ã–zet:\n",
    "\n",
    "* DropPathâ€™i her residual connectionâ€™a ekliyoruz.\n",
    "\n",
    "* drop_prob ile ne kadar blok rastgele bypass edileceÄŸini kontrol ediyoruz.\n",
    "\n",
    "* Training sÄ±rasÄ±nda aktif, eval modunda devre dÄ±ÅŸÄ±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91ad4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da655b0",
   "metadata": {},
   "source": [
    "# 2ï¸âƒ£ LayerScale Nedir?\n",
    "\n",
    "* Transformer bloklarÄ±nÄ±n Ã§Ä±kÄ±ÅŸÄ±nÄ± kÃ¼Ã§Ã¼k bir katsayÄ± ile Ã§arpar (alpha * block_out).\n",
    "\n",
    "* Derin modellerde, Ã¶zellikle LLMâ€™lerde, training stabilitesini artÄ±rÄ±r.\n",
    "\n",
    "* KÃ¼Ã§Ã¼k bir alpha (Ã¶rn. 1e-4 ~ 1e-2) ile baÅŸlatÄ±lÄ±r ve Ã¶ÄŸrenilebilir parametredir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377eb221",
   "metadata": {},
   "source": [
    "### Nereye entegre ediyoruz?\n",
    "\n",
    "* Ã–nceki DropPathâ€™li blokta her residual connection Ã¼zerine uygulayabiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d62b3",
   "metadata": {},
   "source": [
    "```python \n",
    "# x = x + drop_path(block_out)\n",
    "# â†’ LayerScale ile\n",
    "x = x + drop_path(alpha * block_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9985cf5",
   "metadata": {},
   "source": [
    "* alpha tensor olarak embed_dim boyutunda ve learnable olabilir.\n",
    "\n",
    "* Genellikle her blok iÃ§in ayrÄ± alpha kullanÄ±lÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68ae6b",
   "metadata": {},
   "source": [
    "### **Kod Ã–rneÄŸi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b75d1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        # LayerScale\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention + LayerScale + DropPath\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # FFN + LayerScale + DropPath\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51de558",
   "metadata": {},
   "source": [
    "## âœ… Ã–zet:\n",
    "\n",
    "* gamma_1 ve gamma_2 â†’ learnable scale parametreleri\n",
    "\n",
    "* DropPath + LayerScale â†’ modern LLM bloklarÄ±nÄ±n temel stabilite mekanizmasÄ±\n",
    "\n",
    "* Training sÄ±rasÄ±nda aktif, eval modunda normal residual gibi Ã§alÄ±ÅŸÄ±yor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0b4a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c751c3",
   "metadata": {},
   "source": [
    "# 3ï¸âƒ£ FFN: GeLU â†’ SwiGLU (Modern LLM)\n",
    "### Neden SwiGLU?\n",
    "\n",
    "* GeLU: klasik Transformer FFN aktivasyonu.\n",
    "\n",
    "* SwiGLU (Gated Linear Unit):\n",
    "\n",
    "> Daha hÄ±zlÄ± Ã¶ÄŸrenme saÄŸlar\n",
    "\n",
    "> Daha iyi genelleme (Ã¶zellikle LLMâ€™lerde)\n",
    "\n",
    "> Modern LLMâ€™lerde standart (GPT-NeoX, LLaMA, Falcon vb.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86438a99",
   "metadata": {},
   "source": [
    "### **Kod Ã–rneÄŸi: SwiGLU FFN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "331f58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = embed_dim * expansion\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.act = nn.GELU()  # GeLU aktivasyonu\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w3(self.act(self.w2(x)) * self.w1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74231ffd",
   "metadata": {},
   "source": [
    "### **Encoder Blokta KullanÄ±mÄ±**\n",
    "\n",
    "LayerScale ve DropPath ile kombine edersek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6bd2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = SwiGLUFFN(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "        # LayerScale\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention + LayerScale + DropPath\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # SwiGLU FFN + LayerScale + DropPath\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd950ffa",
   "metadata": {},
   "source": [
    "## âœ… Ã–zet:\n",
    "\n",
    "* FFN artÄ±k SwiGLU tabanlÄ± â†’ LLM standardÄ±\n",
    "\n",
    "* LayerScale + DropPath ile residual baÄŸlantÄ±lar stabilize edilmiÅŸ\n",
    "\n",
    "* Self-Attention ve FFN iÃ§in modern LLM tasarÄ±mÄ± tamamlanmÄ±ÅŸ oldu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e785f",
   "metadata": {},
   "source": [
    "----\n",
    "# 4ï¸âƒ£ Rotary Positional Encoding (RoPE)\n",
    "### Neden RoPE?\n",
    "\n",
    "* Standart sin/cos positional encoding yalnÄ±zca ekleme ile Ã§alÄ±ÅŸÄ±r.\n",
    "\n",
    "* RoPE, attention mekanizmasÄ±na rotasyon matrisi ile pozisyon bilgisi ekler.\n",
    "\n",
    "* Uzun dizilerde genelleme performansÄ±nÄ± artÄ±rÄ±r.\n",
    "\n",
    "* Modern LLMâ€™lerin Ã§oÄŸu (LLaMA, Falcon, MPT) kullanÄ±yor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632844d",
   "metadata": {},
   "source": [
    "### **Kod Ã–rneÄŸi: Rotary Embedding UygulamasÄ±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5c5b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, num_heads, T, head_dim]\n",
    "        B, H, T, D = x.shape\n",
    "        t = torch.arange(T, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # [T, D/2]\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)  # [T, D]\n",
    "        cos = emb.cos()[None, None, :, :]  # [1,1,T,D]\n",
    "        sin = emb.sin()[None, None, :, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x_rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d4029",
   "metadata": {},
   "source": [
    "### **Attention Blokta KullanÄ±mÄ±**\n",
    "\n",
    "* MultiHeadAttention iÃ§inde query ve keyâ€™lere uygulayabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08056529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        if use_rope:\n",
    "            self.rope = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        if self.use_rope:\n",
    "            Q = self.rope(Q)\n",
    "            K = self.rope(K)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70770562",
   "metadata": {},
   "source": [
    "## âœ… Ã–zet:\n",
    "\n",
    "* RotaryEmbedding query ve keyâ€™e uygulanÄ±r â†’ pozisyon bilgisi attention matrisine doÄŸal olarak eklenir\n",
    "\n",
    "* Uzun dizilerde standart sin/cosâ€™dan daha iyi genelleme\n",
    "\n",
    "* Modern LLMâ€™lerde self-attentionâ€™Ä± gÃ¼Ã§lendiren bir adÄ±m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1267798",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7738e16",
   "metadata": {},
   "source": [
    "# YukarÄ±da bulunan eklentileri encoder yapÄ±mÄ±za entegre edelim:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe677bbc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cf044e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526a8f5",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ddcf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4dc30",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dff1bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4d381",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9018b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a334e",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Rotary Positional Encoding (RoPE)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8726ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, T, D = x.shape\n",
    "        t = torch.arange(T, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos = emb.cos()[None, None, :, :]\n",
    "        sin = emb.sin()[None, None, :, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b317250",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89595b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        if use_rope:\n",
    "            self.rope = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        if self.use_rope:\n",
    "            Q = self.rope(Q)\n",
    "            K = self.rope(K)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f07a43",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# SwiGLU FFN\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b6b190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = embed_dim * expansion\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w3(self.act(self.w2(x)) * self.w1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b7eb3",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Transformer Encoder Block\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f74860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp, use_rope=use_rope)\n",
    "        self.ffn = SwiGLUFFN(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c75937",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Full Transformer Encoder\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a52796e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=12, num_heads=8, dp=0.1, drop_path=0.1, max_len=5000, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, dp, drop_path, use_rope=use_rope)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54e43d",
   "metadata": {},
   "source": [
    "# TAM KOD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6527aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, T, D = x.shape\n",
    "        t = torch.arange(T, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos = emb.cos()[None, None, :, :]\n",
    "        sin = emb.sin()[None, None, :, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        if use_rope:\n",
    "            self.rope = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        if self.use_rope:\n",
    "            Q = self.rope(Q)\n",
    "            K = self.rope(K)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = embed_dim * expansion\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w3(self.act(self.w2(x)) * self.w1(x)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp, use_rope=use_rope)\n",
    "        self.ffn = SwiGLUFFN(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=12, num_heads=8, dp=0.1, drop_path=0.1, max_len=5000, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, dp, drop_path, use_rope=use_rope)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1ea76",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder iÃ§in LLM Ä°yileÅŸtirmeleri\n",
    "\n",
    "## 1ï¸âƒ£ Embed Dim ve Layer SayÄ±sÄ±\n",
    "- Mevcut: `embed_dim=512`, `num_layers=6`\n",
    "- Ã–neri: `embed_dim=1024`, `num_layers=12`\n",
    "\n",
    "## 2ï¸âƒ£ Attention Heads\n",
    "- Mevcut: 8 head\n",
    "- Ã–neri: 16 head  \n",
    "  _(embed_dim / num_heads = head_dim)_\n",
    "\n",
    "## 3ï¸âƒ£ FeedForward\n",
    "- Expansion: 4 â†’ 4-8\n",
    "- Aktivasyon: GELU veya SwiGLU\n",
    "\n",
    "## 4ï¸âƒ£ Dropout + DropPath + LayerScale\n",
    "- AmaÃ§: Training stabilitesi ve genelleme performansÄ±nÄ± artÄ±rmak\n",
    "- Eklenmesi gerekenler: \n",
    "  - Dropout\n",
    "  - DropPath (Stochastic Depth)\n",
    "  - LayerScale parametreleri\n",
    "\n",
    "## 5ï¸âƒ£ Positional Encoding\n",
    "- Rotary Embedding veya scalable sin/cos\n",
    "- Åžimdilik mevcut sin/cosâ€™u geliÅŸtirmek yeterli\n",
    "\n",
    "## 6ï¸âƒ£ Norm & Stabilite\n",
    "- PreNorm + LayerNorm + residual connection\n",
    "- Her blokta attention ve feedforward iÃ§in uygulanacak\n",
    "\n",
    "## 7ï¸âƒ£ Optimizer ve FP16 UyumluluÄŸu\n",
    "- Training kÄ±smÄ± iÃ§in: \n",
    "  - FP16 ile uyumlu\n",
    "  - AdamW veya Lion optimizer tercih edilebilir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db210c6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1ï¸âƒ£ Token Embedding ve Embed Dim YÃ¼kseltme\n",
    "\n",
    "* AmaÃ§: LLM seviyesinde daha geniÅŸ bir temsil gÃ¼cÃ¼ saÄŸlamak iÃ§in embed_dimâ€™i 1024 yapÄ±yoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e936b58",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74873c50",
   "metadata": {},
   "source": [
    "### Notlar:\n",
    "\n",
    "* embed_dim=1024 â†’ head sayÄ±sÄ± ve feedforward expansion ile uyumlu olmalÄ±.\n",
    "\n",
    "* Bu embedding her tokenâ€™Ä± dense vektÃ¶re Ã§evirir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8c275",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204b32e",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Positional Encoding (GeliÅŸtirilmiÅŸ Sin/Cos)\n",
    "\n",
    "* AmaÃ§: Uzun dizilerde modelin konum bilgisini daha iyi yakalamasÄ±. Mevcut sin/cosâ€™u kullanacaÄŸÄ±z ama embed_dim=1024 ile uyumlu hale getireceÄŸiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6038ad0",
   "metadata": {},
   "source": [
    "```python \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e0d36",
   "metadata": {},
   "source": [
    "### Notlar:\n",
    "\n",
    "* max_len uzun diziler iÃ§in bÃ¼yÃ¼tÃ¼ldÃ¼ (Ã¶rneÄŸin 10k token).\n",
    "\n",
    "* embed_dim=1024 ile uyumlu.\n",
    "\n",
    "* Ä°leride rotary embeddings ile deÄŸiÅŸtirilebilir ama ÅŸimdilik scalable sin/cos yeterli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a3725",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e90f2",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Multi-Head Attention (16 Head, LLM)\n",
    "\n",
    "* AmaÃ§: Daha bÃ¼yÃ¼k embedding ve daha fazla baÅŸ (head) ile modelin baÄŸlamÄ± yakalama kapasitesini artÄ±rmak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96285d09",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db817350",
   "metadata": {},
   "source": [
    "### Ã–zellikler:\n",
    "\n",
    "* embed_dim=1024 â†’ daha geniÅŸ embedding\n",
    "\n",
    "* num_heads=16 â†’ daha fazla dikkat baÅŸÄ±\n",
    "\n",
    "* self.scale = head_dim ** -0.5 ile stabilizasyon\n",
    "\n",
    "* Dropout uygulanÄ±yor, training sÄ±rasÄ±nda regularizasyon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de9d40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bc1f1",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ FeedForward (LLM)\n",
    "\n",
    "* AmaÃ§: Daha gÃ¼Ã§lÃ¼ ve stabil feedforward katmanÄ±, geniÅŸletilmiÅŸ boyut ve yeni aktivasyon (GELU veya SwiGLU) ile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5293378",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        inner_dim = embed_dim * expansion\n",
    "        self.use_swiglu = use_swiglu\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        if use_swiglu:\n",
    "            # SwiGLU: GELU gating mekanizmasÄ±\n",
    "            self.fc1 = nn.Linear(embed_dim, inner_dim * 2)  # ikiye katla\n",
    "            self.fc2 = nn.Linear(inner_dim, embed_dim)\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, inner_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(inner_dim, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        # LayerScale parametresi\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_swiglu:\n",
    "            x1, x2 = self.fc1(x).chunk(2, dim=-1)\n",
    "            x = F.gelu(x1) * x2\n",
    "            x = self.fc2(x)\n",
    "            x = self.gamma * x\n",
    "            return self.dropout(x)\n",
    "        else:\n",
    "            return self.gamma * self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72b26d",
   "metadata": {},
   "source": [
    "### Ã–zellikler:\n",
    "\n",
    "* expansion=8 â†’ daha bÃ¼yÃ¼k hidden dimension\n",
    "\n",
    "* use_swiglu=True â†’ SwiGLU aktivasyonu (GELU gating)\n",
    "\n",
    "* Dropout ile regularizasyon\n",
    "\n",
    "* LayerScale ile training stabilitesi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fde4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141f368",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Transformer Encoder Block (LLM)\n",
    "\n",
    "AmaÃ§:\n",
    "\n",
    "* PreNorm + Residual Connection\n",
    "\n",
    "* MultiHeadAttention (16 head, embed_dim=1024)\n",
    "\n",
    "* GeliÅŸmiÅŸ FeedForward (expansion=8, SwiGLU opsiyonel)\n",
    "\n",
    "* Dropout + DropPath + LayerScale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211ca47",
   "metadata": {},
   "source": [
    "```python \n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic Depth / DropPath\"\"\"\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "\n",
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads=num_heads, dp=dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion=expansion, dp=dp, use_swiglu=use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        # LayerScale parametreleri\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # PreNorm + Self-Attention + Residual + DropPath\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # FeedForward + Residual + DropPath\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb56f6",
   "metadata": {},
   "source": [
    "## Ã–zellikler:\n",
    "\n",
    "* PreNorm: LayerNorm Ã¶nce uygulanÄ±r â†’ training stabilitesi\n",
    "\n",
    "* DropPath: Stochastic depth ile bloklar rastgele bypass edilebilir\n",
    "\n",
    "* LayerScale (gamma_1, gamma_2) â†’ derin modellerde residual scaling\n",
    "\n",
    "* FeedForward: expansion=8 ve opsiyonel SwiGLU\n",
    "\n",
    "* MultiHeadAttention: embed_dim / num_heads = head_dim, Ã¶rneÄŸin 1024/16 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3124c1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df8f81",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Full Transformer Encoder (LLM)\n",
    "\n",
    "Ã–zellikler:\n",
    "\n",
    "* TokenEmbedding + PositionalEncoding\n",
    "\n",
    "* 12 adet LLM Encoder Block (TransformerEncoderBlockLLM)\n",
    "\n",
    "* LayerNorm sonunda\n",
    "\n",
    "* DropPath ve LayerScale aktif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7de816",
   "metadata": {},
   "source": [
    "```python \n",
    "class TransformerEncoderLLM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim=1024, \n",
    "        num_layers=12, \n",
    "        num_heads=16, \n",
    "        dp=0.1, \n",
    "        drop_path=0.1, \n",
    "        expansion=8, \n",
    "        max_len=5000,\n",
    "        use_swiglu=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlockLLM(\n",
    "                embed_dim=embed_dim, \n",
    "                num_heads=num_heads, \n",
    "                dp=dp, \n",
    "                drop_path=drop_path, \n",
    "                expansion=expansion,\n",
    "                use_swiglu=use_swiglu\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: [B, T] (long)\n",
    "        src_mask: mask shape broadcastable to [B, num_heads, T, T] OR [T, T] or [B, T, T]\n",
    "                  1 -> allowed, 0 -> masked\n",
    "        returns: encoder_outputs [B, T, C]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(src_tokens)       # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e58f8",
   "metadata": {},
   "source": [
    "## Ã–zellikler:\n",
    "\n",
    "* 12 layer + 16 head + embed_dim=1024 â†’ LLM Ã¶lÃ§eÄŸinde\n",
    "\n",
    "* PreNorm + DropPath + LayerScale + Residual\n",
    "\n",
    "* FeedForward geniÅŸletilmiÅŸ (expansion=8, opsiyonel SwiGLU)\n",
    "\n",
    "* PositionalEncoding: sin/cos, scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd3a4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601e663",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Full LLM Transformer Encoder\n",
    "\n",
    "Ã–zellikler:\n",
    "\n",
    "* Embed dim 1024, 12 katman (layers), 16 head\n",
    "\n",
    "* PreNorm + LayerNorm + Residual + DropPath + LayerScale\n",
    "\n",
    "* FeedForward geniÅŸletilmiÅŸ (expansion=8, opsiyonel SwiGLU)\n",
    "\n",
    "* Positional Encoding: sin/cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Encoder Block (LLM)\n",
    "# -----------------------------\n",
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Encoder (LLM)\n",
    "# -----------------------------\n",
    "class TransformerEncoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13a3d0",
   "metadata": {},
   "source": [
    "# Transformer Encoder Block (LLM) AkÄ±ÅŸ DiyagramÄ± (Markdown)\n",
    "```bash\n",
    "\n",
    "Input Tokens (`src_tokens`)\n",
    "â”‚\n",
    "â”œâ”€> **Token Embedding**\n",
    "â”‚    - nn.Embedding(vocab_size, embed_dim)\n",
    "â”‚\n",
    "â”œâ”€> **Positional Encoding**\n",
    "â”‚    - sin/cos pozisyon vektÃ¶rleri eklenir\n",
    "â”‚\n",
    "â””â”€> **Encoder Layers** (N adet, Ã¶r: 12)\n",
    "     â”‚\n",
    "     â”œâ”€ **LayerNorm 1**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Self-Attention**\n",
    "     â”‚          - MultiHeadAttention(embed_dim, num_heads)\n",
    "     â”‚          - Mask optional (src_mask)\n",
    "     â”‚          - Dropout\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **DropPath + Î³â‚ scaling**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Residual Add** (x + scaled attn)\n",
    "     â”‚\n",
    "     â”œâ”€ **LayerNorm 2**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **FeedForward Network (FFN)**\n",
    "     â”‚          - Linear â†’ GELU/SwiGLU â†’ Linear\n",
    "     â”‚          - Dropout\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **DropPath + Î³â‚‚ scaling**\n",
    "     â”‚    â”‚\n",
    "     â”‚    â””â”€> **Residual Add**\n",
    "     â”‚\n",
    "     â””â”€> **Output of Layer** (`x` updated)\n",
    "â”‚\n",
    "â”œâ”€> **Final LayerNorm**\n",
    "â”‚\n",
    "â””â”€> **Encoder Output**\n",
    "     - `x` (embedding boyutu: batch_size Ã— seq_len Ã— embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5ce37",
   "metadata": {},
   "source": [
    "## AÃ§Ä±klama:\n",
    "\n",
    "* Token Embedding + Positional Encoding: Token IDâ€™lerini vektÃ¶re Ã§evirir ve pozisyon bilgisi ekler.\n",
    "\n",
    "* Self-Attention: Sadece encoderâ€™daki tokenler arasÄ±nda dikkat (mask opsiyonel).\n",
    "\n",
    "* FeedForward (FFN): Lineer + GELU/SwiGLU + Dropout.\n",
    "\n",
    "* Residual + DropPath: Stochastic depth ve learnable scaling.\n",
    "\n",
    "* Final LayerNorm: Katman Ã§Ä±kÄ±ÅŸÄ±nÄ± normalize eder.\n",
    "\n",
    "* Encoder Output: Decoderâ€™a girdi olarak veya baÅŸka downstream gÃ¶revlerde kullanÄ±labilir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
