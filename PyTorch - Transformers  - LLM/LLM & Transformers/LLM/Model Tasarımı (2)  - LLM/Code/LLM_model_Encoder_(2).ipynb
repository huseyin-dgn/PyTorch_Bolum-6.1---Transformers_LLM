{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef930ea",
   "metadata": {},
   "source": [
    "# 🛠 Transformer Encoder Yapısı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed6d1e",
   "metadata": {},
   "source": [
    "* Encoder, giriş token’larını alır ve her token için bağlamı (context) öğrenen bir gizli temsil üretir.\n",
    "Bu temsil, decoder tarafından kullanılacak ve cross-attention ile output üretilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a7ae4",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4966eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6dbe970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b2a9a",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f65d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T] (long)\n",
    "        return self.embedding(x)  # [B, T, embed_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c791139",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228c84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [B, T, C] -> [B, num_heads, T, head_dim]\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # x: [B, num_heads, T, head_dim] -> [B, T, C]\n",
    "        x = x.transpose(1, 2).contiguous()  # -> [B, T, num_heads, head_dim]\n",
    "        B, T, _, _ = x.size()\n",
    "        return x.view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [B, T, C]\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        # scores: [B, num_heads, T_q, T_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # mask: expected shape can be [T_q, T_k] or [B, T_q, T_k] or broadcastable.\n",
    "        if mask is not None:\n",
    "            # mask should be 1 for allowed positions and 0 for masked positions\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)  # [B, num_heads, T_q, head_dim]\n",
    "        out = self.combine_heads(out)  # [B, T_q, C]\n",
    "        return self.out_proj(out)     # [B, T_q, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a67817",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Position-wise FeedForward\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5220b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e14b00",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Transformer Encoder Block\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72606eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm style\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ffn(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b06c52",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Full Transformer Encoder\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d17904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: [B, T] (long)\n",
    "        src_mask: either None or mask with shape broadcastable to [B, num_heads, T, T] OR [T, T] or [B, T, T]\n",
    "                  mask values: 1 -> allowed, 0 -> masked\n",
    "        returns: encoder_outputs [B, T, C]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(src_tokens)       # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00595086",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Helper: Padding mask builder (optional)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1c1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_padding_mask(pad_mask):\n",
    "    \"\"\"\n",
    "    pad_mask: [B, T] where 1 means token is valid, 0 means padding\n",
    "    returns mask [B, 1, T, T] or broadcastable mask of 1/0\n",
    "    \"\"\"\n",
    "    # We want mask of shape [B, T, T] where allowed positions = 1\n",
    "    if pad_mask is None:\n",
    "        return None\n",
    "    B, T = pad_mask.shape\n",
    "    # allowed positions along keys dimension\n",
    "    mask = pad_mask.unsqueeze(1) * pad_mask.unsqueeze(2)  # [B, T, T]\n",
    "    return mask  # 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92689b85",
   "metadata": {},
   "source": [
    "# TAM KOD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087edf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T] (long)\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [B, T, C] -> [B, num_heads, T, head_dim]\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # x: [B, num_heads, T, head_dim] -> [B, T, C]\n",
    "        x = x.transpose(1, 2).contiguous()  # -> [B, T, num_heads, head_dim]\n",
    "        B, T, _, _ = x.size()\n",
    "        return x.view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [B, T, C]\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        # scores: [B, num_heads, T_q, T_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # mask: expected shape can be [T_q, T_k] or [B, T_q, T_k] or broadcastable.\n",
    "        if mask is not None:\n",
    "            # mask should be 1 for allowed positions and 0 for masked positions\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)  # [B, num_heads, T_q, head_dim]\n",
    "        out = self.combine_heads(out)  # [B, T_q, C]\n",
    "        return self.out_proj(out)     # [B, T_q, C]\n",
    "\n",
    "# -----------------------------\n",
    "# Position-wise FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Encoder Block\n",
    "# -----------------------------\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm style\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Encoder\n",
    "# -----------------------------\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: [B, T] (long)\n",
    "        src_mask: either None or mask with shape broadcastable to [B, num_heads, T, T] OR [T, T] or [B, T, T]\n",
    "                  mask values: 1 -> allowed, 0 -> masked\n",
    "        returns: encoder_outputs [B, T, C]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(src_tokens)       # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Padding mask builder (optional)\n",
    "# -----------------------------\n",
    "def build_padding_mask(pad_mask):\n",
    "    \"\"\"\n",
    "    pad_mask: [B, T] where 1 means token is valid, 0 means padding\n",
    "    returns mask [B, 1, T, T] or broadcastable mask of 1/0\n",
    "    \"\"\"\n",
    "    # We want mask of shape [B, T, T] where allowed positions = 1\n",
    "    if pad_mask is None:\n",
    "        return None\n",
    "    B, T = pad_mask.shape\n",
    "    # allowed positions along keys dimension\n",
    "    mask = pad_mask.unsqueeze(1) * pad_mask.unsqueeze(2)  # [B, T, T]\n",
    "    return mask  # 1/0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363a522",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder Yapısını daha ileriye taşıyalım.Bazı eklentiler ekleyeceğiz.Bu eklentilerin ne olduğunu ve en sonda modelin tamamını daha da ileriye taşımış olalaım.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15c843",
   "metadata": {},
   "source": [
    "# 1️⃣ DropPath (Stochastic Depth)\n",
    "\n",
    "* Derin Transformer’larda bazı blokları rastgele bypass etmek için kullanılır.\n",
    "\n",
    "* Training stabilitesini ve genelleme performansını artırır.\n",
    "\n",
    "Kod: DropPath sınıfı."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76860861",
   "metadata": {},
   "source": [
    "### DropPath (Stochastic Depth) Nedir?\n",
    "\n",
    "* DropPath, derin Transformer’larda bazı blokların çıkışını rastgele sıfırlayıp bypass etmek için kullanılır.\n",
    "\n",
    "* Bu, çok derin ağlarda training stabilitesini artırır ve overfitting’i azaltır.\n",
    "\n",
    "* Farkı Dropout’tan: Dropout bir tensor içindeki elemanları rastgele sıfırlar; DropPath bir bloğu tamamen atlatır (residual yol üzerinden)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bd0c7",
   "metadata": {},
   "source": [
    "#### Nereye entegre edeceğiz?\n",
    "\n",
    "Bir Transformer Encoder bloğu tipik olarak şöyle görünür:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5d643",
   "metadata": {},
   "source": [
    "```python \n",
    "x = x + Attention(LayerNorm(x))\n",
    "x = x + FFN(LayerNorm(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca218a2",
   "metadata": {},
   "source": [
    "* Burada Residual + PreNorm mevcut.\n",
    "\n",
    "#### DropPath’i entegre etmek için:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d3758",
   "metadata": {},
   "source": [
    "```python \n",
    "x = x + drop_path(Attention(LayerNorm(x)))\n",
    "x = x + drop_path(FFN(LayerNorm(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a1d22",
   "metadata": {},
   "source": [
    "## Kod Örneği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f65374bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670dd1f6",
   "metadata": {},
   "source": [
    "## Encoder Blok Örneği ile Kullanımı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebe50773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # PreNorm + DropPath\n",
    "        x = x + self.drop_path(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask))\n",
    "        x = x + self.drop_path(self.ffn(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb79cf5",
   "metadata": {},
   "source": [
    "### ✅ Özet:\n",
    "\n",
    "* DropPath’i her residual connection’a ekliyoruz.\n",
    "\n",
    "* drop_prob ile ne kadar blok rastgele bypass edileceğini kontrol ediyoruz.\n",
    "\n",
    "* Training sırasında aktif, eval modunda devre dışı."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91ad4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da655b0",
   "metadata": {},
   "source": [
    "# 2️⃣ LayerScale Nedir?\n",
    "\n",
    "* Transformer bloklarının çıkışını küçük bir katsayı ile çarpar (alpha * block_out).\n",
    "\n",
    "* Derin modellerde, özellikle LLM’lerde, training stabilitesini artırır.\n",
    "\n",
    "* Küçük bir alpha (örn. 1e-4 ~ 1e-2) ile başlatılır ve öğrenilebilir parametredir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377eb221",
   "metadata": {},
   "source": [
    "### Nereye entegre ediyoruz?\n",
    "\n",
    "* Önceki DropPath’li blokta her residual connection üzerine uygulayabiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d62b3",
   "metadata": {},
   "source": [
    "```python \n",
    "# x = x + drop_path(block_out)\n",
    "# → LayerScale ile\n",
    "x = x + drop_path(alpha * block_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9985cf5",
   "metadata": {},
   "source": [
    "* alpha tensor olarak embed_dim boyutunda ve learnable olabilir.\n",
    "\n",
    "* Genellikle her blok için ayrı alpha kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68ae6b",
   "metadata": {},
   "source": [
    "### **Kod Örneği**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b75d1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        # LayerScale\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention + LayerScale + DropPath\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # FFN + LayerScale + DropPath\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51de558",
   "metadata": {},
   "source": [
    "## ✅ Özet:\n",
    "\n",
    "* gamma_1 ve gamma_2 → learnable scale parametreleri\n",
    "\n",
    "* DropPath + LayerScale → modern LLM bloklarının temel stabilite mekanizması\n",
    "\n",
    "* Training sırasında aktif, eval modunda normal residual gibi çalışıyor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0b4a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c751c3",
   "metadata": {},
   "source": [
    "# 3️⃣ FFN: GeLU → SwiGLU (Modern LLM)\n",
    "### Neden SwiGLU?\n",
    "\n",
    "* GeLU: klasik Transformer FFN aktivasyonu.\n",
    "\n",
    "* SwiGLU (Gated Linear Unit):\n",
    "\n",
    "> Daha hızlı öğrenme sağlar\n",
    "\n",
    "> Daha iyi genelleme (özellikle LLM’lerde)\n",
    "\n",
    "> Modern LLM’lerde standart (GPT-NeoX, LLaMA, Falcon vb.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86438a99",
   "metadata": {},
   "source": [
    "### **Kod Örneği: SwiGLU FFN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "331f58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = embed_dim * expansion\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.act = nn.GELU()  # GeLU aktivasyonu\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w3(self.act(self.w2(x)) * self.w1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74231ffd",
   "metadata": {},
   "source": [
    "### **Encoder Blokta Kullanımı**\n",
    "\n",
    "LayerScale ve DropPath ile kombine edersek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6bd2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = SwiGLUFFN(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "        # LayerScale\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention + LayerScale + DropPath\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # SwiGLU FFN + LayerScale + DropPath\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd950ffa",
   "metadata": {},
   "source": [
    "## ✅ Özet:\n",
    "\n",
    "* FFN artık SwiGLU tabanlı → LLM standardı\n",
    "\n",
    "* LayerScale + DropPath ile residual bağlantılar stabilize edilmiş\n",
    "\n",
    "* Self-Attention ve FFN için modern LLM tasarımı tamamlanmış oldu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e785f",
   "metadata": {},
   "source": [
    "----\n",
    "# 4️⃣ Rotary Positional Encoding (RoPE)\n",
    "### Neden RoPE?\n",
    "\n",
    "* Standart sin/cos positional encoding yalnızca ekleme ile çalışır.\n",
    "\n",
    "* RoPE, attention mekanizmasına rotasyon matrisi ile pozisyon bilgisi ekler.\n",
    "\n",
    "* Uzun dizilerde genelleme performansını artırır.\n",
    "\n",
    "* Modern LLM’lerin çoğu (LLaMA, Falcon, MPT) kullanıyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632844d",
   "metadata": {},
   "source": [
    "### **Kod Örneği: Rotary Embedding Uygulaması**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5c5b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, num_heads, T, head_dim]\n",
    "        B, H, T, D = x.shape\n",
    "        t = torch.arange(T, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # [T, D/2]\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)  # [T, D]\n",
    "        cos = emb.cos()[None, None, :, :]  # [1,1,T,D]\n",
    "        sin = emb.sin()[None, None, :, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x_rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d4029",
   "metadata": {},
   "source": [
    "### **Attention Blokta Kullanımı**\n",
    "\n",
    "* MultiHeadAttention içinde query ve key’lere uygulayabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08056529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        if use_rope:\n",
    "            self.rope = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        if self.use_rope:\n",
    "            Q = self.rope(Q)\n",
    "            K = self.rope(K)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70770562",
   "metadata": {},
   "source": [
    "## ✅ Özet:\n",
    "\n",
    "* RotaryEmbedding query ve key’e uygulanır → pozisyon bilgisi attention matrisine doğal olarak eklenir\n",
    "\n",
    "* Uzun dizilerde standart sin/cos’dan daha iyi genelleme\n",
    "\n",
    "* Modern LLM’lerde self-attention’ı güçlendiren bir adım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1267798",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7738e16",
   "metadata": {},
   "source": [
    "# Yukarıda bulunan eklentileri encoder yapımıza entegre edelim:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe677bbc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cf044e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526a8f5",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Positional Encoding (sin/cos)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ddcf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4dc30",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dff1bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4d381",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9018b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a334e",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Rotary Positional Encoding (RoPE)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8726ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, T, D = x.shape\n",
    "        t = torch.arange(T, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos = emb.cos()[None, None, :, :]\n",
    "        sin = emb.sin()[None, None, :, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b317250",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89595b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        if use_rope:\n",
    "            self.rope = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        if self.use_rope:\n",
    "            Q = self.rope(Q)\n",
    "            K = self.rope(K)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f07a43",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# SwiGLU FFN\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b6b190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = embed_dim * expansion\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w3(self.act(self.w2(x)) * self.w1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b7eb3",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Transformer Encoder Block\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f74860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp, use_rope=use_rope)\n",
    "        self.ffn = SwiGLUFFN(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c75937",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# Full Transformer Encoder\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a52796e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=12, num_heads=8, dp=0.1, drop_path=0.1, max_len=5000, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, dp, drop_path, use_rope=use_rope)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54e43d",
   "metadata": {},
   "source": [
    "# TAM KOD :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6527aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, T, D = x.shape\n",
    "        t = torch.arange(T, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        cos = emb.cos()[None, None, :, :]\n",
    "        sin = emb.sin()[None, None, :, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, use_rope=True):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.use_rope = use_rope\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "        if use_rope:\n",
    "            self.rope = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        if self.use_rope:\n",
    "            Q = self.rope(Q)\n",
    "            K = self.rope(K)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = embed_dim * expansion\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w3(self.act(self.w2(x)) * self.w1(x)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.1, init_values=1e-4, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp, use_rope=use_rope)\n",
    "        self.ffn = SwiGLUFFN(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones(embed_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=12, num_heads=8, dp=0.1, drop_path=0.1, max_len=5000, use_rope=True):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, dp, drop_path, use_rope=use_rope)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1ea76",
   "metadata": {},
   "source": [
    "---\n",
    "# Encoder için LLM İyileştirmeleri\n",
    "\n",
    "## 1️⃣ Embed Dim ve Layer Sayısı\n",
    "- Mevcut: `embed_dim=512`, `num_layers=6`\n",
    "- Öneri: `embed_dim=1024`, `num_layers=12`\n",
    "\n",
    "## 2️⃣ Attention Heads\n",
    "- Mevcut: 8 head\n",
    "- Öneri: 16 head  \n",
    "  _(embed_dim / num_heads = head_dim)_\n",
    "\n",
    "## 3️⃣ FeedForward\n",
    "- Expansion: 4 → 4-8\n",
    "- Aktivasyon: GELU veya SwiGLU\n",
    "\n",
    "## 4️⃣ Dropout + DropPath + LayerScale\n",
    "- Amaç: Training stabilitesi ve genelleme performansını artırmak\n",
    "- Eklenmesi gerekenler: \n",
    "  - Dropout\n",
    "  - DropPath (Stochastic Depth)\n",
    "  - LayerScale parametreleri\n",
    "\n",
    "## 5️⃣ Positional Encoding\n",
    "- Rotary Embedding veya scalable sin/cos\n",
    "- Şimdilik mevcut sin/cos’u geliştirmek yeterli\n",
    "\n",
    "## 6️⃣ Norm & Stabilite\n",
    "- PreNorm + LayerNorm + residual connection\n",
    "- Her blokta attention ve feedforward için uygulanacak\n",
    "\n",
    "## 7️⃣ Optimizer ve FP16 Uyumluluğu\n",
    "- Training kısmı için: \n",
    "  - FP16 ile uyumlu\n",
    "  - AdamW veya Lion optimizer tercih edilebilir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db210c6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1️⃣ Token Embedding ve Embed Dim Yükseltme\n",
    "\n",
    "* Amaç: LLM seviyesinde daha geniş bir temsil gücü sağlamak için embed_dim’i 1024 yapıyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e936b58",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74873c50",
   "metadata": {},
   "source": [
    "### Notlar:\n",
    "\n",
    "* embed_dim=1024 → head sayısı ve feedforward expansion ile uyumlu olmalı.\n",
    "\n",
    "* Bu embedding her token’ı dense vektöre çevirir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8c275",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204b32e",
   "metadata": {},
   "source": [
    "## 2️⃣ Positional Encoding (Geliştirilmiş Sin/Cos)\n",
    "\n",
    "* Amaç: Uzun dizilerde modelin konum bilgisini daha iyi yakalaması. Mevcut sin/cos’u kullanacağız ama embed_dim=1024 ile uyumlu hale getireceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6038ad0",
   "metadata": {},
   "source": [
    "```python \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e0d36",
   "metadata": {},
   "source": [
    "### Notlar:\n",
    "\n",
    "* max_len uzun diziler için büyütüldü (örneğin 10k token).\n",
    "\n",
    "* embed_dim=1024 ile uyumlu.\n",
    "\n",
    "* İleride rotary embeddings ile değiştirilebilir ama şimdilik scalable sin/cos yeterli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a3725",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e90f2",
   "metadata": {},
   "source": [
    "## 3️⃣ Multi-Head Attention (16 Head, LLM)\n",
    "\n",
    "* Amaç: Daha büyük embedding ve daha fazla baş (head) ile modelin bağlamı yakalama kapasitesini artırmak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96285d09",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db817350",
   "metadata": {},
   "source": [
    "### Özellikler:\n",
    "\n",
    "* embed_dim=1024 → daha geniş embedding\n",
    "\n",
    "* num_heads=16 → daha fazla dikkat başı\n",
    "\n",
    "* self.scale = head_dim ** -0.5 ile stabilizasyon\n",
    "\n",
    "* Dropout uygulanıyor, training sırasında regularizasyon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de9d40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bc1f1",
   "metadata": {},
   "source": [
    "## 4️⃣ FeedForward (LLM)\n",
    "\n",
    "* Amaç: Daha güçlü ve stabil feedforward katmanı, genişletilmiş boyut ve yeni aktivasyon (GELU veya SwiGLU) ile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5293378",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        inner_dim = embed_dim * expansion\n",
    "        self.use_swiglu = use_swiglu\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        if use_swiglu:\n",
    "            # SwiGLU: GELU gating mekanizması\n",
    "            self.fc1 = nn.Linear(embed_dim, inner_dim * 2)  # ikiye katla\n",
    "            self.fc2 = nn.Linear(inner_dim, embed_dim)\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, inner_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(inner_dim, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        # LayerScale parametresi\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_swiglu:\n",
    "            x1, x2 = self.fc1(x).chunk(2, dim=-1)\n",
    "            x = F.gelu(x1) * x2\n",
    "            x = self.fc2(x)\n",
    "            x = self.gamma * x\n",
    "            return self.dropout(x)\n",
    "        else:\n",
    "            return self.gamma * self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72b26d",
   "metadata": {},
   "source": [
    "### Özellikler:\n",
    "\n",
    "* expansion=8 → daha büyük hidden dimension\n",
    "\n",
    "* use_swiglu=True → SwiGLU aktivasyonu (GELU gating)\n",
    "\n",
    "* Dropout ile regularizasyon\n",
    "\n",
    "* LayerScale ile training stabilitesi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362fde4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141f368",
   "metadata": {},
   "source": [
    "## 5️⃣ Transformer Encoder Block (LLM)\n",
    "\n",
    "Amaç:\n",
    "\n",
    "* PreNorm + Residual Connection\n",
    "\n",
    "* MultiHeadAttention (16 head, embed_dim=1024)\n",
    "\n",
    "* Gelişmiş FeedForward (expansion=8, SwiGLU opsiyonel)\n",
    "\n",
    "* Dropout + DropPath + LayerScale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211ca47",
   "metadata": {},
   "source": [
    "```python \n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic Depth / DropPath\"\"\"\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "\n",
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads=num_heads, dp=dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion=expansion, dp=dp, use_swiglu=use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        # LayerScale parametreleri\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # PreNorm + Self-Attention + Residual + DropPath\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # FeedForward + Residual + DropPath\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb56f6",
   "metadata": {},
   "source": [
    "## Özellikler:\n",
    "\n",
    "* PreNorm: LayerNorm önce uygulanır → training stabilitesi\n",
    "\n",
    "* DropPath: Stochastic depth ile bloklar rastgele bypass edilebilir\n",
    "\n",
    "* LayerScale (gamma_1, gamma_2) → derin modellerde residual scaling\n",
    "\n",
    "* FeedForward: expansion=8 ve opsiyonel SwiGLU\n",
    "\n",
    "* MultiHeadAttention: embed_dim / num_heads = head_dim, örneğin 1024/16 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3124c1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df8f81",
   "metadata": {},
   "source": [
    "## 6️⃣ Full Transformer Encoder (LLM)\n",
    "\n",
    "Özellikler:\n",
    "\n",
    "* TokenEmbedding + PositionalEncoding\n",
    "\n",
    "* 12 adet LLM Encoder Block (TransformerEncoderBlockLLM)\n",
    "\n",
    "* LayerNorm sonunda\n",
    "\n",
    "* DropPath ve LayerScale aktif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7de816",
   "metadata": {},
   "source": [
    "```python \n",
    "class TransformerEncoderLLM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim=1024, \n",
    "        num_layers=12, \n",
    "        num_heads=16, \n",
    "        dp=0.1, \n",
    "        drop_path=0.1, \n",
    "        expansion=8, \n",
    "        max_len=5000,\n",
    "        use_swiglu=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlockLLM(\n",
    "                embed_dim=embed_dim, \n",
    "                num_heads=num_heads, \n",
    "                dp=dp, \n",
    "                drop_path=drop_path, \n",
    "                expansion=expansion,\n",
    "                use_swiglu=use_swiglu\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: [B, T] (long)\n",
    "        src_mask: mask shape broadcastable to [B, num_heads, T, T] OR [T, T] or [B, T, T]\n",
    "                  1 -> allowed, 0 -> masked\n",
    "        returns: encoder_outputs [B, T, C]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(src_tokens)       # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e58f8",
   "metadata": {},
   "source": [
    "## Özellikler:\n",
    "\n",
    "* 12 layer + 16 head + embed_dim=1024 → LLM ölçeğinde\n",
    "\n",
    "* PreNorm + DropPath + LayerScale + Residual\n",
    "\n",
    "* FeedForward genişletilmiş (expansion=8, opsiyonel SwiGLU)\n",
    "\n",
    "* PositionalEncoding: sin/cos, scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd3a4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601e663",
   "metadata": {},
   "source": [
    "# 🔹 Full LLM Transformer Encoder\n",
    "\n",
    "Özellikler:\n",
    "\n",
    "* Embed dim 1024, 12 katman (layers), 16 head\n",
    "\n",
    "* PreNorm + LayerNorm + Residual + DropPath + LayerScale\n",
    "\n",
    "* FeedForward genişletilmiş (expansion=8, opsiyonel SwiGLU)\n",
    "\n",
    "* Positional Encoding: sin/cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Encoder Block (LLM)\n",
    "# -----------------------------\n",
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Encoder (LLM)\n",
    "# -----------------------------\n",
    "class TransformerEncoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13a3d0",
   "metadata": {},
   "source": [
    "# Transformer Encoder Block (LLM) Akış Diyagramı (Markdown)\n",
    "```bash\n",
    "\n",
    "Input Tokens (`src_tokens`)\n",
    "│\n",
    "├─> **Token Embedding**\n",
    "│    - nn.Embedding(vocab_size, embed_dim)\n",
    "│\n",
    "├─> **Positional Encoding**\n",
    "│    - sin/cos pozisyon vektörleri eklenir\n",
    "│\n",
    "└─> **Encoder Layers** (N adet, ör: 12)\n",
    "     │\n",
    "     ├─ **LayerNorm 1**\n",
    "     │    │\n",
    "     │    └─> **Self-Attention**\n",
    "     │          - MultiHeadAttention(embed_dim, num_heads)\n",
    "     │          - Mask optional (src_mask)\n",
    "     │          - Dropout\n",
    "     │    │\n",
    "     │    └─> **DropPath + γ₁ scaling**\n",
    "     │    │\n",
    "     │    └─> **Residual Add** (x + scaled attn)\n",
    "     │\n",
    "     ├─ **LayerNorm 2**\n",
    "     │    │\n",
    "     │    └─> **FeedForward Network (FFN)**\n",
    "     │          - Linear → GELU/SwiGLU → Linear\n",
    "     │          - Dropout\n",
    "     │    │\n",
    "     │    └─> **DropPath + γ₂ scaling**\n",
    "     │    │\n",
    "     │    └─> **Residual Add**\n",
    "     │\n",
    "     └─> **Output of Layer** (`x` updated)\n",
    "│\n",
    "├─> **Final LayerNorm**\n",
    "│\n",
    "└─> **Encoder Output**\n",
    "     - `x` (embedding boyutu: batch_size × seq_len × embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5ce37",
   "metadata": {},
   "source": [
    "## Açıklama:\n",
    "\n",
    "* Token Embedding + Positional Encoding: Token ID’lerini vektöre çevirir ve pozisyon bilgisi ekler.\n",
    "\n",
    "* Self-Attention: Sadece encoder’daki tokenler arasında dikkat (mask opsiyonel).\n",
    "\n",
    "* FeedForward (FFN): Lineer + GELU/SwiGLU + Dropout.\n",
    "\n",
    "* Residual + DropPath: Stochastic depth ve learnable scaling.\n",
    "\n",
    "* Final LayerNorm: Katman çıkışını normalize eder.\n",
    "\n",
    "* Encoder Output: Decoder’a girdi olarak veya başka downstream görevlerde kullanılabilir."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
