{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565f63d8",
   "metadata": {},
   "source": [
    "# 🧭 LLM’e Giden Yol Haritası (Katman Katman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca1b1f",
   "metadata": {},
   "source": [
    "| Aşama   | Katman / Kavram                 | Ne Öğreneceğiz                                      | Kodla mı?         |\n",
    "| ------- | ------------------------------- | --------------------------------------------------- | ----------------- |\n",
    "| **1️⃣** | **Tokenization**                | Metni sayılara dönüştürmek (subword, BPE)           | ✅                 |\n",
    "| **2️⃣** | **Embedding Layer**             | Token ID → Vektör                                   | ✅                 |\n",
    "| **3️⃣** | **Positional Encoding**         | Sıra bilgisini modele eklemek                       | ✅                 |\n",
    "| **4️⃣** | **Self-Attention (Tek başına)** | Dikkat mekanizmasının özü (Q, K, V)                 | ✅                 |\n",
    "| **5️⃣** | **Multi-Head Attention**        | Paralel dikkat başlarıyla temsil gücü               | ✅                 |\n",
    "| **6️⃣** | **Feed-Forward Network (MLP)**  | Her token’ın bilgisine bağımsız dönüşüm             | ✅                 |\n",
    "| **7️⃣** | **Residual + LayerNorm**        | Stabilite ve gradient akışı                         | ✅                 |\n",
    "| **8️⃣** | **Transformer Block**           | Yukarıdakilerin birleşimi                           | ✅                 |\n",
    "| **9️⃣** | **LLM Base Model (GPT-style)**  | Embedding + N adet Transformer Block + Output Head  | ✅                 |\n",
    "| **🔟**  | **Training Loop & Masking**     | Causal mask, loss hesaplama, autoregressive öğrenme | ✅                 |\n",
    "| **⚡**   | **Inference (Text Generation)** | Greedy, sampling, top-k/p decoding                  | ✅                 |\n",
    "| **🚀**  | **Gelişmişler**                 | KV Cache, RoPE, FlashAttention, LoRA                | ⚙️ (isteğe bağlı) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023fa48",
   "metadata": {},
   "source": [
    "# 🧱 LLM Modelini Katman Katman İnşa Planı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5fc52",
   "metadata": {},
   "source": [
    "| Aşama | Katman                         | Amaç                                         |\n",
    "| ----- | ------------------------------ | -------------------------------------------- |\n",
    "| 1️⃣   | **Embedding Layer**            | Token id’lerini sürekli vektör uzayına taşır |\n",
    "| 2️⃣   | **Positional Encoding**        | Sıralama bilgisini modele katar              |\n",
    "| 3️⃣   | **Self-Attention (QKV)**       | Token’ların birbirine dikkat etmesini sağlar |\n",
    "| 4️⃣   | **Multi-Head Attention**       | Bilgiyi farklı alt uzaylarda işler           |\n",
    "| 5️⃣   | **Feed Forward Network (MLP)** | Her token bilgisini ayrı dönüştürür          |\n",
    "| 6️⃣   | **Residual + LayerNorm**       | Öğrenmeyi stabilize eder                     |\n",
    "| 7️⃣   | **Transformer Block**          | Yukarıdakilerin birleşimi                    |\n",
    "| 8️⃣   | **LLM (GPT-style)**            | Embedding + N blok + çıkış projeksiyonu      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54366a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c83ef",
   "metadata": {},
   "source": [
    "# 🔹 Başlangıç: 1️⃣ Embedding Katmanı\n",
    "🎯 Amaç:\n",
    "\n",
    "* Token ID → vektör uzayına dönüşüm.\n",
    "* Her kelime için öğrenilebilir bir temsil vektörü."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bff1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        return self.embedding(x)  # [batch_size, seq_len, embed_dim]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a14d0",
   "metadata": {},
   "source": [
    "# 🔹 2️⃣ Positional Encoding (Sin/Cos)\n",
    "\n",
    "* Transformers sırayı doğal olarak bilmez, bu yüzden pozisyon bilgisi eklememiz gerekir.\n",
    "\n",
    "🧠 Teori\n",
    "\n",
    "* Pozisyon bilgisi sinüs ve kosinüs fonksiyonlarıyla kodlanır:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef197bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed56481",
   "metadata": {},
   "source": [
    "# 3️⃣ Self-Attention (Tek Başlı)\n",
    "🧠 Teori\n",
    "\n",
    "* Her token, diğer token’lara dikkat (attention) uygular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29f8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = embed_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        return attn @ V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d315c91",
   "metadata": {},
   "source": [
    "# 🔹 4️⃣ Multi-Head Attention\n",
    "\n",
    "* Birden fazla attention başı paralel çalışır.\n",
    "* Her biri farklı bir alt uzayda dikkat öğrenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558d3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)  # her biri [B, T, num_heads, head_dim]\n",
    "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b61b1b",
   "metadata": {},
   "source": [
    "# 🧩 5️⃣ Feed Forward (MLP) Katmanı\n",
    "🎯 Amaç\n",
    "\n",
    "* Her token’ın temsilini bağımsız olarak dönüştürmek.\n",
    "* Self-Attention, token’lar arası etkileşimi yakalarken;\n",
    "* FeedForward, her token’ın kendi iç temsiline dönüşüm uygular.\n",
    "> Bu yapı sayesinde model non-linearity kazanır ve her token kendi bilgisi üzerinde çalışabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf21dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim: int, expansion: int = 4, dp: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expansion * embed_dim, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ab29d",
   "metadata": {},
   "source": [
    "# ⚖️ 6️⃣ Layer Normalization + Residual Connection\n",
    "🎯 Amaç\n",
    "\n",
    "* Derin ağlarda gradyan akışını stabilize etmek ve öğrenmeyi dengede tutmak.\n",
    "\n",
    "* Residual Connection: Giriş + Çıkış → \n",
    "\n",
    "* LayerNorm: Katman bazlı normalizasyon (her örnekte tüm embedding boyutu için)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb89e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x + self.layer(self.norm(x, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699e1ca",
   "metadata": {},
   "source": [
    "# 🧱 7️⃣ Transformer Block (Tam Hal)\n",
    "\n",
    "Artık elimizde:\n",
    "\n",
    "* MultiHeadAttention\n",
    "\n",
    "* FeedForward\n",
    "\n",
    "* Residual + LayerNorm\n",
    "\n",
    "Bunları birleştirip tek bir Transformer bloğu yapıyoruz.\n",
    "\n",
    "* Bu bloğu N kez üst üste koyduğumuzda → LLM oluşur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495b4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dp: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1️⃣ Attention + Residual\n",
    "        attn_out = self.attn(self.norm1(x), mask=mask)\n",
    "        x = x + self.dp(attn_out)\n",
    "\n",
    "        # 2️⃣ FeedForward + Residual\n",
    "        ff_out = self.ff(self.norm2(x))\n",
    "        x = x + self.dp(ff_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67425b",
   "metadata": {},
   "source": [
    "---\n",
    "## Geçen REPO'da yaptıklarımıza bakalım.O modeli aşağıya bırakıyorum.Bu model üzerinden geliştirmelere bakayacağız.Eğer o repoyu incelemediyseniz aşağıya bağlantıyı bırakıyorum;\n",
    "\n",
    "> ### \"https://github.com/huseyin-dgn/PyTorch-Bolum_5---SeqtoSeq_Transformers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9c637",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d047b31",
   "metadata": {},
   "source": [
    "### Bahsettiğim model kodunu aşağıya tekrar yazalım ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05c1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=None, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads or max(1, embed_dim // 64)\n",
    "        self.head_dim = embed_dim // self.num_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, S, E = x.size()\n",
    "        return x.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        B, S, _, _ = x.shape\n",
    "        return x.view(B, S, self.embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                scores += attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                scores += attn_mask.unsqueeze(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask, torch.finfo(scores.dtype).min)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "class EnchancedLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 256, hidden_size: int = 512,\n",
    "                 num_layers: int = 2, bidirectional: bool = True, dp: float = 0.1,\n",
    "                 use_attn: bool = True, pre_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        self.use_attn = use_attn\n",
    "        self.pre_norm = pre_norm\n",
    "        self.dropout = dp\n",
    "\n",
    "        lstm_input_dim = hidden_size * self.directions\n",
    "        self.residual_proj = nn.Linear(embedding_dim, lstm_input_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dp if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.layer_norm_in = nn.LayerNorm(lstm_input_dim)\n",
    "        self.layer_norm_out = nn.LayerNorm(lstm_input_dim)\n",
    "        self.attention = MultiHeadAttention(lstm_input_dim) if use_attn else None\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(lstm_input_dim, lstm_input_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(lstm_input_dim * 4, lstm_input_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None):\n",
    "\n",
    "        embedded = self.embedding(src)\n",
    "        residual = self.residual_proj(embedded)\n",
    "        lstm_input = self.layer_norm_in(residual) if self.pre_norm else residual\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(lstm_input)\n",
    "        outputs = self.layer_norm_out(outputs)\n",
    "\n",
    "        if self.attention:\n",
    "            attn_out = self.attention(outputs, outputs, outputs,\n",
    "                                      key_padding_mask=src_key_padding_mask,\n",
    "                                      attn_mask=attn_mask)\n",
    "            outputs = outputs + attn_out\n",
    "\n",
    "        outputs = outputs + self.ffn(outputs)\n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim : int , num_heads:Optional[int] =None , dp:float = 0.3):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads or max(1, embed_dim//64)\n",
    "        self.head_dim = embed_dim // self.num_heads\n",
    "        assert embed_dim % self.num_heads == 0 , \"embed_idm ve num_heads bölünemiyor\"\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim , embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim , embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self,x):\n",
    "        B,S,E = x.size()\n",
    "        return x.view(B,S,self.num_heads , self.head_dim).transpose(1,2)\n",
    "\n",
    "    def combine_heads(self,x):\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "        B,S,_,_ = x.size()\n",
    "        return x.view(B,S,self.embed_dim)\n",
    "    \n",
    "    def forward(self,query,key,value,key_padding_mask = None , attn_mask = None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "\n",
    "        scores = torch.matmul(Q,K.transpose(-2,-1)) / self.scale\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                scores = scores + attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                scores = scores + attn_mask.unsqueeze(1)\n",
    "\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask, torch.finfo(scores.dtype).min)\n",
    "\n",
    "\n",
    "        attn = F.softmax(scores , dim = -1 )\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn , V)\n",
    "        out = self.combine_heads(out)\n",
    "\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "def init_decoder_states_from_encoder(hidden, cell, num_decoder_layers, decoder_hidden_size,\n",
    "                                     encoder_directions=1, proj_layer=None):\n",
    "    num_enc_total, B, enc_hidden = hidden.size()\n",
    "    num_enc_layers = num_enc_total // encoder_directions\n",
    "\n",
    "    h = hidden.view(num_enc_layers, encoder_directions, B, enc_hidden)\n",
    "    c = cell.view(num_enc_layers, encoder_directions, B, enc_hidden)\n",
    "\n",
    "    # Bidirectional merge\n",
    "    h_merged = h.mean(dim=1)  # (num_layers, B, enc_hidden)\n",
    "    c_merged = c.mean(dim=1)\n",
    "\n",
    "    if proj_layer is None and enc_hidden != decoder_hidden_size:\n",
    "        proj_layer = nn.Linear(enc_hidden, decoder_hidden_size).to(hidden.device)\n",
    "\n",
    "    if proj_layer is not None:\n",
    "        L = num_enc_layers\n",
    "        # ❌ Düzeltme: directions ile çarpma yok\n",
    "        h_proj = proj_layer(h_merged.view(L * B, enc_hidden)).view(L, B, decoder_hidden_size)\n",
    "        c_proj = proj_layer(c_merged.view(L * B, enc_hidden)).view(L, B, decoder_hidden_size)\n",
    "    else:\n",
    "        h_proj, c_proj = h_merged, c_merged\n",
    "\n",
    "    if num_decoder_layers == num_enc_layers:\n",
    "        return h_proj, c_proj\n",
    "    elif num_decoder_layers > num_enc_layers:\n",
    "        h_final = torch.cat([h_proj[min(i, num_enc_layers - 1):min(i, num_enc_layers - 1) + 1]\n",
    "                             for i in range(num_decoder_layers)], dim=0)\n",
    "        c_final = torch.cat([c_proj[min(i, num_enc_layers - 1):min(i, num_enc_layers - 1) + 1]\n",
    "                             for i in range(num_decoder_layers)], dim=0)\n",
    "        return h_final, c_final\n",
    "    \n",
    "class GatedFFN(nn.Module):\n",
    "    def __init__(self, hidden_size:int , dp:float = 0.2):\n",
    "        super().__init__()\n",
    "        self.fc1= nn.Linear(hidden_size , hidden_size * 4)\n",
    "        self.fc2 = nn.Linear(hidden_size * 4 ,hidden_size)\n",
    "        self.gate = nn.Linear(hidden_size , hidden_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.fc2(self.act(self.fc1(x)))\n",
    "        gate = self.sigmoid(self.gate(x))\n",
    "        return self.dropout(out + gate)\n",
    "    \n",
    "class PositionelEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size : int , max_len : int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len , hidden_size)\n",
    "        position = torch.arange(0 , max_len , dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0 , hidden_size,2).float() *  (-math.log(100000.0) / hidden_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe' , pe)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[: , :seq_len , :]\n",
    "    \n",
    "class AdaptiveDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim = 256,hidden_size =512, num_layers=3,dp=0.3 , use_attn =True , pre_norm = True , max_len = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_norm = pre_norm\n",
    "        self.use_attn = use_attn\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.input_proj = nn.Linear(embedding_dim,hidden_size)\n",
    "        self.pos_encoding = PositionelEncoding(hidden_size , max_len=max_len)\n",
    "\n",
    "        self.lstm = nn.LSTM(hidden_size , hidden_size , num_layers=num_layers , dropout=dp  if num_layers>1 else 0.0 , batch_first=True)\n",
    "\n",
    "        self.layer_norm_residual = nn.LayerNorm(hidden_size)\n",
    "        self.attention = MultiHeadAttention(hidden_size) if use_attn else None\n",
    "        self.gated_ffn = GatedFFN(hidden_size , dp)\n",
    "        self.layer_norm_ffn = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden ,cell , src_key_padding = None , encoder_output = None , attn_mask = None):\n",
    "        embedded = self.input_proj(self.embedding(x))\n",
    "        embedded = self.pos_encoding(embedded)\n",
    "        lstm_input = self.layer_norm_residual(embedded) if self.pre_norm else embedded\n",
    "\n",
    "        outputs , (hidden , cell) = self.lstm(lstm_input , (hidden , cell))\n",
    "\n",
    "        if self.attention and encoder_output is not None:\n",
    "            attn_out = self.attention(outputs , encoder_output , encoder_output,key_padding_mask = src_key_padding , attn_mask = attn_mask)\n",
    "            outputs = outputs + attn_out\n",
    "        \n",
    "        outputs = outputs + self.gated_ffn(outputs)\n",
    "        outputs = self.layer_norm_ffn(outputs)\n",
    "\n",
    "        return outputs , hidden , cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a9a27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca6b12",
   "metadata": {},
   "source": [
    "## Şimdi bu modeli LLM'e entegre etmeye çalışalım  ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0c8de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ed118",
   "metadata": {},
   "source": [
    "# 🔹 SEQ2SEQ Modeli İncelemesi ve LLM Uyarlama Analizi\n",
    "\n",
    "Bu hücre, gönderilen SEQ2SEQ kodunu temel alarak **LLM tarzı bir Transformer’a dönüşüm adımlarını** özetler.\n",
    "\n",
    "\n",
    "\n",
    "## 1️⃣ Encoder (EnchancedLSTMEncoder)\n",
    "\n",
    "- **Embedding Layer:** `self.embedding` → token ID’lerini vektöre çevirir.\n",
    "- **Residual Projection:** `self.residual_proj` → embedding boyutunu LSTM giriş boyutuna eşler.\n",
    "- **LayerNorm:** `self.layer_norm_in` / `self.layer_norm_out` → pre/post norm işlemleri.\n",
    "- **LSTM:** Çok katmanlı, bidirectional LSTM.\n",
    "- **Attention:** Opsiyonel `MultiHeadAttention`.\n",
    "- **FeedForward:** `self.ffn` → Gated / GELU aktivasyonlu MLP.\n",
    "\n",
    "**💡 LLM uyarlaması:**\n",
    "- LSTM yerine **Transformer Encoder Block** kullanılacak.\n",
    "- Residual + LayerNorm + Attention + FFN yapısı zaten hazır; doğrudan Transformer blok mantığına uyarlanabilir.\n",
    "\n",
    "\n",
    "\n",
    "## 2️⃣ Decoder (AdaptiveDecoder)\n",
    "\n",
    "- **Embedding + Input Projection:** `self.embedding` + `self.input_proj`.\n",
    "- **Positional Encoding:** `self.pos_encoding`.\n",
    "- **LSTM:** Çok katmanlı decoder LSTM.\n",
    "- **Attention:** Encoder-Decoder attention (`MultiHeadAttention`).\n",
    "- **Gated FFN:** `self.gated_ffn`.\n",
    "- **LayerNorm:** `self.layer_norm_ffn`, `self.layer_norm_residual`.\n",
    "\n",
    "**💡 LLM uyarlaması:**\n",
    "- Decoder zaten **Transformer decoder mantığına yakın**:\n",
    "  - LSTM → Masked Self-Attention + FeedForward\n",
    "  - Encoder-Decoder Attention → Cross-Attention\n",
    "  - GatedFFN → klasik FFN yerine kullanılabilir\n",
    "- Positional Encoding ve Residual + LayerNorm yapısı korunabilir.\n",
    "\n",
    "\n",
    "\n",
    "## 3️⃣ MultiHeadAttention\n",
    "\n",
    "- Mevcut hem encoder hem decoder için ortak `MultiHeadAttention`.\n",
    "- Split/Combine heads, QKV projeksiyonu, mask handling mevcut.\n",
    "- Ufak değişikliklerle **Transformer standard attention** (pre-norm + causal mask) yapılabilir.\n",
    "\n",
    "\n",
    "\n",
    "## 4️⃣ LLM Mimarisine Geçiş Notları\n",
    "\n",
    "| Mevcut | LLM Uyarlaması |\n",
    "|---------|----------------|\n",
    "| LSTM tabanlı encoder | Transformer encoder block |\n",
    "| LSTM tabanlı decoder | Transformer decoder block (masked self-attention) |\n",
    "| Gated FFN / standard FFN | Transformer feedforward (ReLU/GELU) |\n",
    "| Encoder-decoder attention | Cross-attention |\n",
    "| Positional Encoding | Direkt kullanılabilir |\n",
    "| LayerNorm + residual | Pre-norm yapısı olarak aynen kullanılabilir |\n",
    "\n",
    "\n",
    "\n",
    "## ✅ Özet\n",
    "\n",
    "- Kod zaten **modüler ve katmanlı**, LLM için gereken çoğu yapı mevcut.\n",
    "- Yapılması gerekenler:\n",
    "  1. **LSTM’leri kaldırıp Transformer blokları ile değiştirmek**\n",
    "  2. **Decoder’da masked self-attention eklemek**\n",
    "  3. **Cross-attention** mantığını korumak\n",
    "  4. Mevcut **GatedFFN** veya FFN’i Transformer feedforward ile uyarlamak\n",
    "- Bu sayede SEQ2SEQ modeli **GPT-style LLM** mantığıyla çalışabilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac624d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
