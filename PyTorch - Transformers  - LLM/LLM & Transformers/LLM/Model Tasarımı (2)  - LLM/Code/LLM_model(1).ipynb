{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565f63d8",
   "metadata": {},
   "source": [
    "# ğŸ§­ LLMâ€™e Giden Yol HaritasÄ± (Katman Katman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca1b1f",
   "metadata": {},
   "source": [
    "| AÅŸama   | Katman / Kavram                 | Ne Ã–ÄŸreneceÄŸiz                                      | Kodla mÄ±?         |\n",
    "| ------- | ------------------------------- | --------------------------------------------------- | ----------------- |\n",
    "| **1ï¸âƒ£** | **Tokenization**                | Metni sayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼rmek (subword, BPE)           | âœ…                 |\n",
    "| **2ï¸âƒ£** | **Embedding Layer**             | Token ID â†’ VektÃ¶r                                   | âœ…                 |\n",
    "| **3ï¸âƒ£** | **Positional Encoding**         | SÄ±ra bilgisini modele eklemek                       | âœ…                 |\n",
    "| **4ï¸âƒ£** | **Self-Attention (Tek baÅŸÄ±na)** | Dikkat mekanizmasÄ±nÄ±n Ã¶zÃ¼ (Q, K, V)                 | âœ…                 |\n",
    "| **5ï¸âƒ£** | **Multi-Head Attention**        | Paralel dikkat baÅŸlarÄ±yla temsil gÃ¼cÃ¼               | âœ…                 |\n",
    "| **6ï¸âƒ£** | **Feed-Forward Network (MLP)**  | Her tokenâ€™Ä±n bilgisine baÄŸÄ±msÄ±z dÃ¶nÃ¼ÅŸÃ¼m             | âœ…                 |\n",
    "| **7ï¸âƒ£** | **Residual + LayerNorm**        | Stabilite ve gradient akÄ±ÅŸÄ±                         | âœ…                 |\n",
    "| **8ï¸âƒ£** | **Transformer Block**           | YukarÄ±dakilerin birleÅŸimi                           | âœ…                 |\n",
    "| **9ï¸âƒ£** | **LLM Base Model (GPT-style)**  | Embedding + N adet Transformer Block + Output Head  | âœ…                 |\n",
    "| **ğŸ”Ÿ**  | **Training Loop & Masking**     | Causal mask, loss hesaplama, autoregressive Ã¶ÄŸrenme | âœ…                 |\n",
    "| **âš¡**   | **Inference (Text Generation)** | Greedy, sampling, top-k/p decoding                  | âœ…                 |\n",
    "| **ğŸš€**  | **GeliÅŸmiÅŸler**                 | KV Cache, RoPE, FlashAttention, LoRA                | âš™ï¸ (isteÄŸe baÄŸlÄ±) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023fa48",
   "metadata": {},
   "source": [
    "# ğŸ§± LLM Modelini Katman Katman Ä°nÅŸa PlanÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5fc52",
   "metadata": {},
   "source": [
    "| AÅŸama | Katman                         | AmaÃ§                                         |\n",
    "| ----- | ------------------------------ | -------------------------------------------- |\n",
    "| 1ï¸âƒ£   | **Embedding Layer**            | Token idâ€™lerini sÃ¼rekli vektÃ¶r uzayÄ±na taÅŸÄ±r |\n",
    "| 2ï¸âƒ£   | **Positional Encoding**        | SÄ±ralama bilgisini modele katar              |\n",
    "| 3ï¸âƒ£   | **Self-Attention (QKV)**       | Tokenâ€™larÄ±n birbirine dikkat etmesini saÄŸlar |\n",
    "| 4ï¸âƒ£   | **Multi-Head Attention**       | Bilgiyi farklÄ± alt uzaylarda iÅŸler           |\n",
    "| 5ï¸âƒ£   | **Feed Forward Network (MLP)** | Her token bilgisini ayrÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r          |\n",
    "| 6ï¸âƒ£   | **Residual + LayerNorm**       | Ã–ÄŸrenmeyi stabilize eder                     |\n",
    "| 7ï¸âƒ£   | **Transformer Block**          | YukarÄ±dakilerin birleÅŸimi                    |\n",
    "| 8ï¸âƒ£   | **LLM (GPT-style)**            | Embedding + N blok + Ã§Ä±kÄ±ÅŸ projeksiyonu      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54366a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c83ef",
   "metadata": {},
   "source": [
    "# ğŸ”¹ BaÅŸlangÄ±Ã§: 1ï¸âƒ£ Embedding KatmanÄ±\n",
    "ğŸ¯ AmaÃ§:\n",
    "\n",
    "* Token ID â†’ vektÃ¶r uzayÄ±na dÃ¶nÃ¼ÅŸÃ¼m.\n",
    "* Her kelime iÃ§in Ã¶ÄŸrenilebilir bir temsil vektÃ¶rÃ¼."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bff1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        return self.embedding(x)  # [batch_size, seq_len, embed_dim]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a14d0",
   "metadata": {},
   "source": [
    "# ğŸ”¹ 2ï¸âƒ£ Positional Encoding (Sin/Cos)\n",
    "\n",
    "* Transformers sÄ±rayÄ± doÄŸal olarak bilmez, bu yÃ¼zden pozisyon bilgisi eklememiz gerekir.\n",
    "\n",
    "ğŸ§  Teori\n",
    "\n",
    "* Pozisyon bilgisi sinÃ¼s ve kosinÃ¼s fonksiyonlarÄ±yla kodlanÄ±r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef197bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed56481",
   "metadata": {},
   "source": [
    "# 3ï¸âƒ£ Self-Attention (Tek BaÅŸlÄ±)\n",
    "ğŸ§  Teori\n",
    "\n",
    "* Her token, diÄŸer tokenâ€™lara dikkat (attention) uygular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29f8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = embed_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        return attn @ V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d315c91",
   "metadata": {},
   "source": [
    "# ğŸ”¹ 4ï¸âƒ£ Multi-Head Attention\n",
    "\n",
    "* Birden fazla attention baÅŸÄ± paralel Ã§alÄ±ÅŸÄ±r.\n",
    "* Her biri farklÄ± bir alt uzayda dikkat Ã¶ÄŸrenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558d3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)  # her biri [B, T, num_heads, head_dim]\n",
    "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b61b1b",
   "metadata": {},
   "source": [
    "# ğŸ§© 5ï¸âƒ£ Feed Forward (MLP) KatmanÄ±\n",
    "ğŸ¯ AmaÃ§\n",
    "\n",
    "* Her tokenâ€™Ä±n temsilini baÄŸÄ±msÄ±z olarak dÃ¶nÃ¼ÅŸtÃ¼rmek.\n",
    "* Self-Attention, tokenâ€™lar arasÄ± etkileÅŸimi yakalarken;\n",
    "* FeedForward, her tokenâ€™Ä±n kendi iÃ§ temsiline dÃ¶nÃ¼ÅŸÃ¼m uygular.\n",
    "> Bu yapÄ± sayesinde model non-linearity kazanÄ±r ve her token kendi bilgisi Ã¼zerinde Ã§alÄ±ÅŸabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf21dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim: int, expansion: int = 4, dp: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(expansion * embed_dim, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ab29d",
   "metadata": {},
   "source": [
    "# âš–ï¸ 6ï¸âƒ£ Layer Normalization + Residual Connection\n",
    "ğŸ¯ AmaÃ§\n",
    "\n",
    "* Derin aÄŸlarda gradyan akÄ±ÅŸÄ±nÄ± stabilize etmek ve Ã¶ÄŸrenmeyi dengede tutmak.\n",
    "\n",
    "* Residual Connection: GiriÅŸ + Ã‡Ä±kÄ±ÅŸ â†’ \n",
    "\n",
    "* LayerNorm: Katman bazlÄ± normalizasyon (her Ã¶rnekte tÃ¼m embedding boyutu iÃ§in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb89e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, layer: nn.Module, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x + self.layer(self.norm(x, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699e1ca",
   "metadata": {},
   "source": [
    "# ğŸ§± 7ï¸âƒ£ Transformer Block (Tam Hal)\n",
    "\n",
    "ArtÄ±k elimizde:\n",
    "\n",
    "* MultiHeadAttention\n",
    "\n",
    "* FeedForward\n",
    "\n",
    "* Residual + LayerNorm\n",
    "\n",
    "BunlarÄ± birleÅŸtirip tek bir Transformer bloÄŸu yapÄ±yoruz.\n",
    "\n",
    "* Bu bloÄŸu N kez Ã¼st Ã¼ste koyduÄŸumuzda â†’ LLM oluÅŸur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495b4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dp: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dp = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1ï¸âƒ£ Attention + Residual\n",
    "        attn_out = self.attn(self.norm1(x), mask=mask)\n",
    "        x = x + self.dp(attn_out)\n",
    "\n",
    "        # 2ï¸âƒ£ FeedForward + Residual\n",
    "        ff_out = self.ff(self.norm2(x))\n",
    "        x = x + self.dp(ff_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67425b",
   "metadata": {},
   "source": [
    "---\n",
    "## GeÃ§en REPO'da yaptÄ±klarÄ±mÄ±za bakalÄ±m.O modeli aÅŸaÄŸÄ±ya bÄ±rakÄ±yorum.Bu model Ã¼zerinden geliÅŸtirmelere bakayacaÄŸÄ±z.EÄŸer o repoyu incelemediyseniz aÅŸaÄŸÄ±ya baÄŸlantÄ±yÄ± bÄ±rakÄ±yorum;\n",
    "\n",
    "> ### \"https://github.com/huseyin-dgn/PyTorch-Bolum_5---SeqtoSeq_Transformers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9c637",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d047b31",
   "metadata": {},
   "source": [
    "### BahsettiÄŸim model kodunu aÅŸaÄŸÄ±ya tekrar yazalÄ±m ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05c1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=None, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads or max(1, embed_dim // 64)\n",
    "        self.head_dim = embed_dim // self.num_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, S, E = x.size()\n",
    "        return x.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        B, S, _, _ = x.shape\n",
    "        return x.view(B, S, self.embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                scores += attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                scores += attn_mask.unsqueeze(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask, torch.finfo(scores.dtype).min)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.combine_heads(out)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "class EnchancedLSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 256, hidden_size: int = 512,\n",
    "                 num_layers: int = 2, bidirectional: bool = True, dp: float = 0.1,\n",
    "                 use_attn: bool = True, pre_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        self.use_attn = use_attn\n",
    "        self.pre_norm = pre_norm\n",
    "        self.dropout = dp\n",
    "\n",
    "        lstm_input_dim = hidden_size * self.directions\n",
    "        self.residual_proj = nn.Linear(embedding_dim, lstm_input_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dp if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.layer_norm_in = nn.LayerNorm(lstm_input_dim)\n",
    "        self.layer_norm_out = nn.LayerNorm(lstm_input_dim)\n",
    "        self.attention = MultiHeadAttention(lstm_input_dim) if use_attn else None\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(lstm_input_dim, lstm_input_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(lstm_input_dim * 4, lstm_input_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "                attn_mask: Optional[torch.Tensor] = None):\n",
    "\n",
    "        embedded = self.embedding(src)\n",
    "        residual = self.residual_proj(embedded)\n",
    "        lstm_input = self.layer_norm_in(residual) if self.pre_norm else residual\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(lstm_input)\n",
    "        outputs = self.layer_norm_out(outputs)\n",
    "\n",
    "        if self.attention:\n",
    "            attn_out = self.attention(outputs, outputs, outputs,\n",
    "                                      key_padding_mask=src_key_padding_mask,\n",
    "                                      attn_mask=attn_mask)\n",
    "            outputs = outputs + attn_out\n",
    "\n",
    "        outputs = outputs + self.ffn(outputs)\n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_dim : int , num_heads:Optional[int] =None , dp:float = 0.3):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads or max(1, embed_dim//64)\n",
    "        self.head_dim = embed_dim // self.num_heads\n",
    "        assert embed_dim % self.num_heads == 0 , \"embed_idm ve num_heads bÃ¶lÃ¼nemiyor\"\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim , embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim , embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self,x):\n",
    "        B,S,E = x.size()\n",
    "        return x.view(B,S,self.num_heads , self.head_dim).transpose(1,2)\n",
    "\n",
    "    def combine_heads(self,x):\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "        B,S,_,_ = x.size()\n",
    "        return x.view(B,S,self.embed_dim)\n",
    "    \n",
    "    def forward(self,query,key,value,key_padding_mask = None , attn_mask = None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "\n",
    "        scores = torch.matmul(Q,K.transpose(-2,-1)) / self.scale\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() == 2:\n",
    "                scores = scores + attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif attn_mask.dim() == 3:\n",
    "                scores = scores + attn_mask.unsqueeze(1)\n",
    "\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask, torch.finfo(scores.dtype).min)\n",
    "\n",
    "\n",
    "        attn = F.softmax(scores , dim = -1 )\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn , V)\n",
    "        out = self.combine_heads(out)\n",
    "\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "def init_decoder_states_from_encoder(hidden, cell, num_decoder_layers, decoder_hidden_size,\n",
    "                                     encoder_directions=1, proj_layer=None):\n",
    "    num_enc_total, B, enc_hidden = hidden.size()\n",
    "    num_enc_layers = num_enc_total // encoder_directions\n",
    "\n",
    "    h = hidden.view(num_enc_layers, encoder_directions, B, enc_hidden)\n",
    "    c = cell.view(num_enc_layers, encoder_directions, B, enc_hidden)\n",
    "\n",
    "    # Bidirectional merge\n",
    "    h_merged = h.mean(dim=1)  # (num_layers, B, enc_hidden)\n",
    "    c_merged = c.mean(dim=1)\n",
    "\n",
    "    if proj_layer is None and enc_hidden != decoder_hidden_size:\n",
    "        proj_layer = nn.Linear(enc_hidden, decoder_hidden_size).to(hidden.device)\n",
    "\n",
    "    if proj_layer is not None:\n",
    "        L = num_enc_layers\n",
    "        # âŒ DÃ¼zeltme: directions ile Ã§arpma yok\n",
    "        h_proj = proj_layer(h_merged.view(L * B, enc_hidden)).view(L, B, decoder_hidden_size)\n",
    "        c_proj = proj_layer(c_merged.view(L * B, enc_hidden)).view(L, B, decoder_hidden_size)\n",
    "    else:\n",
    "        h_proj, c_proj = h_merged, c_merged\n",
    "\n",
    "    if num_decoder_layers == num_enc_layers:\n",
    "        return h_proj, c_proj\n",
    "    elif num_decoder_layers > num_enc_layers:\n",
    "        h_final = torch.cat([h_proj[min(i, num_enc_layers - 1):min(i, num_enc_layers - 1) + 1]\n",
    "                             for i in range(num_decoder_layers)], dim=0)\n",
    "        c_final = torch.cat([c_proj[min(i, num_enc_layers - 1):min(i, num_enc_layers - 1) + 1]\n",
    "                             for i in range(num_decoder_layers)], dim=0)\n",
    "        return h_final, c_final\n",
    "    \n",
    "class GatedFFN(nn.Module):\n",
    "    def __init__(self, hidden_size:int , dp:float = 0.2):\n",
    "        super().__init__()\n",
    "        self.fc1= nn.Linear(hidden_size , hidden_size * 4)\n",
    "        self.fc2 = nn.Linear(hidden_size * 4 ,hidden_size)\n",
    "        self.gate = nn.Linear(hidden_size , hidden_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.fc2(self.act(self.fc1(x)))\n",
    "        gate = self.sigmoid(self.gate(x))\n",
    "        return self.dropout(out + gate)\n",
    "    \n",
    "class PositionelEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size : int , max_len : int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len , hidden_size)\n",
    "        position = torch.arange(0 , max_len , dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0 , hidden_size,2).float() *  (-math.log(100000.0) / hidden_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe' , pe)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[: , :seq_len , :]\n",
    "    \n",
    "class AdaptiveDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim = 256,hidden_size =512, num_layers=3,dp=0.3 , use_attn =True , pre_norm = True , max_len = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_norm = pre_norm\n",
    "        self.use_attn = use_attn\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.input_proj = nn.Linear(embedding_dim,hidden_size)\n",
    "        self.pos_encoding = PositionelEncoding(hidden_size , max_len=max_len)\n",
    "\n",
    "        self.lstm = nn.LSTM(hidden_size , hidden_size , num_layers=num_layers , dropout=dp  if num_layers>1 else 0.0 , batch_first=True)\n",
    "\n",
    "        self.layer_norm_residual = nn.LayerNorm(hidden_size)\n",
    "        self.attention = MultiHeadAttention(hidden_size) if use_attn else None\n",
    "        self.gated_ffn = GatedFFN(hidden_size , dp)\n",
    "        self.layer_norm_ffn = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden ,cell , src_key_padding = None , encoder_output = None , attn_mask = None):\n",
    "        embedded = self.input_proj(self.embedding(x))\n",
    "        embedded = self.pos_encoding(embedded)\n",
    "        lstm_input = self.layer_norm_residual(embedded) if self.pre_norm else embedded\n",
    "\n",
    "        outputs , (hidden , cell) = self.lstm(lstm_input , (hidden , cell))\n",
    "\n",
    "        if self.attention and encoder_output is not None:\n",
    "            attn_out = self.attention(outputs , encoder_output , encoder_output,key_padding_mask = src_key_padding , attn_mask = attn_mask)\n",
    "            outputs = outputs + attn_out\n",
    "        \n",
    "        outputs = outputs + self.gated_ffn(outputs)\n",
    "        outputs = self.layer_norm_ffn(outputs)\n",
    "\n",
    "        return outputs , hidden , cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a9a27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca6b12",
   "metadata": {},
   "source": [
    "## Åimdi bu modeli LLM'e entegre etmeye Ã§alÄ±ÅŸalÄ±m  ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0c8de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ed118",
   "metadata": {},
   "source": [
    "# ğŸ”¹ SEQ2SEQ Modeli Ä°ncelemesi ve LLM Uyarlama Analizi\n",
    "\n",
    "Bu hÃ¼cre, gÃ¶nderilen SEQ2SEQ kodunu temel alarak **LLM tarzÄ± bir Transformerâ€™a dÃ¶nÃ¼ÅŸÃ¼m adÄ±mlarÄ±nÄ±** Ã¶zetler.\n",
    "\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ Encoder (EnchancedLSTMEncoder)\n",
    "\n",
    "- **Embedding Layer:** `self.embedding` â†’ token IDâ€™lerini vektÃ¶re Ã§evirir.\n",
    "- **Residual Projection:** `self.residual_proj` â†’ embedding boyutunu LSTM giriÅŸ boyutuna eÅŸler.\n",
    "- **LayerNorm:** `self.layer_norm_in` / `self.layer_norm_out` â†’ pre/post norm iÅŸlemleri.\n",
    "- **LSTM:** Ã‡ok katmanlÄ±, bidirectional LSTM.\n",
    "- **Attention:** Opsiyonel `MultiHeadAttention`.\n",
    "- **FeedForward:** `self.ffn` â†’ Gated / GELU aktivasyonlu MLP.\n",
    "\n",
    "**ğŸ’¡ LLM uyarlamasÄ±:**\n",
    "- LSTM yerine **Transformer Encoder Block** kullanÄ±lacak.\n",
    "- Residual + LayerNorm + Attention + FFN yapÄ±sÄ± zaten hazÄ±r; doÄŸrudan Transformer blok mantÄ±ÄŸÄ±na uyarlanabilir.\n",
    "\n",
    "\n",
    "\n",
    "## 2ï¸âƒ£ Decoder (AdaptiveDecoder)\n",
    "\n",
    "- **Embedding + Input Projection:** `self.embedding` + `self.input_proj`.\n",
    "- **Positional Encoding:** `self.pos_encoding`.\n",
    "- **LSTM:** Ã‡ok katmanlÄ± decoder LSTM.\n",
    "- **Attention:** Encoder-Decoder attention (`MultiHeadAttention`).\n",
    "- **Gated FFN:** `self.gated_ffn`.\n",
    "- **LayerNorm:** `self.layer_norm_ffn`, `self.layer_norm_residual`.\n",
    "\n",
    "**ğŸ’¡ LLM uyarlamasÄ±:**\n",
    "- Decoder zaten **Transformer decoder mantÄ±ÄŸÄ±na yakÄ±n**:\n",
    "  - LSTM â†’ Masked Self-Attention + FeedForward\n",
    "  - Encoder-Decoder Attention â†’ Cross-Attention\n",
    "  - GatedFFN â†’ klasik FFN yerine kullanÄ±labilir\n",
    "- Positional Encoding ve Residual + LayerNorm yapÄ±sÄ± korunabilir.\n",
    "\n",
    "\n",
    "\n",
    "## 3ï¸âƒ£ MultiHeadAttention\n",
    "\n",
    "- Mevcut hem encoder hem decoder iÃ§in ortak `MultiHeadAttention`.\n",
    "- Split/Combine heads, QKV projeksiyonu, mask handling mevcut.\n",
    "- Ufak deÄŸiÅŸikliklerle **Transformer standard attention** (pre-norm + causal mask) yapÄ±labilir.\n",
    "\n",
    "\n",
    "\n",
    "## 4ï¸âƒ£ LLM Mimarisine GeÃ§iÅŸ NotlarÄ±\n",
    "\n",
    "| Mevcut | LLM UyarlamasÄ± |\n",
    "|---------|----------------|\n",
    "| LSTM tabanlÄ± encoder | Transformer encoder block |\n",
    "| LSTM tabanlÄ± decoder | Transformer decoder block (masked self-attention) |\n",
    "| Gated FFN / standard FFN | Transformer feedforward (ReLU/GELU) |\n",
    "| Encoder-decoder attention | Cross-attention |\n",
    "| Positional Encoding | Direkt kullanÄ±labilir |\n",
    "| LayerNorm + residual | Pre-norm yapÄ±sÄ± olarak aynen kullanÄ±labilir |\n",
    "\n",
    "\n",
    "\n",
    "## âœ… Ã–zet\n",
    "\n",
    "- Kod zaten **modÃ¼ler ve katmanlÄ±**, LLM iÃ§in gereken Ã§oÄŸu yapÄ± mevcut.\n",
    "- YapÄ±lmasÄ± gerekenler:\n",
    "  1. **LSTMâ€™leri kaldÄ±rÄ±p Transformer bloklarÄ± ile deÄŸiÅŸtirmek**\n",
    "  2. **Decoderâ€™da masked self-attention eklemek**\n",
    "  3. **Cross-attention** mantÄ±ÄŸÄ±nÄ± korumak\n",
    "  4. Mevcut **GatedFFN** veya FFNâ€™i Transformer feedforward ile uyarlamak\n",
    "- Bu sayede SEQ2SEQ modeli **GPT-style LLM** mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸabilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac624d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
