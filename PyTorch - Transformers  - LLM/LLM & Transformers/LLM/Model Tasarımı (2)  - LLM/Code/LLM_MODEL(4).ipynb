{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02300025",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4eb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T] (long)\n",
    "        return self.embedding(x)  # [B, T, embed_dim]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [B, T, C] -> [B, num_heads, T, head_dim]\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # x: [B, num_heads, T, head_dim] -> [B, T, C]\n",
    "        x = x.transpose(1, 2).contiguous()  # -> [B, T, num_heads, head_dim]\n",
    "        B, T, _, _ = x.size()\n",
    "        return x.view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [B, T, C]\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "\n",
    "        # scores: [B, num_heads, T_q, T_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # mask: expected shape can be [T_q, T_k] or [B, T_q, T_k] or broadcastable.\n",
    "        if mask is not None:\n",
    "            # mask should be 1 for allowed positions and 0 for masked positions\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)  # [B, num_heads, T_q, head_dim]\n",
    "        out = self.combine_heads(out)  # [B, T_q, C]\n",
    "        return self.out_proj(out)     # [B, T_q, C]\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm style\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderBlock(embed_dim, num_heads, dp) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        \"\"\"\n",
    "        src_tokens: [B, T] (long)\n",
    "        src_mask: either None or mask with shape broadcastable to [B, num_heads, T, T] OR [T, T] or [B, T, T]\n",
    "                  mask values: 1 -> allowed, 0 -> masked\n",
    "        returns: encoder_outputs [B, T, C]\n",
    "        \"\"\"\n",
    "        x = self.tok_emb(src_tokens)       # [B, T, C]\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "def build_padding_mask(pad_mask):\n",
    "    \"\"\"\n",
    "    pad_mask: [B, T] where 1 means token is valid, 0 means padding\n",
    "    returns mask [B, 1, T, T] or broadcastable mask of 1/0\n",
    "    \"\"\"\n",
    "    # We want mask of shape [B, T, T] where allowed positions = 1\n",
    "    if pad_mask is None:\n",
    "        return None\n",
    "    B, T = pad_mask.shape\n",
    "    # allowed positions along keys dimension\n",
    "    mask = pad_mask.unsqueeze(1) * pad_mask.unsqueeze(2)  # [B, T, T]\n",
    "    return mask  # 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a772241",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e7ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=4, dp=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dp),\n",
    "            nn.Linear(embed_dim * expansion, embed_dim),\n",
    "            nn.Dropout(dp)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8, dp=0.1, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, dp=dp)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        self_attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask=self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * self_attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_attn_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), mask=enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_attn_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_layers=6, num_heads=8, dp=0.1, max_len=5000, drop_path=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBlock(embed_dim, num_heads, dp, drop_path) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e6a08",
   "metadata": {},
   "source": [
    "# 🔹 Encoder\n",
    "\n",
    "* Token embedding + positional encoding\n",
    "\n",
    "* Multi-Head Self-Attention (mask opsiyonel)\n",
    "\n",
    "* FeedForward (MLP + GELU + dropout)\n",
    "\n",
    "* LayerNorm + residual + pre-norm\n",
    "\n",
    "* Çoklu encoder blokları (num_layers)\n",
    "\n",
    "* Padding mask builder\n",
    "\n",
    "# 🔹 Decoder\n",
    "\n",
    "* Token embedding + positional encoding\n",
    "\n",
    "* Masked self-attention\n",
    "\n",
    "* Cross-attention (encoder çıkışını alır)\n",
    "\n",
    "* FeedForward\n",
    "\n",
    "* LayerNorm + residual + pre-norm\n",
    "\n",
    "* DropPath (stochastic depth) + LayerScale (gamma_1,2,3)\n",
    "\n",
    "* Çoklu decoder blokları (num_layers)\n",
    "\n",
    "* Linear output layer (lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa579b8d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77dd83",
   "metadata": {},
   "source": [
    "# 1️⃣ Temel Parametre Artırımı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab0bbb",
   "metadata": {},
   "source": [
    "| Parametre    | Mevcut | Mini-LLM öneri |\n",
    "| ------------ | ------ | -------------- |\n",
    "| embed_dim    | 512    | 1024 / 2048    |\n",
    "| num_heads    | 8      | 16             |\n",
    "| num_layers   | 6      | 12             |\n",
    "| dropout      | 0.1    | 0.1-0.2        |\n",
    "| ff_expansion | 4      | 4-8            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03254c67",
   "metadata": {},
   "source": [
    "# 2️⃣ Positional Encoding\n",
    "\n",
    "* Rotary Positional Embeddings (RoPE) ekleyelim.\n",
    "\n",
    "* Daha uzun sequence’ler için scalable.\n",
    "\n",
    "# 3️⃣ Attention\n",
    "\n",
    "* Self + Cross Attention mevcut → koru.\n",
    "\n",
    "* Opsiyon: Flash Attention / memory-efficient attention kullanalım.\n",
    "\n",
    "# 4️⃣ FeedForward\n",
    "\n",
    "* Genişletilmiş expansion (4 → 8) ile kapasite artıralım.\n",
    "\n",
    "* GELU veya SwiGLU aktivasyonu kullanılabilir.\n",
    "\n",
    "# 5️⃣ DropPath + LayerScale\n",
    "\n",
    "* Mevcut LayerScale parametreleri ve DropPath’i koru → stabilite ve genelleme.\n",
    "\n",
    "# 6️⃣ Output Layer\n",
    "\n",
    "* Weight tying: embedding weight’i output head ile paylaşalım.\n",
    "\n",
    "* Logits üretiminde softmax → causal mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66902573",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6dda6f",
   "metadata": {},
   "source": [
    "# TAM ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e9d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Encoder Block (LLM)\n",
    "# -----------------------------\n",
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Encoder (LLM)\n",
    "# -----------------------------\n",
    "class TransformerEncoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359820a",
   "metadata": {},
   "source": [
    "# TAM DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb1ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# -----------------------------\n",
    "# Token Embedding\n",
    "# -----------------------------\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-Head Attention\n",
    "# -----------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))\n",
    "\n",
    "# -----------------------------\n",
    "# FeedForward\n",
    "# -----------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer Decoder Block (LLM)\n",
    "# -----------------------------\n",
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Full Transformer Decoder (LLM)\n",
    "# -----------------------------\n",
    "class TransformerDecoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab3b5b",
   "metadata": {},
   "source": [
    "# Transformer LLM Genel Akış Diyagramı (Markdown)\n",
    "```BASH\n",
    "\n",
    "Input Tokens (src_tokens / decoder input)\n",
    "│\n",
    "├─> **Encoder Side (src_tokens)**\n",
    "│    │\n",
    "│    ├─> Token Embedding\n",
    "│    │\n",
    "│    ├─> Positional Encoding\n",
    "│    │\n",
    "│    └─> **Encoder Layers** (N adet)\n",
    "│         │\n",
    "│         ├─ LayerNorm 1\n",
    "│         ├─ Self-Attention (mask optional)\n",
    "│         ├─ DropPath + γ₁ scaling\n",
    "│         └─ Residual Add\n",
    "│         │\n",
    "│         ├─ LayerNorm 2\n",
    "│         ├─ FeedForward Network (FFN)\n",
    "│         ├─ DropPath + γ₂ scaling\n",
    "│         └─ Residual Add\n",
    "│\n",
    "├─> Final LayerNorm\n",
    "│\n",
    "└─> **Encoder Output** (enc_out)\n",
    "      │\n",
    "      │\n",
    "      ▼\n",
    "Decoder Input Tokens (tgt_tokens)\n",
    "│\n",
    "├─> Token Embedding\n",
    "│\n",
    "├─> Positional Encoding\n",
    "│\n",
    "└─> **Decoder Layers** (M adet)\n",
    "     │\n",
    "     ├─ LayerNorm 1\n",
    "     ├─ Masked Self-Attention (decoder tokens only)\n",
    "     ├─ DropPath + γ₁ scaling\n",
    "     └─ Residual Add\n",
    "     │\n",
    "     ├─ LayerNorm 2\n",
    "     ├─ Cross-Attention (enc_out)\n",
    "     ├─ DropPath + γ₂ scaling\n",
    "     └─ Residual Add\n",
    "     │\n",
    "     ├─ LayerNorm 3\n",
    "     ├─ FeedForward Network (FFN)\n",
    "     ├─ DropPath + γ₃ scaling\n",
    "     └─ Residual Add\n",
    "│\n",
    "├─> Final LayerNorm\n",
    "│\n",
    "└─> LM Head\n",
    "     └─ Linear(embed_dim → vocab_size)\n",
    "     └─ Output: Logits (next token prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351859c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
