{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8c49ac",
   "metadata": {},
   "source": [
    "# ğŸ§  Large Language Model (LLM) â€” Teorik Temel\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ğŸ§© LLM Nedir?\n",
    "\n",
    "**LLM (Large Language Model)**, Ã§ok sayÄ±da parametreye (milyarlarca) sahip, metin Ã¼zerinde **dil modelleme** yapan bir **Transformer tabanlÄ± derin sinir aÄŸÄ±dÄ±r**.\n",
    "\n",
    "> Girdi olarak bir dizi token alÄ±r, bu dizideki baÄŸlamÄ± (*context*) Ã¶ÄŸrenir ve bir sonraki tokenâ€™Ä±n olasÄ±lÄ±ÄŸÄ±nÄ± tahmin eder.\n",
    "\n",
    "\n",
    "\n",
    "## 2. ğŸ§± LLMâ€™in Temel YapÄ±sÄ±\n",
    "\n",
    "Bir LLM genellikle ÅŸu ana bileÅŸenlerden oluÅŸur:\n",
    "\n",
    "| **BileÅŸen** | **GÃ¶rev** | **Ã–rnek** |\n",
    "|--------------|------------|------------|\n",
    "| **Embedding Layer** | Token IDâ€™lerini vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. | `nn.Embedding` |\n",
    "| **Positional Encoding** | SÄ±ralama bilgisini ekler (Transformers sÄ±ra bilmez). | `sin/cos` encoding |\n",
    "| **Transformer Blocks (N adet)** | Modelin â€œbeyniâ€; attention + feed-forward katmanlarÄ±ndan oluÅŸur. | `MultiHeadAttention`, `FeedForward` |\n",
    "| **LayerNorm + Residual** | EÄŸitimi stabilize eder. | `nn.LayerNorm` + skip connection |\n",
    "| **Output Projection (LM Head)** | Gizli temsili kelime olasÄ±lÄ±klarÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. | `nn.Linear(hidden_dim, vocab_size)` |\n",
    "\n",
    "\n",
    "\n",
    "## 3. âš™ï¸ Transformer Blockâ€™un Ä°Ã§ YapÄ±sÄ±\n",
    "\n",
    "Bir LLM bloÄŸu, aslÄ±nda klasik bir **Transformer Decoder bloÄŸudur (GPT tarzÄ±)**.  \n",
    "Ä°Ã§inde iki ana parÃ§a vardÄ±r:\n",
    "\n",
    "### ğŸ”¸ Masked Multi-Head Self-Attention\n",
    "- Her token sadece **kendinden Ã¶nceki tokenâ€™lara** bakabilir (nedensel / *causal mask*).\n",
    "- Bu sayede model â€œgeleceÄŸi gÃ¶rmezâ€.\n",
    "\n",
    "### ğŸ”¸ Feed-Forward Network (MLP)\n",
    "Her attention Ã§Ä±kÄ±ÅŸÄ±, baÄŸÄ±msÄ±z bir MLPâ€™den geÃ§irilir:\n",
    "\n",
    "\\[\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "\\]\n",
    "\n",
    "### ğŸ”¸ Residual + LayerNorm\n",
    "Her iki aÅŸamada da â€œskip connectionâ€ eklenir ve normalize edilir:\n",
    "\n",
    "\\[\n",
    "x = x + \\text{Attention}(x)\n",
    "\\]\n",
    "\\[\n",
    "x = x + \\text{FFN}(x)\n",
    "\\]\n",
    "\n",
    "\n",
    "## 4. ğŸ” TÃ¼m Modelin AkÄ±ÅŸÄ±\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0478cd",
   "metadata": {},
   "source": [
    "```\n",
    "bash\n",
    "Input Tokens â†’ Embedding â†’ Positional Encoding\n",
    "â†“\n",
    "[Transformer Block 1]\n",
    "â†“\n",
    "[Transformer Block 2]\n",
    "â†“\n",
    "...\n",
    "â†“\n",
    "[Transformer Block N]\n",
    "â†“\n",
    "LM Head (Linear Layer)\n",
    "â†“\n",
    "Softmax â†’ OlasÄ±lÄ±klar â†’ Tahmin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22346541",
   "metadata": {},
   "source": [
    "\n",
    "## 5. âš¡ LLMâ€™lerin Ã–zel Ã–zellikleri\n",
    "\n",
    "| **Ã–zellik** | **AÃ§Ä±klama** |\n",
    "|--------------|---------------|\n",
    "| **Causal Masking** | GeleceÄŸi gÃ¶rmeden Ã¼retim saÄŸlar. |\n",
    "| **Parameter Sharing** | Bazen embedding ve LM head aynÄ± aÄŸÄ±rlÄ±klarÄ± paylaÅŸÄ±r. |\n",
    "| **LayerNorm Before / After** | Modern modeller (GPT-Neo, LLaMA) genelde *pre-norm* kullanÄ±r. |\n",
    "| **Rotary Embeddings (RoPE)** | Sin/cos positional encoding yerine daha kararlÄ± alternatif. |\n",
    "| **FlashAttention / KV Cache** | Bellek ve hÄ±z optimizasyonlarÄ± saÄŸlar. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
