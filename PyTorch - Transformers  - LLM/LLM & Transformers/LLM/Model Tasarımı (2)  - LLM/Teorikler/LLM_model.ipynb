{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8c49ac",
   "metadata": {},
   "source": [
    "# 🧠 Large Language Model (LLM) — Teorik Temel\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 🧩 LLM Nedir?\n",
    "\n",
    "**LLM (Large Language Model)**, çok sayıda parametreye (milyarlarca) sahip, metin üzerinde **dil modelleme** yapan bir **Transformer tabanlı derin sinir ağıdır**.\n",
    "\n",
    "> Girdi olarak bir dizi token alır, bu dizideki bağlamı (*context*) öğrenir ve bir sonraki token’ın olasılığını tahmin eder.\n",
    "\n",
    "\n",
    "\n",
    "## 2. 🧱 LLM’in Temel Yapısı\n",
    "\n",
    "Bir LLM genellikle şu ana bileşenlerden oluşur:\n",
    "\n",
    "| **Bileşen** | **Görev** | **Örnek** |\n",
    "|--------------|------------|------------|\n",
    "| **Embedding Layer** | Token ID’lerini vektörlere dönüştürür. | `nn.Embedding` |\n",
    "| **Positional Encoding** | Sıralama bilgisini ekler (Transformers sıra bilmez). | `sin/cos` encoding |\n",
    "| **Transformer Blocks (N adet)** | Modelin “beyni”; attention + feed-forward katmanlarından oluşur. | `MultiHeadAttention`, `FeedForward` |\n",
    "| **LayerNorm + Residual** | Eğitimi stabilize eder. | `nn.LayerNorm` + skip connection |\n",
    "| **Output Projection (LM Head)** | Gizli temsili kelime olasılıklarına dönüştürür. | `nn.Linear(hidden_dim, vocab_size)` |\n",
    "\n",
    "\n",
    "\n",
    "## 3. ⚙️ Transformer Block’un İç Yapısı\n",
    "\n",
    "Bir LLM bloğu, aslında klasik bir **Transformer Decoder bloğudur (GPT tarzı)**.  \n",
    "İçinde iki ana parça vardır:\n",
    "\n",
    "### 🔸 Masked Multi-Head Self-Attention\n",
    "- Her token sadece **kendinden önceki token’lara** bakabilir (nedensel / *causal mask*).\n",
    "- Bu sayede model “geleceği görmez”.\n",
    "\n",
    "### 🔸 Feed-Forward Network (MLP)\n",
    "Her attention çıkışı, bağımsız bir MLP’den geçirilir:\n",
    "\n",
    "\\[\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "\\]\n",
    "\n",
    "### 🔸 Residual + LayerNorm\n",
    "Her iki aşamada da “skip connection” eklenir ve normalize edilir:\n",
    "\n",
    "\\[\n",
    "x = x + \\text{Attention}(x)\n",
    "\\]\n",
    "\\[\n",
    "x = x + \\text{FFN}(x)\n",
    "\\]\n",
    "\n",
    "\n",
    "## 4. 🔁 Tüm Modelin Akışı\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0478cd",
   "metadata": {},
   "source": [
    "```\n",
    "bash\n",
    "Input Tokens → Embedding → Positional Encoding\n",
    "↓\n",
    "[Transformer Block 1]\n",
    "↓\n",
    "[Transformer Block 2]\n",
    "↓\n",
    "...\n",
    "↓\n",
    "[Transformer Block N]\n",
    "↓\n",
    "LM Head (Linear Layer)\n",
    "↓\n",
    "Softmax → Olasılıklar → Tahmin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22346541",
   "metadata": {},
   "source": [
    "\n",
    "## 5. ⚡ LLM’lerin Özel Özellikleri\n",
    "\n",
    "| **Özellik** | **Açıklama** |\n",
    "|--------------|---------------|\n",
    "| **Causal Masking** | Geleceği görmeden üretim sağlar. |\n",
    "| **Parameter Sharing** | Bazen embedding ve LM head aynı ağırlıkları paylaşır. |\n",
    "| **LayerNorm Before / After** | Modern modeller (GPT-Neo, LLaMA) genelde *pre-norm* kullanır. |\n",
    "| **Rotary Embeddings (RoPE)** | Sin/cos positional encoding yerine daha kararlı alternatif. |\n",
    "| **FlashAttention / KV Cache** | Bellek ve hız optimizasyonları sağlar. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
