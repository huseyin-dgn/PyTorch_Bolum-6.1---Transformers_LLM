{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe51903",
   "metadata": {},
   "source": [
    "## ⚙️ 1️⃣ LLM’de Compile Kavramı\n",
    "\n",
    "`compile` fonksiyonu, özellikle **Keras/TensorFlow** dünyasında modelin eğitime hazır hale gelmesi için kullanılır. Burada 3 ana şey tanımlanır:\n",
    "\n",
    "- **Optimizer** – modelin ağırlıklarını güncelleme yöntemi  \n",
    "  Örnekler: `Adam`, `AdamW`, `SGD`, `Adafactor`  \n",
    "  LLM’lerde genellikle `AdamW` veya `Adafactor` tercih edilir.\n",
    "\n",
    "- **Loss Function (Kayıp Fonksiyonu)** – modelin tahmin hatasını ölçer  \n",
    "  Seq2Seq / LLM için genellikle `CrossEntropyLoss` kullanılır (logit + softmax + target).\n",
    "\n",
    "- **Metrics (Opsiyonel)** – eğitim sırasında performans izlemek için  \n",
    "  Örnek: `accuracy`, `perplexity`\n",
    "\n",
    "> ⚠️ LLM’lerde klasik `accuracy` çoğu zaman yanıltıcıdır, onun yerine **perplexity** veya **token-level accuracy** tercih edilir.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 2️⃣ LLM’de Loss Fonksiyonu\n",
    "\n",
    "LLM’lerde **loss** genellikle **token-level cross entropy** ile hesaplanır:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{V} y_{ij} \\log(\\hat{y}_{ij})\n",
    "\\]\n",
    "\n",
    "- \\(N\\) = batch içindeki token sayısı  \n",
    "- \\(V\\) = vocabulary boyutu  \n",
    "- \\(y_{ij}\\) = gerçek token one-hot  \n",
    "- \\(\\hat{y}_{ij}\\) = modelin tahmini token olasılığı  \n",
    "\n",
    "**Masking:** Padding tokenlerini loss hesaplamaya dahil etmeyiz, aksi halde model yanlış yönlendirilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28b6e7",
   "metadata": {},
   "source": [
    "## ⚙️ 3️⃣ PyTorch Örneği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model ve optimizer\n",
    "model = MySeq2SeqModel(vocab_size=30522, d_model=512)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Eğitim adımı\n",
    "def train_step(input_ids, target_ids):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, labels=target_ids)\n",
    "    loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)),\n",
    "                     target_ids.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3963c",
   "metadata": {},
   "source": [
    "### Notlar:\n",
    "\n",
    "* outputs.logits shape’i [batch, seq_len, vocab_size]\n",
    "\n",
    "* Flatten işlemi ile [batch*seq_len, vocab_size] yapıyoruz, target da [batch*seq_len]\n",
    "\n",
    "* ignore_index padding tokenlerini göz ardı eder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2400e",
   "metadata": {},
   "source": [
    "## ⚙️ 4️⃣ Gelişmiş Loss Seçenekleri\n",
    "\n",
    "* Label Smoothing – modelin aşırı confident olmasını engeller\n",
    "\n",
    "* Focal Loss – nadir tokenler için ağırlık verir\n",
    "\n",
    "* Perplexity – loss’u anlamlı bir ölçüye çevirir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cad29",
   "metadata": {},
   "source": [
    "## ⚙️ 5️⃣ Özet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d760d0",
   "metadata": {},
   "source": [
    "| Adım            | Açıklama                                       |\n",
    "| --------------- | ---------------------------------------------- |\n",
    "| Optimizer       | Ağırlıkları güncellemek için (AdamW/Adafactor) |\n",
    "| Loss            | Token-level CrossEntropy, padding maskeli      |\n",
    "| Metrics         | Token accuracy / perplexity                    |\n",
    "| Label smoothing | Overfitting önler                              |\n",
    "| Masking         | Padding tokenleri hariç tutar                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c781da",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12852b7",
   "metadata": {},
   "source": [
    "# 1️⃣ Optimizer’lar (LLM’lerde yaygın)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1b76e",
   "metadata": {},
   "source": [
    "| Optimizer          | Açıklama                                                                                                                         | Öne çıkan parametreler                                                                              |\n",
    "| ------------------ | -------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **AdamW**          | Adam’ın weight decay (L2 regularization) eklenmiş versiyonu. Transformer tabanlı LLM’lerde en yaygın kullanılan optimizer.       | `lr` (learning rate), `betas` (momentum), `eps` (küçük sabit), `weight_decay`                       |\n",
    "| **Adafactor**      | RAM verimliliği yüksek, büyük model ve düşük batch size için optimize edilmiş Adam türevi. HuggingFace’de LLM’ler için standart. | `lr`, `eps1`, `eps2`, `clip_threshold`, `relative_step` (learning rate schedule), `scale_parameter` |\n",
    "| **Adam**           | Klasik Adam, küçük modeller veya prototipler için. LLM’lerde genellikle weight decay için AdamW tercih edilir.                   | `lr`, `betas`, `eps`                                                                                |\n",
    "| **SGD / Momentum** | Nadir kullanılır, genellikle LLM’lerde tercih edilmez.                                                                           | `lr`, `momentum`, `weight_decay`                                                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21964b7e",
   "metadata": {},
   "source": [
    "# 2️⃣ Loss Fonksiyonları (LLM / Seq2Seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343af406",
   "metadata": {},
   "source": [
    "| Loss Fonksiyonu                    | Açıklama                                                                                                 | Öne çıkan parametreler                                                                |\n",
    "| ---------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **CrossEntropyLoss (token-level)** | Token başına softmax + log + negatif log-likelihood. Seq2Seq ve LLM’lerde en standart loss.              | `ignore_index` (padding tokenleri ignore etmek için), `reduction` (`mean` veya `sum`) |\n",
    "| **LabelSmoothingCrossEntropy**     | CrossEntropy’nin label smoothing ile geliştirilmiş versiyonu. Modelin aşırı confident olmasını engeller. | `smoothing` (örn. 0.1), `ignore_index`, `reduction`                                   |\n",
    "| **Focal Loss**                     | Nadir tokenler veya dengesiz token dağılımları için ağırlık verir.                                       | `gamma`, `alpha`, `ignore_index`                                                      |\n",
    "| **KLDivLoss / Perplexity**         | Genellikle dağılım farklılıklarını ölçmek veya distillation için kullanılır.                             | `log_target`, `reduction`                                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13589a60",
   "metadata": {},
   "source": [
    "# 3️⃣ Parametrelerin Tipik Ayarları (Büyük LLM Örneği)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8d972",
   "metadata": {},
   "source": [
    "| Parametre      | Tipik Değer                                            |\n",
    "| -------------- | ------------------------------------------------------ |\n",
    "| `lr`           | 1e-4 … 5e-5 (warmup ile)                               |\n",
    "| `weight_decay` | 0.01 … 0.1                                             |\n",
    "| `betas`        | (0.9, 0.999)                                           |\n",
    "| `eps`          | 1e-8                                                   |\n",
    "| `smoothing`    | 0.1 (label smoothing için)                             |\n",
    "| `ignore_index` | pad_token_id (genellikle 0 veya tokenizer özel değeri) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e83da5",
   "metadata": {},
   "source": [
    "### 💡 Özet Mantık:\n",
    "\n",
    "* Optimizer: Model ağırlıklarını stabil ve verimli güncellemek için → AdamW veya Adafactor.\n",
    "\n",
    "* Loss: Her tokenin hatasını ölçmek için → token-level CrossEntropy, isteğe bağlı label smoothing.\n",
    "\n",
    "* Parametreler: Learning rate, weight decay, betas, eps ve padding mask en kritik ayarlar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b056dca",
   "metadata": {},
   "source": [
    "# = >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f26ef",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW, Adafactor, get_linear_schedule_with_warmup\n",
    "\n",
    "# -----------------------\n",
    "# Model Tanımı (Örnek)\n",
    "# -----------------------\n",
    "model = MySeq2SeqModel(vocab_size=30522, d_model=512)\n",
    "\n",
    "# -----------------------\n",
    "# 1️⃣ Optimizer Seçimi\n",
    "# -----------------------\n",
    "# AdamW: LLM'lerde standart\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5,          # learning rate\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Alternatif: Adafactor (hafıza dostu)\n",
    "# optimizer = Adafactor(\n",
    "#     model.parameters(),\n",
    "#     lr=None,\n",
    "#     relative_step=True,\n",
    "#     scale_parameter=True,\n",
    "#     warmup_init=True\n",
    "# )\n",
    "\n",
    "# -----------------------\n",
    "# 2️⃣ Loss Fonksiyonu\n",
    "# -----------------------\n",
    "# Padding tokenlerini ignore eden standart CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# -----------------------\n",
    "# 3️⃣ Scheduler (Opsiyonel)\n",
    "# -----------------------\n",
    "num_training_steps = 10000\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4️⃣ Train Step (Compile-like)\n",
    "# -----------------------\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Model forward\n",
    "    outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # logits: [batch, seq_len, vocab_size], target: [batch, seq_len]\n",
    "    loss = criterion(\n",
    "        outputs.logits.view(-1, outputs.logits.size(-1)),\n",
    "        target_ids.view(-1)\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()  # opsiyonel\n",
    "    return loss.item()\n",
    "\n",
    "# -----------------------\n",
    "# 5️⃣ Metric Örnek\n",
    "# -----------------------\n",
    "def perplexity(loss):\n",
    "    return torch.exp(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b66e5c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
