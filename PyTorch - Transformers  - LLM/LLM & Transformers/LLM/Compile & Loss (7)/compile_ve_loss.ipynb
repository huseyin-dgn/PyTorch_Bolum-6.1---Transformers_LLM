{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe51903",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1Ô∏è‚É£ LLM‚Äôde Compile Kavramƒ±\n",
    "\n",
    "`compile` fonksiyonu, √∂zellikle **Keras/TensorFlow** d√ºnyasƒ±nda modelin eƒüitime hazƒ±r hale gelmesi i√ßin kullanƒ±lƒ±r. Burada 3 ana ≈üey tanƒ±mlanƒ±r:\n",
    "\n",
    "- **Optimizer** ‚Äì modelin aƒüƒ±rlƒ±klarƒ±nƒ± g√ºncelleme y√∂ntemi  \n",
    "  √ñrnekler: `Adam`, `AdamW`, `SGD`, `Adafactor`  \n",
    "  LLM‚Äôlerde genellikle `AdamW` veya `Adafactor` tercih edilir.\n",
    "\n",
    "- **Loss Function (Kayƒ±p Fonksiyonu)** ‚Äì modelin tahmin hatasƒ±nƒ± √∂l√ßer  \n",
    "  Seq2Seq / LLM i√ßin genellikle `CrossEntropyLoss` kullanƒ±lƒ±r (logit + softmax + target).\n",
    "\n",
    "- **Metrics (Opsiyonel)** ‚Äì eƒüitim sƒ±rasƒ±nda performans izlemek i√ßin  \n",
    "  √ñrnek: `accuracy`, `perplexity`\n",
    "\n",
    "> ‚ö†Ô∏è LLM‚Äôlerde klasik `accuracy` √ßoƒüu zaman yanƒ±ltƒ±cƒ±dƒ±r, onun yerine **perplexity** veya **token-level accuracy** tercih edilir.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 2Ô∏è‚É£ LLM‚Äôde Loss Fonksiyonu\n",
    "\n",
    "LLM‚Äôlerde **loss** genellikle **token-level cross entropy** ile hesaplanƒ±r:\n",
    "\n",
    "\\[\n",
    "\\text{Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{V} y_{ij} \\log(\\hat{y}_{ij})\n",
    "\\]\n",
    "\n",
    "- \\(N\\) = batch i√ßindeki token sayƒ±sƒ±  \n",
    "- \\(V\\) = vocabulary boyutu  \n",
    "- \\(y_{ij}\\) = ger√ßek token one-hot  \n",
    "- \\(\\hat{y}_{ij}\\) = modelin tahmini token olasƒ±lƒ±ƒüƒ±  \n",
    "\n",
    "**Masking:** Padding tokenlerini loss hesaplamaya dahil etmeyiz, aksi halde model yanlƒ±≈ü y√∂nlendirilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28b6e7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3Ô∏è‚É£ PyTorch √ñrneƒüi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "\n",
    "# Model ve optimizer\n",
    "model = MySeq2SeqModel(vocab_size=30522, d_model=512)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Eƒüitim adƒ±mƒ±\n",
    "def train_step(input_ids, target_ids):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, labels=target_ids)\n",
    "    loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)),\n",
    "                     target_ids.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3963c",
   "metadata": {},
   "source": [
    "### Notlar:\n",
    "\n",
    "* outputs.logits shape‚Äôi [batch, seq_len, vocab_size]\n",
    "\n",
    "* Flatten i≈ülemi ile [batch*seq_len, vocab_size] yapƒ±yoruz, target da [batch*seq_len]\n",
    "\n",
    "* ignore_index padding tokenlerini g√∂z ardƒ± eder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2400e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 4Ô∏è‚É£ Geli≈ümi≈ü Loss Se√ßenekleri\n",
    "\n",
    "* Label Smoothing ‚Äì modelin a≈üƒ±rƒ± confident olmasƒ±nƒ± engeller\n",
    "\n",
    "* Focal Loss ‚Äì nadir tokenler i√ßin aƒüƒ±rlƒ±k verir\n",
    "\n",
    "* Perplexity ‚Äì loss‚Äôu anlamlƒ± bir √∂l√ß√ºye √ßevirir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cad29",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 5Ô∏è‚É£ √ñzet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d760d0",
   "metadata": {},
   "source": [
    "| Adƒ±m            | A√ßƒ±klama                                       |\n",
    "| --------------- | ---------------------------------------------- |\n",
    "| Optimizer       | Aƒüƒ±rlƒ±klarƒ± g√ºncellemek i√ßin (AdamW/Adafactor) |\n",
    "| Loss            | Token-level CrossEntropy, padding maskeli      |\n",
    "| Metrics         | Token accuracy / perplexity                    |\n",
    "| Label smoothing | Overfitting √∂nler                              |\n",
    "| Masking         | Padding tokenleri hari√ß tutar                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c781da",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12852b7",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Optimizer‚Äôlar (LLM‚Äôlerde yaygƒ±n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1b76e",
   "metadata": {},
   "source": [
    "| Optimizer          | A√ßƒ±klama                                                                                                                         | √ñne √ßƒ±kan parametreler                                                                              |\n",
    "| ------------------ | -------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **AdamW**          | Adam‚Äôƒ±n weight decay (L2 regularization) eklenmi≈ü versiyonu. Transformer tabanlƒ± LLM‚Äôlerde en yaygƒ±n kullanƒ±lan optimizer.       | `lr` (learning rate), `betas` (momentum), `eps` (k√º√ß√ºk sabit), `weight_decay`                       |\n",
    "| **Adafactor**      | RAM verimliliƒüi y√ºksek, b√ºy√ºk model ve d√º≈ü√ºk batch size i√ßin optimize edilmi≈ü Adam t√ºrevi. HuggingFace‚Äôde LLM‚Äôler i√ßin standart. | `lr`, `eps1`, `eps2`, `clip_threshold`, `relative_step` (learning rate schedule), `scale_parameter` |\n",
    "| **Adam**           | Klasik Adam, k√º√ß√ºk modeller veya prototipler i√ßin. LLM‚Äôlerde genellikle weight decay i√ßin AdamW tercih edilir.                   | `lr`, `betas`, `eps`                                                                                |\n",
    "| **SGD / Momentum** | Nadir kullanƒ±lƒ±r, genellikle LLM‚Äôlerde tercih edilmez.                                                                           | `lr`, `momentum`, `weight_decay`                                                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21964b7e",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Loss Fonksiyonlarƒ± (LLM / Seq2Seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343af406",
   "metadata": {},
   "source": [
    "| Loss Fonksiyonu                    | A√ßƒ±klama                                                                                                 | √ñne √ßƒ±kan parametreler                                                                |\n",
    "| ---------------------------------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **CrossEntropyLoss (token-level)** | Token ba≈üƒ±na softmax + log + negatif log-likelihood. Seq2Seq ve LLM‚Äôlerde en standart loss.              | `ignore_index` (padding tokenleri ignore etmek i√ßin), `reduction` (`mean` veya `sum`) |\n",
    "| **LabelSmoothingCrossEntropy**     | CrossEntropy‚Äônin label smoothing ile geli≈ütirilmi≈ü versiyonu. Modelin a≈üƒ±rƒ± confident olmasƒ±nƒ± engeller. | `smoothing` (√∂rn. 0.1), `ignore_index`, `reduction`                                   |\n",
    "| **Focal Loss**                     | Nadir tokenler veya dengesiz token daƒüƒ±lƒ±mlarƒ± i√ßin aƒüƒ±rlƒ±k verir.                                       | `gamma`, `alpha`, `ignore_index`                                                      |\n",
    "| **KLDivLoss / Perplexity**         | Genellikle daƒüƒ±lƒ±m farklƒ±lƒ±klarƒ±nƒ± √∂l√ßmek veya distillation i√ßin kullanƒ±lƒ±r.                             | `log_target`, `reduction`                                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13589a60",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Parametrelerin Tipik Ayarlarƒ± (B√ºy√ºk LLM √ñrneƒüi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8d972",
   "metadata": {},
   "source": [
    "| Parametre      | Tipik Deƒüer                                            |\n",
    "| -------------- | ------------------------------------------------------ |\n",
    "| `lr`           | 1e-4 ‚Ä¶ 5e-5 (warmup ile)                               |\n",
    "| `weight_decay` | 0.01 ‚Ä¶ 0.1                                             |\n",
    "| `betas`        | (0.9, 0.999)                                           |\n",
    "| `eps`          | 1e-8                                                   |\n",
    "| `smoothing`    | 0.1 (label smoothing i√ßin)                             |\n",
    "| `ignore_index` | pad_token_id (genellikle 0 veya tokenizer √∂zel deƒüeri) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e83da5",
   "metadata": {},
   "source": [
    "### üí° √ñzet Mantƒ±k:\n",
    "\n",
    "* Optimizer: Model aƒüƒ±rlƒ±klarƒ±nƒ± stabil ve verimli g√ºncellemek i√ßin ‚Üí AdamW veya Adafactor.\n",
    "\n",
    "* Loss: Her tokenin hatasƒ±nƒ± √∂l√ßmek i√ßin ‚Üí token-level CrossEntropy, isteƒüe baƒülƒ± label smoothing.\n",
    "\n",
    "* Parametreler: Learning rate, weight decay, betas, eps ve padding mask en kritik ayarlar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b056dca",
   "metadata": {},
   "source": [
    "# = >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f26ef",
   "metadata": {},
   "source": [
    "```python \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW, Adafactor, get_linear_schedule_with_warmup\n",
    "\n",
    "# -----------------------\n",
    "# Model Tanƒ±mƒ± (√ñrnek)\n",
    "# -----------------------\n",
    "model = MySeq2SeqModel(vocab_size=30522, d_model=512)\n",
    "\n",
    "# -----------------------\n",
    "# 1Ô∏è‚É£ Optimizer Se√ßimi\n",
    "# -----------------------\n",
    "# AdamW: LLM'lerde standart\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-5,          # learning rate\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Alternatif: Adafactor (hafƒ±za dostu)\n",
    "# optimizer = Adafactor(\n",
    "#     model.parameters(),\n",
    "#     lr=None,\n",
    "#     relative_step=True,\n",
    "#     scale_parameter=True,\n",
    "#     warmup_init=True\n",
    "# )\n",
    "\n",
    "# -----------------------\n",
    "# 2Ô∏è‚É£ Loss Fonksiyonu\n",
    "# -----------------------\n",
    "# Padding tokenlerini ignore eden standart CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# -----------------------\n",
    "# 3Ô∏è‚É£ Scheduler (Opsiyonel)\n",
    "# -----------------------\n",
    "num_training_steps = 10000\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 4Ô∏è‚É£ Train Step (Compile-like)\n",
    "# -----------------------\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Model forward\n",
    "    outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # logits: [batch, seq_len, vocab_size], target: [batch, seq_len]\n",
    "    loss = criterion(\n",
    "        outputs.logits.view(-1, outputs.logits.size(-1)),\n",
    "        target_ids.view(-1)\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()  # opsiyonel\n",
    "    return loss.item()\n",
    "\n",
    "# -----------------------\n",
    "# 5Ô∏è‚É£ Metric √ñrnek\n",
    "# -----------------------\n",
    "def perplexity(loss):\n",
    "    return torch.exp(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b66e5c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
