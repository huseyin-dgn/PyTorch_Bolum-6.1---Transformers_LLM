{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b760b5eb",
   "metadata": {},
   "source": [
    "# ğŸ”¹ Transformers Tokenization\n",
    "\n",
    "## 1ï¸âƒ£ AmaÃ§\n",
    "- Modelin doÄŸal dil girdisini **sayÄ±sal tensorâ€™a Ã§evirmesi** gerekiyor.  \n",
    "- Her token bir **vocab index** ile temsil edilir.  \n",
    "- Subword tokenization â†’ nadir kelimeler parÃ§alanÄ±r, bilinmeyen tokenâ€™lar azaltÄ±lÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Tokenizer TÃ¼rleri\n",
    "\n",
    "| Tokenizer | AÃ§Ä±klama |\n",
    "|-----------|----------|\n",
    "| **Word-level** | Her kelime ayrÄ± token, nadir kelimeler OOV (out-of-vocab) olur |\n",
    "| **Subword (BPE, WordPiece, SentencePiece)** | Kelimeler alt parÃ§alara bÃ¶lÃ¼nÃ¼r, bilinmeyen kelime problemi azalÄ±r |\n",
    "| **Character-level** | Her harf bir token, sequence uzunluÄŸu Ã§ok artar |\n",
    "\n",
    "> Modern Transformers Ã§oÄŸunlukla **BPE veya WordPiece** kullanÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Ã–zel Tokenlar\n",
    "- `[BOS]` â†’ sequence baÅŸlangÄ±cÄ±  \n",
    "- `[EOS]` â†’ sequence sonu  \n",
    "- `[PAD]` â†’ padding  \n",
    "- `[MASK]` â†’ masked language modeling (MLM) iÃ§in\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Tokenization Ã–rnekleri\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, Transformers!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "# ['hello', ',', 'transformers', '!']\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "# [7592, 1010, 19081, 999]\n",
    "\n",
    "# Special tokens ekleme\n",
    "token_ids = tokenizer.build_inputs_with_special_tokens(token_ids)\n",
    "print(\"Token IDs with special tokens:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18ed86",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Encoder-Decoder HazÄ±rlÄ±ÄŸÄ±\n",
    "\n",
    "* Encoder Input: token IDs + padding\n",
    "\n",
    "* Decoder Input: shifted right + [BOS] token\n",
    "\n",
    "* Target Output: shifted left + [EOS] token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f69981",
   "metadata": {},
   "source": [
    "```python \n",
    "input_ids = tokenizer.encode(\"Hello, Transformers!\", add_special_tokens=True)\n",
    "target_ids = tokenizer.encode(\"Hi there!\", add_special_tokens=True)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Target IDs:\", target_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f0c14",
   "metadata": {},
   "source": [
    "ğŸ’¡ Ã–zet MantÄ±k\n",
    "\n",
    "Tokenizer â†’ kelimeleri veya subwordâ€™leri tokenâ€™a Ã§evirir\n",
    "\n",
    "Token â†’ vocab ID\n",
    "\n",
    "Special tokenâ€™lar eklenir â†’ [BOS], [EOS], [PAD]\n",
    "\n",
    "Encoder ve decoder sequenceâ€™leri hazÄ±rlanÄ±r â†’ padding ve shift uygulanÄ±r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6612634",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47e833",
   "metadata": {},
   "source": [
    "# ğŸ”¹ Tokenization Seviyeleri\n",
    "\n",
    "## 1ï¸âƒ£ Word-level Tokenization\n",
    "\n",
    "### MantÄ±k\n",
    "- Her kelime bir token olarak ele alÄ±nÄ±r.  \n",
    "- BoÅŸluk ve noktalama iÅŸaretleri token ayÄ±rÄ±cÄ± olarak kullanÄ±lÄ±r.  \n",
    "\n",
    "### AvantajlarÄ±\n",
    "- Basit ve anlaÅŸÄ±lÄ±r  \n",
    "- KÃ¼Ã§Ã¼k vocab boyutu  \n",
    "\n",
    "### DezavantajlarÄ±\n",
    "- Nadir kelimeler â†’ **OOV (Out-of-Vocab)** problemi  \n",
    "- Morphologically zengin dillerde kÃ¶tÃ¼ performans  \n",
    "\n",
    "### Ã–rnek\n",
    "```python\n",
    "text = \"Transformers are amazing!\"\n",
    "tokens = text.split()\n",
    "# ['Transformers', 'are', 'amazing!']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c37c2",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Subword-level Tokenization (BPE / WordPiece / SentencePiece)\n",
    "\n",
    "### MantÄ±k\n",
    "- Kelimeler alt birimlere (subword) ayrÄ±lÄ±r.  \n",
    "- Nadir kelimeler parÃ§alanarak modelin anlamasÄ± kolaylaÅŸÄ±r.\n",
    "\n",
    "### AvantajlarÄ±\n",
    "- OOV problemi azalÄ±r  \n",
    "- Vocab boyutu orta seviyede  \n",
    "- Modern Transformers Ã§oÄŸunlukla bunu kullanÄ±r\n",
    "\n",
    "### Ã–rnek (BPE / WordPiece)\n",
    "```python\n",
    "text = \"Transformers\"\n",
    "tokens = [\"Trans\", \"##form\", \"##ers\"]  # WordPiece tarzÄ±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13383b",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Character-level Tokenization\n",
    "\n",
    "### MantÄ±k\n",
    "- Her harf veya karakter bir token olarak ele alÄ±nÄ±r.  \n",
    "- Tokenization Ã§ok basit â†’ vocab = tÃ¼m karakterler\n",
    "\n",
    "### AvantajlarÄ±\n",
    "- OOV problemi neredeyse yok  \n",
    "- Dil baÄŸÄ±msÄ±z\n",
    "\n",
    "### DezavantajlarÄ±\n",
    "- Sequence uzunluÄŸu Ã§ok artar â†’ eÄŸitim yavaÅŸ  \n",
    "- Context Ã¶ÄŸrenimi zor\n",
    "\n",
    "### Ã–rnek\n",
    "```python\n",
    "text = \"Hi!\"\n",
    "tokens = ['H', 'i', '!']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c374e32",
   "metadata": {},
   "source": [
    "# ğŸ”¹ Ã–zet KarÅŸÄ±laÅŸtÄ±rma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59524b",
   "metadata": {},
   "source": [
    "| Level           | Avantaj                         | Dezavantaj                   |\n",
    "| --------------- | ------------------------------- | ---------------------------- |\n",
    "| Word-level      | Basit, kÄ±sa sequence            | OOV problemi, dÃ¼ÅŸÃ¼k esneklik |\n",
    "| Subword-level   | OOV azalÄ±r, modern NLP standart | Tokenization biraz karmaÅŸÄ±k  |\n",
    "| Character-level | OOV yok, dil baÄŸÄ±msÄ±z           | Uzun sequence, eÄŸitim yavaÅŸ  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab12112",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Not:\n",
    "\n",
    "Modern Transformers (BERT, GPT, T5 vb.) subword tokenization kullanÄ±r.\n",
    "\n",
    "Seq2Seq LSTM modellerinde bazen word-level veya character-level tercih edilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4673a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
