{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b760b5eb",
   "metadata": {},
   "source": [
    "# 🔹 Transformers Tokenization\n",
    "\n",
    "## 1️⃣ Amaç\n",
    "- Modelin doğal dil girdisini **sayısal tensor’a çevirmesi** gerekiyor.  \n",
    "- Her token bir **vocab index** ile temsil edilir.  \n",
    "- Subword tokenization → nadir kelimeler parçalanır, bilinmeyen token’lar azaltılır.\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Tokenizer Türleri\n",
    "\n",
    "| Tokenizer | Açıklama |\n",
    "|-----------|----------|\n",
    "| **Word-level** | Her kelime ayrı token, nadir kelimeler OOV (out-of-vocab) olur |\n",
    "| **Subword (BPE, WordPiece, SentencePiece)** | Kelimeler alt parçalara bölünür, bilinmeyen kelime problemi azalır |\n",
    "| **Character-level** | Her harf bir token, sequence uzunluğu çok artar |\n",
    "\n",
    "> Modern Transformers çoğunlukla **BPE veya WordPiece** kullanır.\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Özel Tokenlar\n",
    "- `[BOS]` → sequence başlangıcı  \n",
    "- `[EOS]` → sequence sonu  \n",
    "- `[PAD]` → padding  \n",
    "- `[MASK]` → masked language modeling (MLM) için\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Tokenization Örnekleri\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, Transformers!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "# ['hello', ',', 'transformers', '!']\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "# [7592, 1010, 19081, 999]\n",
    "\n",
    "# Special tokens ekleme\n",
    "token_ids = tokenizer.build_inputs_with_special_tokens(token_ids)\n",
    "print(\"Token IDs with special tokens:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18ed86",
   "metadata": {},
   "source": [
    "## 5️⃣ Encoder-Decoder Hazırlığı\n",
    "\n",
    "* Encoder Input: token IDs + padding\n",
    "\n",
    "* Decoder Input: shifted right + [BOS] token\n",
    "\n",
    "* Target Output: shifted left + [EOS] token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f69981",
   "metadata": {},
   "source": [
    "```python \n",
    "input_ids = tokenizer.encode(\"Hello, Transformers!\", add_special_tokens=True)\n",
    "target_ids = tokenizer.encode(\"Hi there!\", add_special_tokens=True)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Target IDs:\", target_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f0c14",
   "metadata": {},
   "source": [
    "💡 Özet Mantık\n",
    "\n",
    "Tokenizer → kelimeleri veya subword’leri token’a çevirir\n",
    "\n",
    "Token → vocab ID\n",
    "\n",
    "Special token’lar eklenir → [BOS], [EOS], [PAD]\n",
    "\n",
    "Encoder ve decoder sequence’leri hazırlanır → padding ve shift uygulanır"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6612634",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47e833",
   "metadata": {},
   "source": [
    "# 🔹 Tokenization Seviyeleri\n",
    "\n",
    "## 1️⃣ Word-level Tokenization\n",
    "\n",
    "### Mantık\n",
    "- Her kelime bir token olarak ele alınır.  \n",
    "- Boşluk ve noktalama işaretleri token ayırıcı olarak kullanılır.  \n",
    "\n",
    "### Avantajları\n",
    "- Basit ve anlaşılır  \n",
    "- Küçük vocab boyutu  \n",
    "\n",
    "### Dezavantajları\n",
    "- Nadir kelimeler → **OOV (Out-of-Vocab)** problemi  \n",
    "- Morphologically zengin dillerde kötü performans  \n",
    "\n",
    "### Örnek\n",
    "```python\n",
    "text = \"Transformers are amazing!\"\n",
    "tokens = text.split()\n",
    "# ['Transformers', 'are', 'amazing!']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c37c2",
   "metadata": {},
   "source": [
    "## 2️⃣ Subword-level Tokenization (BPE / WordPiece / SentencePiece)\n",
    "\n",
    "### Mantık\n",
    "- Kelimeler alt birimlere (subword) ayrılır.  \n",
    "- Nadir kelimeler parçalanarak modelin anlaması kolaylaşır.\n",
    "\n",
    "### Avantajları\n",
    "- OOV problemi azalır  \n",
    "- Vocab boyutu orta seviyede  \n",
    "- Modern Transformers çoğunlukla bunu kullanır\n",
    "\n",
    "### Örnek (BPE / WordPiece)\n",
    "```python\n",
    "text = \"Transformers\"\n",
    "tokens = [\"Trans\", \"##form\", \"##ers\"]  # WordPiece tarzı\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13383b",
   "metadata": {},
   "source": [
    "## 3️⃣ Character-level Tokenization\n",
    "\n",
    "### Mantık\n",
    "- Her harf veya karakter bir token olarak ele alınır.  \n",
    "- Tokenization çok basit → vocab = tüm karakterler\n",
    "\n",
    "### Avantajları\n",
    "- OOV problemi neredeyse yok  \n",
    "- Dil bağımsız\n",
    "\n",
    "### Dezavantajları\n",
    "- Sequence uzunluğu çok artar → eğitim yavaş  \n",
    "- Context öğrenimi zor\n",
    "\n",
    "### Örnek\n",
    "```python\n",
    "text = \"Hi!\"\n",
    "tokens = ['H', 'i', '!']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c374e32",
   "metadata": {},
   "source": [
    "# 🔹 Özet Karşılaştırma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59524b",
   "metadata": {},
   "source": [
    "| Level           | Avantaj                         | Dezavantaj                   |\n",
    "| --------------- | ------------------------------- | ---------------------------- |\n",
    "| Word-level      | Basit, kısa sequence            | OOV problemi, düşük esneklik |\n",
    "| Subword-level   | OOV azalır, modern NLP standart | Tokenization biraz karmaşık  |\n",
    "| Character-level | OOV yok, dil bağımsız           | Uzun sequence, eğitim yavaş  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab12112",
   "metadata": {},
   "source": [
    "## 💡 Not:\n",
    "\n",
    "Modern Transformers (BERT, GPT, T5 vb.) subword tokenization kullanır.\n",
    "\n",
    "Seq2Seq LSTM modellerinde bazen word-level veya character-level tercih edilebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4673a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
