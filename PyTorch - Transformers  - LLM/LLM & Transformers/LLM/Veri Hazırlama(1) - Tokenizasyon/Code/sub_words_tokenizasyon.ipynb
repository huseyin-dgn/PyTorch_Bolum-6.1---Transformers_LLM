{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f738b1bc",
   "metadata": {},
   "source": [
    "# üîπ Subword-level Tokenization (BPE / WordPiece / SentencePiece)\n",
    "\n",
    "## 1Ô∏è‚É£ Mantƒ±k\n",
    "- Kelimeler alt birimlere (subword) ayrƒ±lƒ±r.  \n",
    "- Nadir kelimeler par√ßalanarak modelin anlamasƒ± kolayla≈üƒ±r.  \n",
    "- √ñrneƒüin ‚ÄúTransformers‚Äù ‚Üí [\"Trans\", \"##form\", \"##ers\"] (WordPiece tarzƒ±).  \n",
    "- B√∂ylece OOV (out-of-vocab) problemi azalƒ±r.\n",
    "\n",
    "\n",
    "## 2Ô∏è‚É£ Avantajlarƒ±\n",
    "- OOV problemi b√ºy√ºk √∂l√ß√ºde azalƒ±r.  \n",
    "- Vocab boyutu orta seviyede kalƒ±r.  \n",
    "- Modern Transformers (BERT, GPT, T5) √ßoƒüunlukla subword kullanƒ±r.  \n",
    "- Morfolojik a√ßƒ±dan zengin dillerde daha esnek.\n",
    "\n",
    "\n",
    "\n",
    "## 3Ô∏è‚É£ Dezavantajlarƒ±\n",
    "- Tokenization biraz karma≈üƒ±ktƒ±r, preprocessing adƒ±mlarƒ± gerekir.  \n",
    "- Token ID‚Äôlerini geri kelimelere √ßevirmek bazen zor olabilir.  \n",
    "\n",
    "\n",
    "\n",
    "## 4Ô∏è‚É£ √ñrnek (Python)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Transformers are amazing!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "# ['transform', '##ers', 'are', 'amazing', '!']\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "# [19081, 2741, 2024, 6429, 999]\n",
    "\n",
    "# Special tokens ekleme\n",
    "token_ids_with_special = tokenizer.build_inputs_with_special_tokens(token_ids)\n",
    "print(\"Token IDs with special tokens:\", token_ids_with_special)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba23b88",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Encoder-Decoder Hazƒ±rlƒ±ƒüƒ±\n",
    "\n",
    "- **Encoder Input:** token IDs + padding  \n",
    "- **Decoder Input:** shifted right + `[BOS]` token  \n",
    "- **Target Output:** shifted left + `[EOS]` token  \n",
    "\n",
    "\n",
    "üí° **√ñzet Mantƒ±k**\n",
    "\n",
    "1. Kelimeler veya subword‚Äôler token‚Äôlara ayrƒ±lƒ±r  \n",
    "2. Her token vocab ID‚Äôye √ßevrilir  \n",
    "3. Special token‚Äôlar eklenir ‚Üí `[BOS]`, `[EOS]`, `[PAD]`  \n",
    "4. Encoder ve decoder sequence‚Äôleri hazƒ±rlanƒ±r ‚Üí padding ve shift uygulanƒ±r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282e8c6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609770ef",
   "metadata": {},
   "source": [
    "# Encoder-decoder hazƒ±rlƒ±ƒüƒ±nƒ± adƒ±m adƒ±m kod ile g√∂stereceƒüiz. Her adƒ±m ayrƒ± ayrƒ± olacak ≈üekilde ilerleyeceƒüiz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0591f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ========================================\n",
    "# 1Ô∏è‚É£ Tokenizer Y√ºkleme\n",
    "# ========================================\n",
    "# BERT tokenizer kullanƒ±yoruz (subword)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ========================================\n",
    "# 2Ô∏è‚É£ √ñrnek Metinler\n",
    "# ========================================\n",
    "input_text = \"Hello, Transformers!\"\n",
    "target_text = \"Hi there!\"\n",
    "\n",
    "# ========================================\n",
    "# 3Ô∏è‚É£ Subword Tokenization\n",
    "# ========================================\n",
    "# Encoder ve decoder i√ßin token IDs\n",
    "encoder_tokens = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "decoder_tokens = tokenizer.encode(target_text, add_special_tokens=True)\n",
    "\n",
    "print(\"Encoder Tokens:\", encoder_tokens)\n",
    "print(\"Decoder Tokens:\", decoder_tokens)\n",
    "\n",
    "# ========================================\n",
    "# 4Ô∏è‚É£ Padding\n",
    "# ========================================\n",
    "# Max sequence length belirleme\n",
    "max_len = max(len(encoder_tokens), len(decoder_tokens))\n",
    "\n",
    "# Padding token ekleme\n",
    "encoder_tokens += [tokenizer.pad_token_id] * (max_len - len(encoder_tokens))\n",
    "decoder_tokens += [tokenizer.pad_token_id] * (max_len - len(decoder_tokens))\n",
    "\n",
    "print(\"Padded Encoder Tokens:\", encoder_tokens)\n",
    "print(\"Padded Decoder Tokens:\", decoder_tokens)\n",
    "\n",
    "# ========================================\n",
    "# 5Ô∏è‚É£ Tensor'a √áevirme\n",
    "# ========================================\n",
    "encoder_input = torch.tensor([encoder_tokens])  # (1, seq_len)\n",
    "decoder_target = torch.tensor([decoder_tokens])  # (1, seq_len)\n",
    "\n",
    "print(\"Encoder Input Tensor:\", encoder_input)\n",
    "print(\"Decoder Target Tensor:\", decoder_target)\n",
    "\n",
    "# ========================================\n",
    "# 6Ô∏è‚É£ Decoder Input ‚Üí Shifted Right + [BOS]\n",
    "# ========================================\n",
    "# [CLS] token = [BOS]\n",
    "decoder_input = torch.cat(\n",
    "    [torch.tensor([[tokenizer.cls_token_id]]), decoder_target[:, :-1]], dim=1\n",
    ")\n",
    "\n",
    "print(\"Decoder Input Tensor:\", decoder_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235cb75",
   "metadata": {},
   "source": [
    "#### üîπ A√ßƒ±klamalar Adƒ±m Adƒ±m\n",
    "\n",
    "* Tokenizer Y√ºkleme: BERT tokenizer ile subword tokenization yapƒ±yoruz.\n",
    "\n",
    "* √ñrnek Metinler: Encoder ve decoder i√ßin input metinler.\n",
    "\n",
    "* Tokenization: encode() ‚Üí kelimeler/subword‚Äôler token IDs‚Äôe d√∂n√º≈üt√ºr√ºl√ºr.\n",
    "\n",
    "* Padding: Sequence uzunluklarƒ± e≈üitlenir, [PAD] token ile doldurulur.\n",
    "\n",
    "* Tensor: Model input formatƒ± ‚Üí (batch_size, seq_len)\n",
    "\n",
    "* Decoder Input: Shifted right + [BOS] token (ba≈ülangƒ±√ß token‚Äôƒ±).\n",
    "\n",
    "* Decoder Target: Asƒ±l output, [EOS] token i√ßerir ve sequence aynƒ± uzunlukta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00952001",
   "metadata": {},
   "source": [
    "## tokenization + padding + shift + mask i≈ülemlerini birle≈ütirip adƒ±m adƒ±m Transformer‚Äôa hazƒ±r h√¢le getirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40e39b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hdgn5\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: tensor([[  101,  7592,  1010, 19081,   999,   102]])\n",
      "Decoder Input: tensor([[ 101,  101, 7632, 2045,  999,  102]])\n",
      "Encoder Mask: tensor([[1, 1, 1, 1, 1, 1]])\n",
      "Decoder Mask: tensor([[1, 1, 1, 1, 1, 1]])\n",
      "Causal Mask: tensor([[[1., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ========================================\n",
    "# 1Ô∏è‚É£ Tokenizer Y√ºkleme\n",
    "# ========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ========================================\n",
    "# 2Ô∏è‚É£ √ñrnek Metinler\n",
    "# ========================================\n",
    "input_text = \"Hello, Transformers!\"\n",
    "target_text = \"Hi there!\"\n",
    "\n",
    "# ========================================\n",
    "# 3Ô∏è‚É£ Tokenization (Subword)\n",
    "# ========================================\n",
    "encoder_tokens = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "decoder_tokens = tokenizer.encode(target_text, add_special_tokens=True)\n",
    "\n",
    "# ========================================\n",
    "# 4Ô∏è‚É£ Padding\n",
    "# ========================================\n",
    "max_len = max(len(encoder_tokens), len(decoder_tokens))\n",
    "encoder_tokens += [tokenizer.pad_token_id] * (max_len - len(encoder_tokens))\n",
    "decoder_tokens += [tokenizer.pad_token_id] * (max_len - len(decoder_tokens))\n",
    "\n",
    "# ========================================\n",
    "# 5Ô∏è‚É£ Tensor‚Äôa √áevirme\n",
    "# ========================================\n",
    "encoder_input = torch.tensor([encoder_tokens])\n",
    "decoder_target = torch.tensor([decoder_tokens])\n",
    "\n",
    "# ========================================\n",
    "# 6Ô∏è‚É£ Decoder Input ‚Üí Shifted Right + [BOS]\n",
    "# ========================================\n",
    "decoder_input = torch.cat(\n",
    "    [torch.tensor([[tokenizer.cls_token_id]]), decoder_target[:, :-1]], dim=1\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# 7Ô∏è‚É£ Padding Mask\n",
    "# ========================================\n",
    "# Encoder padding mask\n",
    "encoder_mask = (encoder_input != tokenizer.pad_token_id).long()  # 1 = token var, 0 = PAD\n",
    "\n",
    "# Decoder padding mask\n",
    "decoder_mask = (decoder_input != tokenizer.pad_token_id).long()\n",
    "\n",
    "# ========================================\n",
    "# 8Ô∏è‚É£ Causal / Look-Ahead Mask\n",
    "# ========================================\n",
    "seq_len = decoder_input.size(1)\n",
    "causal_mask = torch.tril(torch.ones((seq_len, seq_len))).unsqueeze(0)  # (1, seq_len, seq_len)\n",
    "\n",
    "print(\"Encoder Input:\", encoder_input)\n",
    "print(\"Decoder Input:\", decoder_input)\n",
    "print(\"Encoder Mask:\", encoder_mask)\n",
    "print(\"Decoder Mask:\", decoder_mask)\n",
    "print(\"Causal Mask:\", causal_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c08e24",
   "metadata": {},
   "source": [
    "## üîπ A√ßƒ±klamalar\n",
    "\n",
    "* Tokenizer: Subword tokenization ile token IDs olu≈üturuyoruz.\n",
    "\n",
    "* Padding: Encoder ve decoder sequence‚Äôleri aynƒ± uzunlukta.\n",
    "\n",
    "* Shifted Decoder Input: [BOS] token eklenir, target saƒüa kaydƒ±rƒ±lƒ±r.\n",
    "\n",
    "* Padding Mask: Model padding token‚Äôlarƒ±nƒ± dikkate almasƒ±n diye mask olu≈üturulur.\n",
    "\n",
    "* Causal Mask: Decoder‚Äôƒ±n geleceƒüe bakmasƒ±nƒ± engeller ‚Üí autoregressive g√∂revler i√ßin gerekli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606ab66",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5af202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import csv\n",
    "\n",
    "src_texts = []\n",
    "trg_texts = []\n",
    "\n",
    "with open(r\"C:\\Users\\hdgn5\\OneDrive\\Masa√ºst√º\\PyTorch - SEQ2SEQ\\SEQ2SEQ (3)\\turkish_english_dataset_large.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            src, trg = row\n",
    "            src_texts.append(src.strip())\n",
    "            trg_texts.append(trg.strip())\n",
    "\n",
    "print(f\"Toplam {len(src_texts)} √∂rnek y√ºklendi.\")\n",
    "\n",
    "class Sƒ±mpleTokenizer:\n",
    "    def __init__(self , mode=\"word\"):\n",
    "        self.mode = mode\n",
    "        self.vocab = {\"<PAD>\":0 , \"<SOS>\":1 , \"<EOS>\":2 , \"<UNK>\":3}\n",
    "        self.reverse_vocab = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.idx = 4\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        return text.lower().split() if self.mode==\"word\"  else list(text.lower())\n",
    "    \n",
    "    def build_vocab(self,texts):\n",
    "        for text in texts:\n",
    "            for t in self.tokenize(text):\n",
    "                if t not in self.vocab:\n",
    "                    self.vocab[t] = self.idx\n",
    "                    self.reverse_vocab[self.idx]= t        \n",
    "                    self.idx +=1\n",
    "    \n",
    "    def encode(self,text,add_sos_eos_unk = True):\n",
    "        indices = [self.vocab.get(t,self.vocab[\"<UNK>\"]) for t in self.tokenize(text)]\n",
    "        if add_sos_eos_unk:\n",
    "            indices = [self.vocab[\"<SOS>\"]] + indices + [self.vocab[\"<EOS>\"]]\n",
    "        return indices\n",
    "\n",
    "    def decode(self,indices):\n",
    "        return \" \".join([self.reverse_vocab.get(i,\"<UNK>\") for i in indices if i not in (0,1,2)])\n",
    "    \n",
    "class SimpleAugmenter:\n",
    "    def __init__(self, synonym_dict=None, insert_tokens=None, seed=42):\n",
    "        random.seed(seed)\n",
    "        self.synonym_dict = synonym_dict or {}\n",
    "        self.insert_tokens = insert_tokens or [\"ve\", \"ile\", \"da\", \"please\", \"today\"]\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def _detokenize(self, tokens):\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def synonym_replace(self, text, p=0.1):\n",
    "        tokens = self._tokenize(text)\n",
    "        new_tokens = [\n",
    "            random.choice(self.synonym_dict[t])\n",
    "            if t in self.synonym_dict and random.random() < p\n",
    "            else t\n",
    "            for t in tokens\n",
    "        ]\n",
    "        return self._detokenize(new_tokens)\n",
    "\n",
    "    def random_deletion(self, text, p=0.05):\n",
    "        tokens = self._tokenize(text)\n",
    "        new_tokens = [t for t in tokens if random.random() > p]\n",
    "        return self._detokenize(new_tokens) if new_tokens else random.choice(tokens)\n",
    "\n",
    "    def apply_policy(self, source, target, policy=None):\n",
    "        policy = policy or {}\n",
    "        s, t = source, target\n",
    "        for fn in policy.get(\"source_only\", []):\n",
    "            s = fn(s)\n",
    "        for fn in policy.get(\"target_only\", []):\n",
    "            t = fn(t)\n",
    "        for fn in policy.get(\"both\", []):\n",
    "            s = fn(s)\n",
    "            t = fn(t)\n",
    "        return s, t\n",
    "    \n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self,sources,targets,source_tokenizer,target_tokenizer , augmenter=None , policy =None):\n",
    "        super().__init__()\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        self.augmenter = augmenter\n",
    "        self.policy = policy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s,t = self.sources[idx] , self.targets[idx]\n",
    "        if self.augmenter:\n",
    "            s ,t = self.augmenter.apply_policy(s,t,self.policy)\n",
    "        \n",
    "        s_encoded = torch.tensor(self.source_tokenizer.encode(s, add_sos_eos_unk=True) , dtype=torch.long)\n",
    "        t_encoded = torch.tensor(self.target_tokenizer.encode(t , add_sos_eos_unk = True) , dtype=torch.long)\n",
    "        return s_encoded , t_encoded\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    s_seqs , t_seqs = zip(*batch)\n",
    "    s_padded = pad_sequence(s_seqs , batch_first=True,padding_value=0)\n",
    "    t_padded = pad_sequence(t_seqs,batch_first=True,padding_value=0)\n",
    "    return s_padded , t_padded\n",
    "\n",
    "train_src, val_src, train_tgt, val_tgt = train_test_split(\n",
    "  src_texts, trg_texts, test_size=0.3, random_state=42)\n",
    "\n",
    "source_tokenizer = Sƒ±mpleTokenizer(mode=\"word\")\n",
    "target_tokenizer = Sƒ±mpleTokenizer(mode=\"word\")\n",
    "\n",
    "source_tokenizer.build_vocab(train_src)\n",
    "target_tokenizer.build_vocab(train_tgt)\n",
    "\n",
    "train_dataset = Seq2SeqDataset(train_src,train_tgt , source_tokenizer , target_tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_src,val_tgt,source_tokenizer,target_tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset , batch_size=32 , shuffle=False,collate_fn=collate_fn)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32 , shuffle=True ,collate_fn=collate_fn)\n",
    "\n",
    "for s_batch, t_batch in train_loader:\n",
    "    print(\"Source batch shape:\", s_batch.shape)\n",
    "    print(\"Target batch shape:\", t_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5a793",
   "metadata": {},
   "source": [
    "## Biz normal Seq2Seq modellerde yukarƒ±da olan tokenizasyon i≈ülemlerini kullanƒ±yorduk.≈ûimdi ise LLM tabanlƒ± yazacaƒüoz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035d12a",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ √ñzet Kar≈üƒ±la≈ütƒ±rma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3668144",
   "metadata": {},
   "source": [
    "| Feature        | Senin Seq2Seq Tokenizer      | Transformer-Ready Pipeline               |\n",
    "| -------------- | ---------------------------- | ---------------------------------------- |\n",
    "| Tokenization   | Word-level                   | Subword / BPE / WordPiece                |\n",
    "| Special Tokens | `<PAD>, <SOS>, <EOS>, <UNK>` | `[PAD], [BOS]/[CLS], [EOS]/[SEP], <UNK>` |\n",
    "| Masking        | Padding mask                 | Padding + Look-ahead / Causal mask       |\n",
    "| Batch          | Var                          | Batch-first + Mask hazƒ±r                 |\n",
    "| LLM uyumu      | Kƒ±sƒ±tlƒ±                      | Tam hazƒ±r                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c0c40",
   "metadata": {},
   "source": [
    "---\n",
    "## ≈ûimdi ise bu yapƒ±da ama LLM ve transformers'a uygun bir tokenizasyon i≈ülemi yapacaƒüƒ±z\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da0a34",
   "metadata": {},
   "source": [
    "# üîπ Transformers-Ready Tokenization Pipeline\n",
    "\n",
    "Bu pipeline yalnƒ±zca tokenizasyon ve batch-ready hazƒ±rlƒ±k i√ßin:\n",
    "\n",
    "1. Subword tokenization (WordPiece / BPE)\n",
    "2. Padding ve batch\n",
    "3. Special tokens\n",
    "4. Causal mask opsiyonel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0603768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch: tensor([[ 1,  4,  5,  6,  7,  8,  9,  7,  2,  0,  0,  0],\n",
      "        [ 1, 10, 11, 12, 13, 14, 15, 16, 17,  4, 18,  2]])\n",
      "Target batch: tensor([[ 1,  4,  5,  6,  7,  8,  9,  2,  0,  0,  0,  0],\n",
      "        [ 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  2]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# ----------------------------------\n",
    "# 1Ô∏è‚É£ Simple Subword Tokenizer (WordPiece tarzƒ±)\n",
    "# ----------------------------------\n",
    "class SubwordTokenizer:\n",
    "    def __init__(self, vocab=None):\n",
    "        # Special tokens\n",
    "        self.vocab = vocab or {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        self.reverse_vocab = {v:k for k,v in self.vocab.items()}\n",
    "        self.idx = len(self.vocab)\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            for token in self._tokenize(text):\n",
    "                if token not in self.vocab:\n",
    "                    self.vocab[token] = self.idx\n",
    "                    self.reverse_vocab[self.idx] = token\n",
    "                    self.idx += 1\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # √áok basit WordPiece tarzƒ±: kelimeleri 2-3 harfli subword par√ßalarƒ±na ayƒ±r\n",
    "        tokens = []\n",
    "        for word in text.lower().split():\n",
    "            if len(word) <= 2:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                # split into 2-char subwords\n",
    "                for i in range(0, len(word), 2):\n",
    "                    tokens.append(word[i:i+2])\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text, add_sos_eos=True):\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in self._tokenize(text)]\n",
    "        if add_sos_eos:\n",
    "            ids = [self.vocab[\"<SOS>\"]] + ids + [self.vocab[\"<EOS>\"]]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \"\".join([self.reverse_vocab.get(i,\"<UNK>\") for i in ids if i not in (0,1,2)])\n",
    "\n",
    "# ----------------------------------\n",
    "# 2Ô∏è‚É£ Dataset ve DataLoader\n",
    "# ----------------------------------\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, sources, targets, src_tokenizer, tgt_tokenizer):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.sources[idx]\n",
    "        t = self.targets[idx]\n",
    "        s_ids = torch.tensor(self.src_tokenizer.encode(s), dtype=torch.long)\n",
    "        t_ids = torch.tensor(self.tgt_tokenizer.encode(t), dtype=torch.long)\n",
    "        return s_ids, t_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    s_seqs, t_seqs = zip(*batch)\n",
    "    s_padded = pad_sequence(s_seqs, batch_first=True, padding_value=0)\n",
    "    t_padded = pad_sequence(t_seqs, batch_first=True, padding_value=0)\n",
    "    return s_padded, t_padded\n",
    "\n",
    "# ----------------------------------\n",
    "# 3Ô∏è‚É£ √ñrnek kullanƒ±m\n",
    "# ----------------------------------\n",
    "src_texts = [\"Merhaba d√ºnya\", \"PyTorch Transformers\"]\n",
    "tgt_texts = [\"Hello world\", \"PyTorch Transformers\"]\n",
    "\n",
    "src_tokenizer = SubwordTokenizer()\n",
    "tgt_tokenizer = SubwordTokenizer()\n",
    "\n",
    "src_tokenizer.build_vocab(src_texts)\n",
    "tgt_tokenizer.build_vocab(tgt_texts)\n",
    "\n",
    "dataset = Seq2SeqDataset(src_texts, tgt_texts, src_tokenizer, tgt_tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for s_batch, t_batch in loader:\n",
    "    print(\"Source batch:\", s_batch)\n",
    "    print(\"Target batch:\", t_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fd0dc",
   "metadata": {},
   "source": [
    "---\n",
    "# ≈ûimdi ise bu i≈ülem adƒ±mlarƒ±nƒ± alƒ±p HuggingFace tarzƒ± subword tokenization ve Transformers-ready padding / masks ile g√ºncelleyelim. Adƒ±m adƒ±m yapacaƒüƒ±z, b√∂ylece:\n",
    "\n",
    "* Subword tokenization (BPE / WordPiece tarzƒ±) eklenir.\n",
    "\n",
    "* Encoder ve decoder inputlarƒ± hazƒ±rlanƒ±r (shifted, BOS/EOS).\n",
    "\n",
    "* Padding mask ve causal mask √ºretilir.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import csv\n",
    "\n",
    "# ========================================\n",
    "# 1Ô∏è‚É£ Tokenizer Y√ºkleme (Subword / Pretrained)\n",
    "# ========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # veya \"t5-small\", \"gpt2\"\n",
    "\n",
    "# ========================================\n",
    "# 2Ô∏è‚É£ Dataset Y√ºkleme\n",
    "# ========================================\n",
    "src_texts, trg_texts = [], []\n",
    "\n",
    "with open(r\"C:\\Users\\hdgn5\\OneDrive\\Masa√ºst√º\\PyTorch - SEQ2SEQ\\SEQ2SEQ (3)\\turkish_english_dataset_large.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            src, trg = row\n",
    "            src_texts.append(src.strip())\n",
    "            trg_texts.append(trg.strip())\n",
    "\n",
    "print(f\"Toplam {len(src_texts)} √∂rnek y√ºklendi.\")\n",
    "\n",
    "# ========================================\n",
    "# 3Ô∏è‚É£ Dataset ve Tokenization\n",
    "# ========================================\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, sources, targets, tokenizer, max_len=128):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.sources[idx]\n",
    "        trg = self.targets[idx]\n",
    "\n",
    "        # Encoder tokenization\n",
    "        enc = tokenizer.encode_plus(\n",
    "            src, add_special_tokens=True,\n",
    "            max_length=self.max_len, padding='max_length', truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Decoder tokenization\n",
    "        dec = tokenizer.encode_plus(\n",
    "            trg, add_special_tokens=True,\n",
    "            max_length=self.max_len, padding='max_length', truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Decoder input: shifted right (BOS)\n",
    "        decoder_input_ids = torch.cat([torch.tensor([[tokenizer.cls_token_id]]), dec['input_ids'][:, :-1]], dim=1)\n",
    "\n",
    "        # Padding mask\n",
    "        encoder_mask = enc['attention_mask']\n",
    "        decoder_mask = dec['attention_mask']\n",
    "\n",
    "        # Causal mask (look-ahead)\n",
    "        seq_len = decoder_input_ids.size(1)\n",
    "        causal_mask = torch.tril(torch.ones((seq_len, seq_len))).unsqueeze(0)  # (1, seq_len, seq_len)\n",
    "\n",
    "        return {\n",
    "            'encoder_input': enc['input_ids'].squeeze(0),\n",
    "            'decoder_input': decoder_input_ids.squeeze(0),\n",
    "            'decoder_target': dec['input_ids'].squeeze(0),\n",
    "            'encoder_mask': encoder_mask.squeeze(0),\n",
    "            'decoder_mask': decoder_mask.squeeze(0),\n",
    "            'causal_mask': causal_mask.squeeze(0)\n",
    "        }\n",
    "\n",
    "# ========================================\n",
    "# 4Ô∏è‚É£ DataLoader\n",
    "# ========================================\n",
    "dataset = TransformerDataset(src_texts, trg_texts, tokenizer, max_len=32)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Test batch\n",
    "for batch in loader:\n",
    "    print(\"Encoder input shape:\", batch['encoder_input'].shape)\n",
    "    print(\"Decoder input shape:\", batch['decoder_input'].shape)\n",
    "    print(\"Decoder target shape:\", batch['decoder_target'].shape)\n",
    "    print(\"Encoder mask shape:\", batch['encoder_mask'].shape)\n",
    "    print(\"Decoder mask shape:\", batch['decoder_mask'].shape)\n",
    "    print(\"Causal mask shape:\", batch['causal_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cab933",
   "metadata": {},
   "source": [
    "### ‚úÖ Bu pipeline artƒ±k:\n",
    "\n",
    "* HuggingFace subword tokenizer kullanƒ±yor (LLM hazƒ±r).\n",
    "\n",
    "* Encoder / decoder input ve target hazƒ±rlanmƒ±≈ü.\n",
    "\n",
    "* Padding mask ve causal mask otomatik.\n",
    "\n",
    "* Transformers ve autoregressive LLM‚Äôlerde direkt kullanƒ±labilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c02a8",
   "metadata": {},
   "source": [
    "## ≈ûimdi ise : bir sonraki adƒ±mda bu pipeline‚Äôƒ± train-ready h√¢le getireceƒüiz ve batch‚Äôleri model-ready tensorlar olarak hazƒ±rlayacaƒüƒ±z.\n",
    "\n",
    "* Bunu yaparken ≈üunlarƒ± ekleyeceƒüiz:\n",
    "\n",
    "* Padding ve truncation zaten tokenizasyonda ayarlƒ±, batch‚Äôler e≈üit uzunlukta olacak.\n",
    "\n",
    "* Encoder mask ve decoder mask batch olarak d√∂necek.\n",
    "\n",
    "* Causal / look-ahead mask batch i√ßin otomatik √ºretilecek.\n",
    "\n",
    "* Batch dictionary formatƒ±nda d√∂necek, b√∂ylece doƒürudan forward‚Äôa verilebilecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aaf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    encoder_inputs = torch.stack([item['encoder_input'] for item in batch])\n",
    "    decoder_inputs = torch.stack([item['decoder_input'] for item in batch])\n",
    "    decoder_targets = torch.stack([item['decoder_target'] for item in batch])\n",
    "    encoder_masks = torch.stack([item['encoder_mask'] for item in batch])\n",
    "    decoder_masks = torch.stack([item['decoder_mask'] for item in batch])\n",
    "    causal_masks = torch.stack([item['causal_mask'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'encoder_input': encoder_inputs,\n",
    "        'decoder_input': decoder_inputs,\n",
    "        'decoder_target': decoder_targets,\n",
    "        'encoder_mask': encoder_masks,\n",
    "        'decoder_mask': decoder_masks,\n",
    "        'causal_mask': causal_masks\n",
    "    }\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test batch\n",
    "for batch in train_loader:\n",
    "    print(\"Encoder input shape:\", batch['encoder_input'].shape)\n",
    "    print(\"Decoder input shape:\", batch['decoder_input'].shape)\n",
    "    print(\"Decoder target shape:\", batch['decoder_target'].shape)\n",
    "    print(\"Encoder mask shape:\", batch['encoder_mask'].shape)\n",
    "    print(\"Decoder mask shape:\", batch['decoder_mask'].shape)\n",
    "    print(\"Causal mask shape:\", batch['causal_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4cbce7",
   "metadata": {},
   "source": [
    "---\n",
    "# A≈üaƒüƒ±da yazƒ±lan kodlar LLM ve Transformer‚Äôlar i√ßin hazƒ±rlanmƒ±≈ü b√ºt√ºn bir Veri hazƒ±rlama adƒ±mƒ±nƒ±n kodlarƒ±dƒ±r.Yukarƒ±da anlatƒ±lanlarƒ±n birle≈ütirilmi≈ü hali a≈üaƒüƒ±dadƒ±r:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d798f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: tensor([[  101, 21442, 25459,  2050,  1010, 17235, 11722,  4877, 11722,  2078,\n",
      "         11722,  2480,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 19081, 17869, 19204, 21335,  6508,  2239, 13958,  7389, 28008,\n",
      "         20527,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "Encoder Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Decoder Input IDs: tensor([[  101,   101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,   101,  1045,  2572,  4083, 19204,  3989,  2007, 19081,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "Decoder Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Decoder Target IDs: tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2572,  4083, 19204,  3989,  2007, 19081,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ========================================\n",
    "# 1Ô∏è‚É£ Tokenizer Y√ºkleme (Subword)\n",
    "# ========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # veya ba≈üka LLM\n",
    "\n",
    "# ========================================\n",
    "# 2Ô∏è‚É£ √ñrnek Dataset\n",
    "# ========================================\n",
    "input_texts = [\n",
    "    \"Merhaba, nasƒ±lsƒ±nƒ±z?\",\n",
    "    \"Transformers ile tokenizasyon √∂ƒüreniyorum.\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning tokenization with Transformers.\"\n",
    "]\n",
    "\n",
    "# ========================================\n",
    "# 3Ô∏è‚É£ Dataset Sƒ±nƒ±fƒ± (LLM-ready Pipeline)\n",
    "# ========================================\n",
    "class LLMSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, sources, targets, tokenizer, max_length=32):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.sources[idx]\n",
    "        tgt_text = self.targets[idx]\n",
    "\n",
    "        # ========================\n",
    "        # Encoder: tokenize, pad, truncate\n",
    "        # ========================\n",
    "        enc = self.tokenizer(\n",
    "            src_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # ========================\n",
    "        # Decoder: tokenize, pad, truncate\n",
    "        # ========================\n",
    "        dec = self.tokenizer(\n",
    "            tgt_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # ========================\n",
    "        # Decoder input: shifted right + [BOS]/CLS token\n",
    "        # ========================\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [torch.full((1,1), self.tokenizer.cls_token_id), dec['input_ids'][:,:-1]], dim=1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': enc['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'decoder_target_ids': dec['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# ========================================\n",
    "# 4Ô∏è‚É£ Dataset ve DataLoader\n",
    "# ========================================\n",
    "dataset = LLMSeq2SeqDataset(input_texts, target_texts, tokenizer, max_length=32)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# ========================================\n",
    "# 5Ô∏è‚É£ Test\n",
    "# ========================================\n",
    "for batch in dataloader:\n",
    "    print(\"Encoder Input IDs:\", batch['encoder_input_ids'])\n",
    "    print(\"Encoder Attention Mask:\", batch['encoder_attention_mask'])\n",
    "    print(\"Decoder Input IDs:\", batch['decoder_input_ids'])\n",
    "    print(\"Decoder Attention Mask:\", batch['decoder_attention_mask'])\n",
    "    print(\"Decoder Target IDs:\", batch['decoder_target_ids'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105e160",
   "metadata": {},
   "source": [
    " ========================================\n",
    "# üîπ √ñzet Pipeline Adƒ±mlarƒ±\n",
    " ========================================\n",
    "### 1. Subword Tokenization (BPE/WordPiece/SentencePiece)\n",
    "### 2. Encoder/Decoder sequence olu≈üturma\n",
    "### 3. Padding + Truncation\n",
    "### 4. Attention Mask olu≈üturma\n",
    "### 5. Decoder input shifted right (+ BOS/CLS)\n",
    "### 6. Tensor d√∂n√º≈ü√ºm√º (return_tensors='pt')\n",
    "### 7. Batch ile DataLoader √ºzerinden eƒüitim-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0678a",
   "metadata": {},
   "source": [
    "## üîπ Bu pipeline‚Äôƒ±n avantajlarƒ±\n",
    "\n",
    "* LLM-ready: Transformer veya GPT tarzƒ± modellerle direkt kullanƒ±labilir.\n",
    "\n",
    "* Subword tokenization: OOV problemini azaltƒ±r ve vocab boyutu dengelidir.\n",
    "\n",
    "* Automatic attention mask: padding token‚Äôlarƒ± dikkate alƒ±nmaz.\n",
    "\n",
    "* Decoder shift: Autoregressive / seq2seq g√∂revleri i√ßin uygun.\n",
    "\n",
    "* Batch ve tensor ready: GPU √ºzerinde direkt training‚Äôe uygun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca904f",
   "metadata": {},
   "source": [
    "----\n",
    "# i≈üte LLM ve Transformer‚Äôlar i√ßin tam veri hazƒ±rlama pipeline‚Äôƒ±. Subword tokenization, padding, attention mask, decoder shift, tensor d√∂n√º≈ü√ºm√º ve batch handling hepsi bir arada.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac92857",
   "metadata": {},
   "source": [
    "### üîπ Memory-Efficient Batch Tokenization Pipeline\n",
    "\n",
    "- **Lazy Tokenization:** Dataset‚Äôi liste halinde tut, ancak tokenizasyon her batch √ßaƒürƒ±ldƒ±ƒüƒ±nda yapƒ±lƒ±r.  \n",
    "- **Tokenizer Kullanƒ±mƒ±:** `batch_encode_plus` veya `__call__` fonksiyonunu kullan.  \n",
    "  - Bu sayede tek seferde t√ºm batch‚Äôleri tokenize edebilir, padding ve truncation uygulayabilirsin.  \n",
    "- **Attention Mask:** Padding token‚Äôlarƒ±nƒ± otomatik olarak dikkate alƒ±r.  \n",
    "- **Decoder Input (Shifted Right):**  \n",
    "  ```python\n",
    "  decoder_input_ids = torch.cat([CLS/BOS, decoder_target[:, :-1]], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc89482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: torch.Size([2, 16])\n",
      "Encoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Input IDs: torch.Size([2, 16])\n",
      "Decoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Target IDs: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# =====================================\n",
    "# 1Ô∏è‚É£ Full LLM / Transformer Tokenization Dataset\n",
    "# =====================================\n",
    "class LLMTokenizedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Full tokenization pipeline for LLMs / Transformers.\n",
    "    Includes encoder/decoder tokenization, attention masks, and shifted decoder input.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources, targets, tokenizer_name=\"bert-base-uncased\", max_length=64):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # -------------------\n",
    "        # Encoder tokenization\n",
    "        # -------------------\n",
    "        enc = self.tokenizer(\n",
    "            self.sources[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder tokenization\n",
    "        # -------------------\n",
    "        dec = self.tokenizer(\n",
    "            self.targets[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder input (shifted right + BOS/CLS)\n",
    "        # -------------------\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [torch.full((1,1), self.tokenizer.cls_token_id), dec['input_ids'][:, :-1]], dim=1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': enc['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'decoder_target_ids': dec['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# =====================================\n",
    "# 2Ô∏è‚É£ Collate function for batching\n",
    "# =====================================\n",
    "def collate_fn(batch):\n",
    "    # Keys: encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask, decoder_target_ids\n",
    "    enc_ids = torch.stack([item['encoder_input_ids'] for item in batch])\n",
    "    enc_mask = torch.stack([item['encoder_attention_mask'] for item in batch])\n",
    "    dec_ids = torch.stack([item['decoder_input_ids'] for item in batch])\n",
    "    dec_mask = torch.stack([item['decoder_attention_mask'] for item in batch])\n",
    "    dec_target = torch.stack([item['decoder_target_ids'] for item in batch])\n",
    "    return {\n",
    "        'encoder_input_ids': enc_ids,\n",
    "        'encoder_attention_mask': enc_mask,\n",
    "        'decoder_input_ids': dec_ids,\n",
    "        'decoder_attention_mask': dec_mask,\n",
    "        'decoder_target_ids': dec_target\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# 3Ô∏è‚É£ Example Usage\n",
    "# =====================================\n",
    "input_texts = [\n",
    "    \"Merhaba d√ºnya!\",\n",
    "    \"Transformers √ßok g√º√ßl√º.\",\n",
    "    \"Memory-efficient pipeline.\"\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are powerful.\",\n",
    "    \"Super useful pipeline.\"\n",
    "]\n",
    "\n",
    "dataset = LLMTokenizedDataset(input_texts, target_texts, tokenizer_name=\"bert-base-uncased\", max_length=16)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# 4Ô∏è‚É£ Test one batch\n",
    "# =====================================\n",
    "for batch in dataloader:\n",
    "    print(\"Encoder Input IDs:\", batch['encoder_input_ids'].shape)\n",
    "    print(\"Encoder Attention Mask:\", batch['encoder_attention_mask'].shape)\n",
    "    print(\"Decoder Input IDs:\", batch['decoder_input_ids'].shape)\n",
    "    print(\"Decoder Attention Mask:\", batch['decoder_attention_mask'].shape)\n",
    "    print(\"Decoder Target IDs:\", batch['decoder_target_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ad2a7",
   "metadata": {},
   "source": [
    "# üöÄ Model Giri≈üi ƒ∞√ßin Hazƒ±rlƒ±k Durumu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfc46a",
   "metadata": {},
   "source": [
    "| Tensor                   | A√ßƒ±klama                               | Kullanƒ±m                       |\n",
    "| :----------------------- | :------------------------------------- | :----------------------------- |\n",
    "| `encoder_input_ids`      | Token ID‚Äôleri (subword seviyesinde)    | Encoder‚Äôa girer                |\n",
    "| `encoder_attention_mask` | PAD olmayan yerler = 1                 | Encoder maskesi                |\n",
    "| `decoder_input_ids`      | Shifted right + `[CLS]` (veya `[BOS]`) | Decoder giri≈üi                 |\n",
    "| `decoder_attention_mask` | PAD olmayan yerler = 1                 | Decoder maskesi                |\n",
    "| `decoder_target_ids`     | Ger√ßek hedef diziler                   | Loss hesaplamasƒ±nda kullanƒ±lƒ±r |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b09bd",
   "metadata": {},
   "source": [
    "## üîß √ñrnek: Transformer Modeline Baƒülama\n",
    "\n",
    "* A≈üaƒüƒ±daki √∂rnek, HuggingFace‚Äôteki T5 / BART gibi encoder‚Äìdecoder yapƒ±larƒ±nda bu pipeline‚Äôƒ±n doƒürudan nasƒ±l kullanƒ±ldƒ±ƒüƒ±nƒ± g√∂steriyor üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b46709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hdgn5\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.565789222717285\n",
      "Logits shape: torch.Size([2, 16, 32128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# Modeli y√ºkle (√∂rnek: T5 small)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Bir batch al\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Model giri≈üine ver\n",
    "outputs = model(\n",
    "    input_ids=batch[\"encoder_input_ids\"],\n",
    "    attention_mask=batch[\"encoder_attention_mask\"],\n",
    "    decoder_input_ids=batch[\"decoder_input_ids\"],\n",
    "    labels=batch[\"decoder_target_ids\"]\n",
    ")\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Logits shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d406e4c",
   "metadata": {},
   "source": [
    "## üí° Notlar\n",
    "\n",
    "* Bu yapƒ± LLM‚Äôlerde, Seq2Seq Transformer‚Äôlarda (T5, BART, MarianMT vb.) doƒürudan √ßalƒ±≈üƒ±r.\n",
    "\n",
    "* Eƒüer GPT gibi sadece decoder tabanlƒ± bir model kullanacaksan (encoder_input_ids yok), pipeline‚Äôƒ± biraz sadele≈ütirip tek taraflƒ± hale getiririz (yani input = prompt, label = shifted target).\n",
    "\n",
    "* Tokenizer olarak da \"bert-base-uncased\", \"t5-small\", \"gpt2\", \"openai-community/gpt2-medium\" gibi istediƒüini se√ßebilirsin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65559f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8ccd5d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ad920d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
