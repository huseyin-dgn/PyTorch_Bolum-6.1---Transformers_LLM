{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f738b1bc",
   "metadata": {},
   "source": [
    "# 🔹 Subword-level Tokenization (BPE / WordPiece / SentencePiece)\n",
    "\n",
    "## 1️⃣ Mantık\n",
    "- Kelimeler alt birimlere (subword) ayrılır.  \n",
    "- Nadir kelimeler parçalanarak modelin anlaması kolaylaşır.  \n",
    "- Örneğin “Transformers” → [\"Trans\", \"##form\", \"##ers\"] (WordPiece tarzı).  \n",
    "- Böylece OOV (out-of-vocab) problemi azalır.\n",
    "\n",
    "\n",
    "## 2️⃣ Avantajları\n",
    "- OOV problemi büyük ölçüde azalır.  \n",
    "- Vocab boyutu orta seviyede kalır.  \n",
    "- Modern Transformers (BERT, GPT, T5) çoğunlukla subword kullanır.  \n",
    "- Morfolojik açıdan zengin dillerde daha esnek.\n",
    "\n",
    "\n",
    "\n",
    "## 3️⃣ Dezavantajları\n",
    "- Tokenization biraz karmaşıktır, preprocessing adımları gerekir.  \n",
    "- Token ID’lerini geri kelimelere çevirmek bazen zor olabilir.  \n",
    "\n",
    "\n",
    "\n",
    "## 4️⃣ Örnek (Python)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Transformers are amazing!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "# ['transform', '##ers', 'are', 'amazing', '!']\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "# [19081, 2741, 2024, 6429, 999]\n",
    "\n",
    "# Special tokens ekleme\n",
    "token_ids_with_special = tokenizer.build_inputs_with_special_tokens(token_ids)\n",
    "print(\"Token IDs with special tokens:\", token_ids_with_special)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba23b88",
   "metadata": {},
   "source": [
    "## 5️⃣ Encoder-Decoder Hazırlığı\n",
    "\n",
    "- **Encoder Input:** token IDs + padding  \n",
    "- **Decoder Input:** shifted right + `[BOS]` token  \n",
    "- **Target Output:** shifted left + `[EOS]` token  \n",
    "\n",
    "\n",
    "💡 **Özet Mantık**\n",
    "\n",
    "1. Kelimeler veya subword’ler token’lara ayrılır  \n",
    "2. Her token vocab ID’ye çevrilir  \n",
    "3. Special token’lar eklenir → `[BOS]`, `[EOS]`, `[PAD]`  \n",
    "4. Encoder ve decoder sequence’leri hazırlanır → padding ve shift uygulanır\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282e8c6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609770ef",
   "metadata": {},
   "source": [
    "# Encoder-decoder hazırlığını adım adım kod ile göstereceğiz. Her adım ayrı ayrı olacak şekilde ilerleyeceğiz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0591f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ========================================\n",
    "# 1️⃣ Tokenizer Yükleme\n",
    "# ========================================\n",
    "# BERT tokenizer kullanıyoruz (subword)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ========================================\n",
    "# 2️⃣ Örnek Metinler\n",
    "# ========================================\n",
    "input_text = \"Hello, Transformers!\"\n",
    "target_text = \"Hi there!\"\n",
    "\n",
    "# ========================================\n",
    "# 3️⃣ Subword Tokenization\n",
    "# ========================================\n",
    "# Encoder ve decoder için token IDs\n",
    "encoder_tokens = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "decoder_tokens = tokenizer.encode(target_text, add_special_tokens=True)\n",
    "\n",
    "print(\"Encoder Tokens:\", encoder_tokens)\n",
    "print(\"Decoder Tokens:\", decoder_tokens)\n",
    "\n",
    "# ========================================\n",
    "# 4️⃣ Padding\n",
    "# ========================================\n",
    "# Max sequence length belirleme\n",
    "max_len = max(len(encoder_tokens), len(decoder_tokens))\n",
    "\n",
    "# Padding token ekleme\n",
    "encoder_tokens += [tokenizer.pad_token_id] * (max_len - len(encoder_tokens))\n",
    "decoder_tokens += [tokenizer.pad_token_id] * (max_len - len(decoder_tokens))\n",
    "\n",
    "print(\"Padded Encoder Tokens:\", encoder_tokens)\n",
    "print(\"Padded Decoder Tokens:\", decoder_tokens)\n",
    "\n",
    "# ========================================\n",
    "# 5️⃣ Tensor'a Çevirme\n",
    "# ========================================\n",
    "encoder_input = torch.tensor([encoder_tokens])  # (1, seq_len)\n",
    "decoder_target = torch.tensor([decoder_tokens])  # (1, seq_len)\n",
    "\n",
    "print(\"Encoder Input Tensor:\", encoder_input)\n",
    "print(\"Decoder Target Tensor:\", decoder_target)\n",
    "\n",
    "# ========================================\n",
    "# 6️⃣ Decoder Input → Shifted Right + [BOS]\n",
    "# ========================================\n",
    "# [CLS] token = [BOS]\n",
    "decoder_input = torch.cat(\n",
    "    [torch.tensor([[tokenizer.cls_token_id]]), decoder_target[:, :-1]], dim=1\n",
    ")\n",
    "\n",
    "print(\"Decoder Input Tensor:\", decoder_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235cb75",
   "metadata": {},
   "source": [
    "#### 🔹 Açıklamalar Adım Adım\n",
    "\n",
    "* Tokenizer Yükleme: BERT tokenizer ile subword tokenization yapıyoruz.\n",
    "\n",
    "* Örnek Metinler: Encoder ve decoder için input metinler.\n",
    "\n",
    "* Tokenization: encode() → kelimeler/subword’ler token IDs’e dönüştürülür.\n",
    "\n",
    "* Padding: Sequence uzunlukları eşitlenir, [PAD] token ile doldurulur.\n",
    "\n",
    "* Tensor: Model input formatı → (batch_size, seq_len)\n",
    "\n",
    "* Decoder Input: Shifted right + [BOS] token (başlangıç token’ı).\n",
    "\n",
    "* Decoder Target: Asıl output, [EOS] token içerir ve sequence aynı uzunlukta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00952001",
   "metadata": {},
   "source": [
    "## tokenization + padding + shift + mask işlemlerini birleştirip adım adım Transformer’a hazır hâle getirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40e39b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hdgn5\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: tensor([[  101,  7592,  1010, 19081,   999,   102]])\n",
      "Decoder Input: tensor([[ 101,  101, 7632, 2045,  999,  102]])\n",
      "Encoder Mask: tensor([[1, 1, 1, 1, 1, 1]])\n",
      "Decoder Mask: tensor([[1, 1, 1, 1, 1, 1]])\n",
      "Causal Mask: tensor([[[1., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ========================================\n",
    "# 1️⃣ Tokenizer Yükleme\n",
    "# ========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ========================================\n",
    "# 2️⃣ Örnek Metinler\n",
    "# ========================================\n",
    "input_text = \"Hello, Transformers!\"\n",
    "target_text = \"Hi there!\"\n",
    "\n",
    "# ========================================\n",
    "# 3️⃣ Tokenization (Subword)\n",
    "# ========================================\n",
    "encoder_tokens = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "decoder_tokens = tokenizer.encode(target_text, add_special_tokens=True)\n",
    "\n",
    "# ========================================\n",
    "# 4️⃣ Padding\n",
    "# ========================================\n",
    "max_len = max(len(encoder_tokens), len(decoder_tokens))\n",
    "encoder_tokens += [tokenizer.pad_token_id] * (max_len - len(encoder_tokens))\n",
    "decoder_tokens += [tokenizer.pad_token_id] * (max_len - len(decoder_tokens))\n",
    "\n",
    "# ========================================\n",
    "# 5️⃣ Tensor’a Çevirme\n",
    "# ========================================\n",
    "encoder_input = torch.tensor([encoder_tokens])\n",
    "decoder_target = torch.tensor([decoder_tokens])\n",
    "\n",
    "# ========================================\n",
    "# 6️⃣ Decoder Input → Shifted Right + [BOS]\n",
    "# ========================================\n",
    "decoder_input = torch.cat(\n",
    "    [torch.tensor([[tokenizer.cls_token_id]]), decoder_target[:, :-1]], dim=1\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# 7️⃣ Padding Mask\n",
    "# ========================================\n",
    "# Encoder padding mask\n",
    "encoder_mask = (encoder_input != tokenizer.pad_token_id).long()  # 1 = token var, 0 = PAD\n",
    "\n",
    "# Decoder padding mask\n",
    "decoder_mask = (decoder_input != tokenizer.pad_token_id).long()\n",
    "\n",
    "# ========================================\n",
    "# 8️⃣ Causal / Look-Ahead Mask\n",
    "# ========================================\n",
    "seq_len = decoder_input.size(1)\n",
    "causal_mask = torch.tril(torch.ones((seq_len, seq_len))).unsqueeze(0)  # (1, seq_len, seq_len)\n",
    "\n",
    "print(\"Encoder Input:\", encoder_input)\n",
    "print(\"Decoder Input:\", decoder_input)\n",
    "print(\"Encoder Mask:\", encoder_mask)\n",
    "print(\"Decoder Mask:\", decoder_mask)\n",
    "print(\"Causal Mask:\", causal_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c08e24",
   "metadata": {},
   "source": [
    "## 🔹 Açıklamalar\n",
    "\n",
    "* Tokenizer: Subword tokenization ile token IDs oluşturuyoruz.\n",
    "\n",
    "* Padding: Encoder ve decoder sequence’leri aynı uzunlukta.\n",
    "\n",
    "* Shifted Decoder Input: [BOS] token eklenir, target sağa kaydırılır.\n",
    "\n",
    "* Padding Mask: Model padding token’larını dikkate almasın diye mask oluşturulur.\n",
    "\n",
    "* Causal Mask: Decoder’ın geleceğe bakmasını engeller → autoregressive görevler için gerekli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606ab66",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5af202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import csv\n",
    "\n",
    "src_texts = []\n",
    "trg_texts = []\n",
    "\n",
    "with open(r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\PyTorch - SEQ2SEQ\\SEQ2SEQ (3)\\turkish_english_dataset_large.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            src, trg = row\n",
    "            src_texts.append(src.strip())\n",
    "            trg_texts.append(trg.strip())\n",
    "\n",
    "print(f\"Toplam {len(src_texts)} örnek yüklendi.\")\n",
    "\n",
    "class SımpleTokenizer:\n",
    "    def __init__(self , mode=\"word\"):\n",
    "        self.mode = mode\n",
    "        self.vocab = {\"<PAD>\":0 , \"<SOS>\":1 , \"<EOS>\":2 , \"<UNK>\":3}\n",
    "        self.reverse_vocab = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.idx = 4\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        return text.lower().split() if self.mode==\"word\"  else list(text.lower())\n",
    "    \n",
    "    def build_vocab(self,texts):\n",
    "        for text in texts:\n",
    "            for t in self.tokenize(text):\n",
    "                if t not in self.vocab:\n",
    "                    self.vocab[t] = self.idx\n",
    "                    self.reverse_vocab[self.idx]= t        \n",
    "                    self.idx +=1\n",
    "    \n",
    "    def encode(self,text,add_sos_eos_unk = True):\n",
    "        indices = [self.vocab.get(t,self.vocab[\"<UNK>\"]) for t in self.tokenize(text)]\n",
    "        if add_sos_eos_unk:\n",
    "            indices = [self.vocab[\"<SOS>\"]] + indices + [self.vocab[\"<EOS>\"]]\n",
    "        return indices\n",
    "\n",
    "    def decode(self,indices):\n",
    "        return \" \".join([self.reverse_vocab.get(i,\"<UNK>\") for i in indices if i not in (0,1,2)])\n",
    "    \n",
    "class SimpleAugmenter:\n",
    "    def __init__(self, synonym_dict=None, insert_tokens=None, seed=42):\n",
    "        random.seed(seed)\n",
    "        self.synonym_dict = synonym_dict or {}\n",
    "        self.insert_tokens = insert_tokens or [\"ve\", \"ile\", \"da\", \"please\", \"today\"]\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return text.split()\n",
    "\n",
    "    def _detokenize(self, tokens):\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def synonym_replace(self, text, p=0.1):\n",
    "        tokens = self._tokenize(text)\n",
    "        new_tokens = [\n",
    "            random.choice(self.synonym_dict[t])\n",
    "            if t in self.synonym_dict and random.random() < p\n",
    "            else t\n",
    "            for t in tokens\n",
    "        ]\n",
    "        return self._detokenize(new_tokens)\n",
    "\n",
    "    def random_deletion(self, text, p=0.05):\n",
    "        tokens = self._tokenize(text)\n",
    "        new_tokens = [t for t in tokens if random.random() > p]\n",
    "        return self._detokenize(new_tokens) if new_tokens else random.choice(tokens)\n",
    "\n",
    "    def apply_policy(self, source, target, policy=None):\n",
    "        policy = policy or {}\n",
    "        s, t = source, target\n",
    "        for fn in policy.get(\"source_only\", []):\n",
    "            s = fn(s)\n",
    "        for fn in policy.get(\"target_only\", []):\n",
    "            t = fn(t)\n",
    "        for fn in policy.get(\"both\", []):\n",
    "            s = fn(s)\n",
    "            t = fn(t)\n",
    "        return s, t\n",
    "    \n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self,sources,targets,source_tokenizer,target_tokenizer , augmenter=None , policy =None):\n",
    "        super().__init__()\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.source_tokenizer = source_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        self.augmenter = augmenter\n",
    "        self.policy = policy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s,t = self.sources[idx] , self.targets[idx]\n",
    "        if self.augmenter:\n",
    "            s ,t = self.augmenter.apply_policy(s,t,self.policy)\n",
    "        \n",
    "        s_encoded = torch.tensor(self.source_tokenizer.encode(s, add_sos_eos_unk=True) , dtype=torch.long)\n",
    "        t_encoded = torch.tensor(self.target_tokenizer.encode(t , add_sos_eos_unk = True) , dtype=torch.long)\n",
    "        return s_encoded , t_encoded\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    s_seqs , t_seqs = zip(*batch)\n",
    "    s_padded = pad_sequence(s_seqs , batch_first=True,padding_value=0)\n",
    "    t_padded = pad_sequence(t_seqs,batch_first=True,padding_value=0)\n",
    "    return s_padded , t_padded\n",
    "\n",
    "train_src, val_src, train_tgt, val_tgt = train_test_split(\n",
    "  src_texts, trg_texts, test_size=0.3, random_state=42)\n",
    "\n",
    "source_tokenizer = SımpleTokenizer(mode=\"word\")\n",
    "target_tokenizer = SımpleTokenizer(mode=\"word\")\n",
    "\n",
    "source_tokenizer.build_vocab(train_src)\n",
    "target_tokenizer.build_vocab(train_tgt)\n",
    "\n",
    "train_dataset = Seq2SeqDataset(train_src,train_tgt , source_tokenizer , target_tokenizer)\n",
    "val_dataset = Seq2SeqDataset(val_src,val_tgt,source_tokenizer,target_tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset , batch_size=32 , shuffle=False,collate_fn=collate_fn)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32 , shuffle=True ,collate_fn=collate_fn)\n",
    "\n",
    "for s_batch, t_batch in train_loader:\n",
    "    print(\"Source batch shape:\", s_batch.shape)\n",
    "    print(\"Target batch shape:\", t_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5a793",
   "metadata": {},
   "source": [
    "## Biz normal Seq2Seq modellerde yukarıda olan tokenizasyon işlemlerini kullanıyorduk.Şimdi ise LLM tabanlı yazacağoz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035d12a",
   "metadata": {},
   "source": [
    "# 4️⃣ Özet Karşılaştırma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3668144",
   "metadata": {},
   "source": [
    "| Feature        | Senin Seq2Seq Tokenizer      | Transformer-Ready Pipeline               |\n",
    "| -------------- | ---------------------------- | ---------------------------------------- |\n",
    "| Tokenization   | Word-level                   | Subword / BPE / WordPiece                |\n",
    "| Special Tokens | `<PAD>, <SOS>, <EOS>, <UNK>` | `[PAD], [BOS]/[CLS], [EOS]/[SEP], <UNK>` |\n",
    "| Masking        | Padding mask                 | Padding + Look-ahead / Causal mask       |\n",
    "| Batch          | Var                          | Batch-first + Mask hazır                 |\n",
    "| LLM uyumu      | Kısıtlı                      | Tam hazır                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c0c40",
   "metadata": {},
   "source": [
    "---\n",
    "## Şimdi ise bu yapıda ama LLM ve transformers'a uygun bir tokenizasyon işlemi yapacağız\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da0a34",
   "metadata": {},
   "source": [
    "# 🔹 Transformers-Ready Tokenization Pipeline\n",
    "\n",
    "Bu pipeline yalnızca tokenizasyon ve batch-ready hazırlık için:\n",
    "\n",
    "1. Subword tokenization (WordPiece / BPE)\n",
    "2. Padding ve batch\n",
    "3. Special tokens\n",
    "4. Causal mask opsiyonel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0603768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch: tensor([[ 1,  4,  5,  6,  7,  8,  9,  7,  2,  0,  0,  0],\n",
      "        [ 1, 10, 11, 12, 13, 14, 15, 16, 17,  4, 18,  2]])\n",
      "Target batch: tensor([[ 1,  4,  5,  6,  7,  8,  9,  2,  0,  0,  0,  0],\n",
      "        [ 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  2]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# ----------------------------------\n",
    "# 1️⃣ Simple Subword Tokenizer (WordPiece tarzı)\n",
    "# ----------------------------------\n",
    "class SubwordTokenizer:\n",
    "    def __init__(self, vocab=None):\n",
    "        # Special tokens\n",
    "        self.vocab = vocab or {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "        self.reverse_vocab = {v:k for k,v in self.vocab.items()}\n",
    "        self.idx = len(self.vocab)\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            for token in self._tokenize(text):\n",
    "                if token not in self.vocab:\n",
    "                    self.vocab[token] = self.idx\n",
    "                    self.reverse_vocab[self.idx] = token\n",
    "                    self.idx += 1\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # Çok basit WordPiece tarzı: kelimeleri 2-3 harfli subword parçalarına ayır\n",
    "        tokens = []\n",
    "        for word in text.lower().split():\n",
    "            if len(word) <= 2:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                # split into 2-char subwords\n",
    "                for i in range(0, len(word), 2):\n",
    "                    tokens.append(word[i:i+2])\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text, add_sos_eos=True):\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in self._tokenize(text)]\n",
    "        if add_sos_eos:\n",
    "            ids = [self.vocab[\"<SOS>\"]] + ids + [self.vocab[\"<EOS>\"]]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \"\".join([self.reverse_vocab.get(i,\"<UNK>\") for i in ids if i not in (0,1,2)])\n",
    "\n",
    "# ----------------------------------\n",
    "# 2️⃣ Dataset ve DataLoader\n",
    "# ----------------------------------\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, sources, targets, src_tokenizer, tgt_tokenizer):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.sources[idx]\n",
    "        t = self.targets[idx]\n",
    "        s_ids = torch.tensor(self.src_tokenizer.encode(s), dtype=torch.long)\n",
    "        t_ids = torch.tensor(self.tgt_tokenizer.encode(t), dtype=torch.long)\n",
    "        return s_ids, t_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    s_seqs, t_seqs = zip(*batch)\n",
    "    s_padded = pad_sequence(s_seqs, batch_first=True, padding_value=0)\n",
    "    t_padded = pad_sequence(t_seqs, batch_first=True, padding_value=0)\n",
    "    return s_padded, t_padded\n",
    "\n",
    "# ----------------------------------\n",
    "# 3️⃣ Örnek kullanım\n",
    "# ----------------------------------\n",
    "src_texts = [\"Merhaba dünya\", \"PyTorch Transformers\"]\n",
    "tgt_texts = [\"Hello world\", \"PyTorch Transformers\"]\n",
    "\n",
    "src_tokenizer = SubwordTokenizer()\n",
    "tgt_tokenizer = SubwordTokenizer()\n",
    "\n",
    "src_tokenizer.build_vocab(src_texts)\n",
    "tgt_tokenizer.build_vocab(tgt_texts)\n",
    "\n",
    "dataset = Seq2SeqDataset(src_texts, tgt_texts, src_tokenizer, tgt_tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for s_batch, t_batch in loader:\n",
    "    print(\"Source batch:\", s_batch)\n",
    "    print(\"Target batch:\", t_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fd0dc",
   "metadata": {},
   "source": [
    "---\n",
    "# Şimdi ise bu işlem adımlarını alıp HuggingFace tarzı subword tokenization ve Transformers-ready padding / masks ile güncelleyelim. Adım adım yapacağız, böylece:\n",
    "\n",
    "* Subword tokenization (BPE / WordPiece tarzı) eklenir.\n",
    "\n",
    "* Encoder ve decoder inputları hazırlanır (shifted, BOS/EOS).\n",
    "\n",
    "* Padding mask ve causal mask üretilir.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import csv\n",
    "\n",
    "# ========================================\n",
    "# 1️⃣ Tokenizer Yükleme (Subword / Pretrained)\n",
    "# ========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # veya \"t5-small\", \"gpt2\"\n",
    "\n",
    "# ========================================\n",
    "# 2️⃣ Dataset Yükleme\n",
    "# ========================================\n",
    "src_texts, trg_texts = [], []\n",
    "\n",
    "with open(r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\PyTorch - SEQ2SEQ\\SEQ2SEQ (3)\\turkish_english_dataset_large.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            src, trg = row\n",
    "            src_texts.append(src.strip())\n",
    "            trg_texts.append(trg.strip())\n",
    "\n",
    "print(f\"Toplam {len(src_texts)} örnek yüklendi.\")\n",
    "\n",
    "# ========================================\n",
    "# 3️⃣ Dataset ve Tokenization\n",
    "# ========================================\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, sources, targets, tokenizer, max_len=128):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.sources[idx]\n",
    "        trg = self.targets[idx]\n",
    "\n",
    "        # Encoder tokenization\n",
    "        enc = tokenizer.encode_plus(\n",
    "            src, add_special_tokens=True,\n",
    "            max_length=self.max_len, padding='max_length', truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Decoder tokenization\n",
    "        dec = tokenizer.encode_plus(\n",
    "            trg, add_special_tokens=True,\n",
    "            max_length=self.max_len, padding='max_length', truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Decoder input: shifted right (BOS)\n",
    "        decoder_input_ids = torch.cat([torch.tensor([[tokenizer.cls_token_id]]), dec['input_ids'][:, :-1]], dim=1)\n",
    "\n",
    "        # Padding mask\n",
    "        encoder_mask = enc['attention_mask']\n",
    "        decoder_mask = dec['attention_mask']\n",
    "\n",
    "        # Causal mask (look-ahead)\n",
    "        seq_len = decoder_input_ids.size(1)\n",
    "        causal_mask = torch.tril(torch.ones((seq_len, seq_len))).unsqueeze(0)  # (1, seq_len, seq_len)\n",
    "\n",
    "        return {\n",
    "            'encoder_input': enc['input_ids'].squeeze(0),\n",
    "            'decoder_input': decoder_input_ids.squeeze(0),\n",
    "            'decoder_target': dec['input_ids'].squeeze(0),\n",
    "            'encoder_mask': encoder_mask.squeeze(0),\n",
    "            'decoder_mask': decoder_mask.squeeze(0),\n",
    "            'causal_mask': causal_mask.squeeze(0)\n",
    "        }\n",
    "\n",
    "# ========================================\n",
    "# 4️⃣ DataLoader\n",
    "# ========================================\n",
    "dataset = TransformerDataset(src_texts, trg_texts, tokenizer, max_len=32)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Test batch\n",
    "for batch in loader:\n",
    "    print(\"Encoder input shape:\", batch['encoder_input'].shape)\n",
    "    print(\"Decoder input shape:\", batch['decoder_input'].shape)\n",
    "    print(\"Decoder target shape:\", batch['decoder_target'].shape)\n",
    "    print(\"Encoder mask shape:\", batch['encoder_mask'].shape)\n",
    "    print(\"Decoder mask shape:\", batch['decoder_mask'].shape)\n",
    "    print(\"Causal mask shape:\", batch['causal_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cab933",
   "metadata": {},
   "source": [
    "### ✅ Bu pipeline artık:\n",
    "\n",
    "* HuggingFace subword tokenizer kullanıyor (LLM hazır).\n",
    "\n",
    "* Encoder / decoder input ve target hazırlanmış.\n",
    "\n",
    "* Padding mask ve causal mask otomatik.\n",
    "\n",
    "* Transformers ve autoregressive LLM’lerde direkt kullanılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c02a8",
   "metadata": {},
   "source": [
    "## Şimdi ise : bir sonraki adımda bu pipeline’ı train-ready hâle getireceğiz ve batch’leri model-ready tensorlar olarak hazırlayacağız.\n",
    "\n",
    "* Bunu yaparken şunları ekleyeceğiz:\n",
    "\n",
    "* Padding ve truncation zaten tokenizasyonda ayarlı, batch’ler eşit uzunlukta olacak.\n",
    "\n",
    "* Encoder mask ve decoder mask batch olarak dönecek.\n",
    "\n",
    "* Causal / look-ahead mask batch için otomatik üretilecek.\n",
    "\n",
    "* Batch dictionary formatında dönecek, böylece doğrudan forward’a verilebilecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aaf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    encoder_inputs = torch.stack([item['encoder_input'] for item in batch])\n",
    "    decoder_inputs = torch.stack([item['decoder_input'] for item in batch])\n",
    "    decoder_targets = torch.stack([item['decoder_target'] for item in batch])\n",
    "    encoder_masks = torch.stack([item['encoder_mask'] for item in batch])\n",
    "    decoder_masks = torch.stack([item['decoder_mask'] for item in batch])\n",
    "    causal_masks = torch.stack([item['causal_mask'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'encoder_input': encoder_inputs,\n",
    "        'decoder_input': decoder_inputs,\n",
    "        'decoder_target': decoder_targets,\n",
    "        'encoder_mask': encoder_masks,\n",
    "        'decoder_mask': decoder_masks,\n",
    "        'causal_mask': causal_masks\n",
    "    }\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test batch\n",
    "for batch in train_loader:\n",
    "    print(\"Encoder input shape:\", batch['encoder_input'].shape)\n",
    "    print(\"Decoder input shape:\", batch['decoder_input'].shape)\n",
    "    print(\"Decoder target shape:\", batch['decoder_target'].shape)\n",
    "    print(\"Encoder mask shape:\", batch['encoder_mask'].shape)\n",
    "    print(\"Decoder mask shape:\", batch['decoder_mask'].shape)\n",
    "    print(\"Causal mask shape:\", batch['causal_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4cbce7",
   "metadata": {},
   "source": [
    "---\n",
    "# Aşağıda yazılan kodlar LLM ve Transformer’lar için hazırlanmış bütün bir Veri hazırlama adımının kodlarıdır.Yukarıda anlatılanların birleştirilmiş hali aşağıdadır:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d798f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: tensor([[  101, 21442, 25459,  2050,  1010, 17235, 11722,  4877, 11722,  2078,\n",
      "         11722,  2480,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 19081, 17869, 19204, 21335,  6508,  2239, 13958,  7389, 28008,\n",
      "         20527,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "Encoder Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Decoder Input IDs: tensor([[  101,   101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,   101,  1045,  2572,  4083, 19204,  3989,  2007, 19081,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "Decoder Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Decoder Target IDs: tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1045,  2572,  4083, 19204,  3989,  2007, 19081,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ========================================\n",
    "# 1️⃣ Tokenizer Yükleme (Subword)\n",
    "# ========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # veya başka LLM\n",
    "\n",
    "# ========================================\n",
    "# 2️⃣ Örnek Dataset\n",
    "# ========================================\n",
    "input_texts = [\n",
    "    \"Merhaba, nasılsınız?\",\n",
    "    \"Transformers ile tokenizasyon öğreniyorum.\"\n",
    "]\n",
    "target_texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning tokenization with Transformers.\"\n",
    "]\n",
    "\n",
    "# ========================================\n",
    "# 3️⃣ Dataset Sınıfı (LLM-ready Pipeline)\n",
    "# ========================================\n",
    "class LLMSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, sources, targets, tokenizer, max_length=32):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.sources[idx]\n",
    "        tgt_text = self.targets[idx]\n",
    "\n",
    "        # ========================\n",
    "        # Encoder: tokenize, pad, truncate\n",
    "        # ========================\n",
    "        enc = self.tokenizer(\n",
    "            src_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # ========================\n",
    "        # Decoder: tokenize, pad, truncate\n",
    "        # ========================\n",
    "        dec = self.tokenizer(\n",
    "            tgt_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # ========================\n",
    "        # Decoder input: shifted right + [BOS]/CLS token\n",
    "        # ========================\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [torch.full((1,1), self.tokenizer.cls_token_id), dec['input_ids'][:,:-1]], dim=1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': enc['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'decoder_target_ids': dec['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# ========================================\n",
    "# 4️⃣ Dataset ve DataLoader\n",
    "# ========================================\n",
    "dataset = LLMSeq2SeqDataset(input_texts, target_texts, tokenizer, max_length=32)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# ========================================\n",
    "# 5️⃣ Test\n",
    "# ========================================\n",
    "for batch in dataloader:\n",
    "    print(\"Encoder Input IDs:\", batch['encoder_input_ids'])\n",
    "    print(\"Encoder Attention Mask:\", batch['encoder_attention_mask'])\n",
    "    print(\"Decoder Input IDs:\", batch['decoder_input_ids'])\n",
    "    print(\"Decoder Attention Mask:\", batch['decoder_attention_mask'])\n",
    "    print(\"Decoder Target IDs:\", batch['decoder_target_ids'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105e160",
   "metadata": {},
   "source": [
    " ========================================\n",
    "# 🔹 Özet Pipeline Adımları\n",
    " ========================================\n",
    "### 1. Subword Tokenization (BPE/WordPiece/SentencePiece)\n",
    "### 2. Encoder/Decoder sequence oluşturma\n",
    "### 3. Padding + Truncation\n",
    "### 4. Attention Mask oluşturma\n",
    "### 5. Decoder input shifted right (+ BOS/CLS)\n",
    "### 6. Tensor dönüşümü (return_tensors='pt')\n",
    "### 7. Batch ile DataLoader üzerinden eğitim-ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0678a",
   "metadata": {},
   "source": [
    "## 🔹 Bu pipeline’ın avantajları\n",
    "\n",
    "* LLM-ready: Transformer veya GPT tarzı modellerle direkt kullanılabilir.\n",
    "\n",
    "* Subword tokenization: OOV problemini azaltır ve vocab boyutu dengelidir.\n",
    "\n",
    "* Automatic attention mask: padding token’ları dikkate alınmaz.\n",
    "\n",
    "* Decoder shift: Autoregressive / seq2seq görevleri için uygun.\n",
    "\n",
    "* Batch ve tensor ready: GPU üzerinde direkt training’e uygun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca904f",
   "metadata": {},
   "source": [
    "----\n",
    "# işte LLM ve Transformer’lar için tam veri hazırlama pipeline’ı. Subword tokenization, padding, attention mask, decoder shift, tensor dönüşümü ve batch handling hepsi bir arada.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac92857",
   "metadata": {},
   "source": [
    "### 🔹 Memory-Efficient Batch Tokenization Pipeline\n",
    "\n",
    "- **Lazy Tokenization:** Dataset’i liste halinde tut, ancak tokenizasyon her batch çağrıldığında yapılır.  \n",
    "- **Tokenizer Kullanımı:** `batch_encode_plus` veya `__call__` fonksiyonunu kullan.  \n",
    "  - Bu sayede tek seferde tüm batch’leri tokenize edebilir, padding ve truncation uygulayabilirsin.  \n",
    "- **Attention Mask:** Padding token’larını otomatik olarak dikkate alır.  \n",
    "- **Decoder Input (Shifted Right):**  \n",
    "  ```python\n",
    "  decoder_input_ids = torch.cat([CLS/BOS, decoder_target[:, :-1]], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc89482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: torch.Size([2, 16])\n",
      "Encoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Input IDs: torch.Size([2, 16])\n",
      "Decoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Target IDs: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# =====================================\n",
    "# 1️⃣ Full LLM / Transformer Tokenization Dataset\n",
    "# =====================================\n",
    "class LLMTokenizedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Full tokenization pipeline for LLMs / Transformers.\n",
    "    Includes encoder/decoder tokenization, attention masks, and shifted decoder input.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources, targets, tokenizer_name=\"bert-base-uncased\", max_length=64):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # -------------------\n",
    "        # Encoder tokenization\n",
    "        # -------------------\n",
    "        enc = self.tokenizer(\n",
    "            self.sources[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder tokenization\n",
    "        # -------------------\n",
    "        dec = self.tokenizer(\n",
    "            self.targets[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder input (shifted right + BOS/CLS)\n",
    "        # -------------------\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [torch.full((1,1), self.tokenizer.cls_token_id), dec['input_ids'][:, :-1]], dim=1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': enc['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'decoder_target_ids': dec['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# =====================================\n",
    "# 2️⃣ Collate function for batching\n",
    "# =====================================\n",
    "def collate_fn(batch):\n",
    "    # Keys: encoder_input_ids, encoder_attention_mask, decoder_input_ids, decoder_attention_mask, decoder_target_ids\n",
    "    enc_ids = torch.stack([item['encoder_input_ids'] for item in batch])\n",
    "    enc_mask = torch.stack([item['encoder_attention_mask'] for item in batch])\n",
    "    dec_ids = torch.stack([item['decoder_input_ids'] for item in batch])\n",
    "    dec_mask = torch.stack([item['decoder_attention_mask'] for item in batch])\n",
    "    dec_target = torch.stack([item['decoder_target_ids'] for item in batch])\n",
    "    return {\n",
    "        'encoder_input_ids': enc_ids,\n",
    "        'encoder_attention_mask': enc_mask,\n",
    "        'decoder_input_ids': dec_ids,\n",
    "        'decoder_attention_mask': dec_mask,\n",
    "        'decoder_target_ids': dec_target\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# 3️⃣ Example Usage\n",
    "# =====================================\n",
    "input_texts = [\n",
    "    \"Merhaba dünya!\",\n",
    "    \"Transformers çok güçlü.\",\n",
    "    \"Memory-efficient pipeline.\"\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are powerful.\",\n",
    "    \"Super useful pipeline.\"\n",
    "]\n",
    "\n",
    "dataset = LLMTokenizedDataset(input_texts, target_texts, tokenizer_name=\"bert-base-uncased\", max_length=16)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# 4️⃣ Test one batch\n",
    "# =====================================\n",
    "for batch in dataloader:\n",
    "    print(\"Encoder Input IDs:\", batch['encoder_input_ids'].shape)\n",
    "    print(\"Encoder Attention Mask:\", batch['encoder_attention_mask'].shape)\n",
    "    print(\"Decoder Input IDs:\", batch['decoder_input_ids'].shape)\n",
    "    print(\"Decoder Attention Mask:\", batch['decoder_attention_mask'].shape)\n",
    "    print(\"Decoder Target IDs:\", batch['decoder_target_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ad2a7",
   "metadata": {},
   "source": [
    "# 🚀 Model Girişi İçin Hazırlık Durumu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfc46a",
   "metadata": {},
   "source": [
    "| Tensor                   | Açıklama                               | Kullanım                       |\n",
    "| :----------------------- | :------------------------------------- | :----------------------------- |\n",
    "| `encoder_input_ids`      | Token ID’leri (subword seviyesinde)    | Encoder’a girer                |\n",
    "| `encoder_attention_mask` | PAD olmayan yerler = 1                 | Encoder maskesi                |\n",
    "| `decoder_input_ids`      | Shifted right + `[CLS]` (veya `[BOS]`) | Decoder girişi                 |\n",
    "| `decoder_attention_mask` | PAD olmayan yerler = 1                 | Decoder maskesi                |\n",
    "| `decoder_target_ids`     | Gerçek hedef diziler                   | Loss hesaplamasında kullanılır |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b09bd",
   "metadata": {},
   "source": [
    "## 🔧 Örnek: Transformer Modeline Bağlama\n",
    "\n",
    "* Aşağıdaki örnek, HuggingFace’teki T5 / BART gibi encoder–decoder yapılarında bu pipeline’ın doğrudan nasıl kullanıldığını gösteriyor 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b46709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdgn5\\.conda\\envs\\torch_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hdgn5\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.565789222717285\n",
      "Logits shape: torch.Size([2, 16, 32128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# Modeli yükle (örnek: T5 small)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Bir batch al\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Model girişine ver\n",
    "outputs = model(\n",
    "    input_ids=batch[\"encoder_input_ids\"],\n",
    "    attention_mask=batch[\"encoder_attention_mask\"],\n",
    "    decoder_input_ids=batch[\"decoder_input_ids\"],\n",
    "    labels=batch[\"decoder_target_ids\"]\n",
    ")\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Logits shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d406e4c",
   "metadata": {},
   "source": [
    "## 💡 Notlar\n",
    "\n",
    "* Bu yapı LLM’lerde, Seq2Seq Transformer’larda (T5, BART, MarianMT vb.) doğrudan çalışır.\n",
    "\n",
    "* Eğer GPT gibi sadece decoder tabanlı bir model kullanacaksan (encoder_input_ids yok), pipeline’ı biraz sadeleştirip tek taraflı hale getiririz (yani input = prompt, label = shifted target).\n",
    "\n",
    "* Tokenizer olarak da \"bert-base-uncased\", \"t5-small\", \"gpt2\", \"openai-community/gpt2-medium\" gibi istediğini seçebilirsin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65559f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8ccd5d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ad920d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
