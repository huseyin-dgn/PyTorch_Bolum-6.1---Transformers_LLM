{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34cb869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37197e4",
   "metadata": {},
   "source": [
    "# TOKENÄ°ZASYON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812302dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: torch.Size([2, 16])\n",
      "Encoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Input IDs: torch.Size([2, 16])\n",
      "Decoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Target IDs: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 1ï¸âƒ£ Seq2Seq Tokenization Dataset (T5/BART uyumlu)\n",
    "# =====================================\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Tokenization pipeline for seq2seq LLMs (T5, BART, mBART)\n",
    "    Includes encoder/decoder tokenization, attention masks, shifted decoder input.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources, targets, tokenizer_name=\"t5-small\", max_length=64):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Seq2Seq modellerinde decoder_start_token_id Ã¶nemli\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # zorunlu pad token\n",
    "        self.decoder_start_token_id = self.tokenizer.pad_token_id if self.tokenizer.bos_token_id is None else self.tokenizer.bos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # -------------------\n",
    "        # Encoder tokenization\n",
    "        # -------------------\n",
    "        enc = self.tokenizer(\n",
    "            self.sources[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder tokenization\n",
    "        # -------------------\n",
    "        dec = self.tokenizer(\n",
    "            self.targets[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder input (shifted right)\n",
    "        # -------------------\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [torch.full((1,1), self.decoder_start_token_id), dec['input_ids'][:, :-1]], dim=1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': enc['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'decoder_target_ids': dec['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# =====================================\n",
    "# 2ï¸âƒ£ Collate function\n",
    "# =====================================\n",
    "def collate_fn(batch):\n",
    "    enc_ids = torch.stack([item['encoder_input_ids'] for item in batch])\n",
    "    enc_mask = torch.stack([item['encoder_attention_mask'] for item in batch])\n",
    "    dec_ids = torch.stack([item['decoder_input_ids'] for item in batch])\n",
    "    dec_mask = torch.stack([item['decoder_attention_mask'] for item in batch])\n",
    "    dec_target = torch.stack([item['decoder_target_ids'] for item in batch])\n",
    "    return {\n",
    "        'encoder_input_ids': enc_ids,\n",
    "        'encoder_attention_mask': enc_mask,\n",
    "        'decoder_input_ids': dec_ids,\n",
    "        'decoder_attention_mask': dec_mask,\n",
    "        'decoder_target_ids': dec_target\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# 3ï¸âƒ£ Ã–rnek kullanÄ±m\n",
    "# =====================================\n",
    "input_texts = [\n",
    "    \"Merhaba dÃ¼nya!\",\n",
    "    \"Transformers Ã§ok gÃ¼Ã§lÃ¼.\",\n",
    "    \"Memory-efficient pipeline.\"\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are powerful.\",\n",
    "    \"Super useful pipeline.\"\n",
    "]\n",
    "\n",
    "dataset = Seq2SeqDataset(input_texts, target_texts, tokenizer_name=\"t5-small\", max_length=16)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# 4ï¸âƒ£ Test bir batch\n",
    "# =====================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    print(\"Encoder Input IDs:\", batch['encoder_input_ids'].shape)\n",
    "    print(\"Encoder Attention Mask:\", batch['encoder_attention_mask'].shape)\n",
    "    print(\"Decoder Input IDs:\", batch['decoder_input_ids'].shape)\n",
    "    print(\"Decoder Attention Mask:\", batch['decoder_attention_mask'].shape)\n",
    "    print(\"Decoder Target IDs:\", batch['decoder_target_ids'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2d281",
   "metadata": {},
   "source": [
    "# DECODER + ENCODER Ortak Class'lar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07639fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0) , ) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape , dtype=x.dtype , device = x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) *  random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f98b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e7fea",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79874ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersEncoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size , embed_dim = 1024 , num_layers = 12 , dp = 0.1 ,num_heads=16 ,  expansion = 8 , max_len= 5000 , drop_path = 0.1 , use_swiglu =False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = TokenEmbedding(vocab_size,embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim , max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerEncoderBlockLLM(embed_dim , num_heads , dp , drop_path , expansion , use_swiglu) for _ in range(num_layers)]\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self,src_tokens , src_mask =None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask = src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3be73",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef9ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e533afd",
   "metadata": {},
   "source": [
    "----\n",
    "# YukarÄ±da bulunan kodlarla alakalÄ± ; \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cd921",
   "metadata": {},
   "source": [
    "## Soru 1-) DropPath classÄ±ndaki parametreler ne iÅŸe yarÄ±yor? Ä°ÅŸleyiÅŸi nasÄ±l oluyor?AdÄ±m adÄ±m aÃ§Ä±kayalÄ±m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3ef89",
   "metadata": {},
   "source": [
    "```python\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0) , ) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape , dtype=x.dtype , device = x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) *  random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f76bd9",
   "metadata": {},
   "source": [
    "### ğŸ¯ 1ï¸âƒ£ AmaÃ§ â€” DropPath ne iÅŸe yarar?\n",
    "\n",
    "* Normal Dropout, tensor iÃ§indeki bazÄ± nÃ¶ron aktivasyonlarÄ±nÄ± 0 yapar.\n",
    "* DropPath ise, bÃ¼tÃ¼n bir residual dalÄ±nÄ± (katmanÄ±) tamamen atlayabilir.\n",
    "*  Bu sayede modelin derin katmanlarÄ± rastgele â€œdevre dÄ±ÅŸÄ±â€ bÄ±rakÄ±lÄ±r,\n",
    "*  BÃ¶ylece model, daha sÄ±ÄŸ yollarÄ± da Ã¶ÄŸrenmeye zorlanÄ±r (regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7ec2",
   "metadata": {},
   "source": [
    "### ğŸ§© 2ï¸âƒ£ Parametreler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91894d2e",
   "metadata": {},
   "source": [
    "| Parametre   | AnlamÄ±            | AÃ§Ä±klama                                                                    |\n",
    "| ----------- | ----------------- | --------------------------------------------------------------------------- |\n",
    "| `drop_prob` | float (0â€“1 arasÄ±) | KatmanÄ± atlama olasÄ±lÄ±ÄŸÄ±. Ã–rneÄŸin 0.1 â†’ %10 ihtimalle residual dal atlanÄ±r. |\n",
    "| `x`         | Tensor            | Modelden geÃ§en aktivasyon (Ã¶rneÄŸin `F(x)` sonucu).                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb316ca6",
   "metadata": {},
   "source": [
    "## âš™ï¸ 3ï¸âƒ£ AdÄ±m AdÄ±m Ä°ÅŸleyiÅŸ\n",
    "\n",
    "\n",
    "### ğŸ”¸ AdÄ±m 1: \n",
    "```python\n",
    "if self.drop_prob == 0.0 or not self.training:\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129dd6ac",
   "metadata": {},
   "source": [
    "* DropPath sadece eÄŸitim sÄ±rasÄ±nda aktiftir, inference (tahmin) sÄ±rasÄ±nda devre dÄ±ÅŸÄ± kalÄ±r.\n",
    "\n",
    "* EÄŸer drop_prob = 0 ise, hiÃ§bir katman atlanmaz (normal Ã§alÄ±ÅŸÄ±r)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597825c8",
   "metadata": {},
   "source": [
    "### ğŸ”¸ AdÄ±m 2:\n",
    "```python\n",
    "keep_prob = 1 - self.drop_prob\n",
    "```\n",
    "\n",
    "\n",
    "* â€œKatmanÄ± koruma olasÄ±lÄ±ÄŸÄ±â€ belirlenir.\n",
    "* Ã–rnek: drop_prob = 0.1 â†’ keep_prob = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f95643",
   "metadata": {},
   "source": [
    "### ğŸ”¸ AdÄ±m 3:\n",
    "```python\n",
    "shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "```\n",
    "\n",
    "\n",
    "* Her batch Ã¶rneÄŸi iÃ§in tek bir rastgele sayÄ± Ã¼retmek Ã¼zere bir ÅŸekil oluÅŸturulur.\n",
    "\n",
    "Ã–rnek:\n",
    "\n",
    "> x boyutu: (B, L, D)\n",
    "\n",
    "> shape: (B, 1, 1)\n",
    "\n",
    "\n",
    "* ğŸ‘‰ Bu sayede her Ã¶rnek iÃ§in tÃ¼m zaman adÄ±mlarÄ±na aynÄ± maske uygulanÄ±r\n",
    "(yani â€œkatman bazÄ±nda dropoutâ€ yapÄ±lÄ±r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde920c5",
   "metadata": {},
   "source": [
    "### ğŸ”¸ AdÄ±m 4:\n",
    "```python\n",
    "random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.rand(shape) â†’ 0 ile 1 arasÄ±nda rastgele deÄŸerler Ã¼retir.\n",
    "\n",
    "* Buna keep_prob eklenir â†’ daÄŸÄ±lÄ±m saÄŸa kayar.\n",
    "* Ã–rnek: keep_prob = 0.9 â†’ random_tensor deÄŸerleri 0.9â€“1.9 aralÄ±ÄŸÄ±nda olur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7695db7",
   "metadata": {},
   "source": [
    "### ğŸ”¸ AdÄ±m 5:\n",
    "```python\n",
    "random_tensor.floor_()\n",
    "```\n",
    "\n",
    "\n",
    "floor_() iÅŸlemi ile:\n",
    "\n",
    "* DeÄŸer < 1.0 â†’ 0 olur (katman atlandÄ±)\n",
    "\n",
    "* DeÄŸer â‰¥ 1.0 â†’ 1 olur (katman korundu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6404217",
   "metadata": {},
   "source": [
    "## ğŸ§   Ã–zet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4597e6",
   "metadata": {},
   "source": [
    "| AÅŸama                  | Ä°ÅŸlev                                  |\n",
    "| ---------------------- | -------------------------------------- |\n",
    "| `drop_prob`            | KatmanÄ± atlama oranÄ± belirler          |\n",
    "| `keep_prob`            | KatmanÄ± koruma olasÄ±lÄ±ÄŸÄ±               |\n",
    "| `random_tensor`        | Hangi Ã¶rneklerin atlanacaÄŸÄ±nÄ± belirler |\n",
    "| `x.div(keep_prob)`     | Aktivasyon Ã¶lÃ§eÄŸini dengeler           |\n",
    "| `* random_tensor`      | KatmanÄ± aktif veya pasif hale getirir  |\n",
    "| `if not self.training` | Sadece eÄŸitim sÄ±rasÄ±nda etkin          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc251cb",
   "metadata": {},
   "source": [
    "### ğŸ“Š  GÃ¶rsel Ã–rnek (basitleÅŸtirilmiÅŸ)\n",
    "```bash\n",
    "Input (batch_size=3)\n",
    "x = [[1,1,1], [2,2,2], [3,3,3]]\n",
    "\n",
    "drop_prob=0.33 â†’ keep_prob=0.67\n",
    "random_tensor = [1, 0, 1]\n",
    "output = [[1.5,1.5,1.5], [0,0,0], [4.5,4.5,4.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7ebdd",
   "metadata": {},
   "source": [
    "----\n",
    "## Soru 2-) PositionelEncoding iÅŸlemindeki parametreler nedir? Torch iÃ§erisinde Ã§aÄŸÄ±rÄ±lan .exp ve .arange gibi terimler ne iÅŸe yarar? AdÄ±m adÄ±m aÃ§Ä±klayÄ±nÄ±z.\n",
    "```python\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a127c1",
   "metadata": {},
   "source": [
    "### âš™ï¸ 2ï¸âƒ£ PositionalEncoding SÄ±nÄ±fÄ± â€” AdÄ±m AdÄ±m AÃ§Ä±klama\n",
    "\n",
    "* Transformerlarda â€œpozisyon bilgisiâ€, sÄ±radaki kelimelerin **konum farklarÄ±nÄ±** modele Ã¶ÄŸretmek iÃ§in kullanÄ±lÄ±r.  \n",
    "* Bu sÄ±nÄ±f, **her pozisyona sinÃ¼s ve kosinÃ¼s temelli benzersiz bir vektÃ¶r** ekler.\n",
    "\n",
    "\n",
    "### ğŸ”¹ AdÄ±m 1: \n",
    "```python\n",
    "pe = torch.zeros(max_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cb31b",
   "metadata": {},
   "source": [
    "* pe: Pozisyonel kodlamalarÄ± tutacak bir matris.\n",
    "\n",
    "* Boyut: (max_len, embed_dim)\n",
    "\n",
    "* max_len: CÃ¼mlenin maksimum uzunluÄŸu (Ã¶r. 5000 token).\n",
    "\n",
    "* embed_dim: Her kelimenin embedding boyutu (Ã¶r. 512).\n",
    "\n",
    "* BaÅŸlangÄ±Ã§ta sÄ±fÄ±rlarla doldurulur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b38deb",
   "metadata": {},
   "source": [
    "### ğŸ”¹ AdÄ±m 2:\n",
    "```python\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.arange(0, max_len) â†’ 0â€™dan max_len-1â€™e kadar bir sayÄ± dizisi Ã¼retir.\n",
    "(Ã¶rneÄŸin [0, 1, 2, 3, ... 4999])\n",
    "\n",
    "* .unsqueeze(1) â†’ 1 boyut ekler â†’ (max_len, 1) haline getirir.\n",
    "\n",
    "- Bu vektÃ¶r her pozisyonun indeksini temsil eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1920d9",
   "metadata": {},
   "source": [
    "### ğŸ”¹ AdÄ±m 3:\n",
    "```python\n",
    "div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "```\n",
    "\n",
    "Bu kÄ±sÄ±m frekans Ã¶lÃ§eklerini hesaplar:\n",
    "\n",
    "* torch.arange(0, embed_dim, 2) â†’ 0, 2, 4, ... (her Ã§ift indeks iÃ§in)\n",
    "\n",
    "* (-math.log(10000.0) / embed_dim) â†’ sabit bir oran belirler.\n",
    "\n",
    "* torch.exp(...) â†’ bu oranÄ± Ã¼stel (exponential) fonksiyonla Ã¶lÃ§eklendirir.\n",
    "\n",
    "ğŸ‘‰ AmaÃ§: Her boyutta farklÄ± bir â€œdalga frekansÄ±â€ oluÅŸturmak.\n",
    "* Yani kÃ¼Ã§Ã¼k boyutlar yavaÅŸ deÄŸiÅŸen sinÃ¼s/kosinÃ¼s, bÃ¼yÃ¼k boyutlar hÄ±zlÄ± deÄŸiÅŸen frekans Ã¼retir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66581a3",
   "metadata": {},
   "source": [
    "### ğŸ”¹ AdÄ±m 4:\n",
    "```python\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Ã‡ift indeksler (0,2,4,...) iÃ§in sinÃ¼s,\n",
    "\n",
    "* Tek indeksler (1,3,5,...) iÃ§in kosinÃ¼s uygulanÄ±r.\n",
    "\n",
    "ğŸ”¸ SonuÃ§:\n",
    "* Her pozisyon iÃ§in embed_dim uzunluÄŸunda benzersiz bir vektÃ¶r elde edilir.\n",
    "* Bu vektÃ¶rlerin sinÃ¼s ve kosinÃ¼s kombinasyonu, mutlak pozisyonu ve gÃ¶reli farklarÄ± kodlar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd9e0",
   "metadata": {},
   "source": [
    "### ğŸ”¹ AdÄ±m 5:\n",
    "```python\n",
    "pe = pe.unsqueeze(0)\n",
    "self.register_buffer('pe', pe)\n",
    "```\n",
    "\n",
    "* .unsqueeze(0) â†’ batch boyutu ekler: (1, max_len, embed_dim)\n",
    "\n",
    "* register_buffer â†’ pe tensÃ¶rÃ¼nÃ¼ modelin parametresi yapmaz, ama state_dictâ€™e dahil eder.\n",
    "(yani requires_grad=False, ama modelle birlikte GPUâ€™ya taÅŸÄ±nÄ±r.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2bc2d",
   "metadata": {},
   "source": [
    "### ğŸ”¹ AdÄ±m 6:\n",
    "```python\n",
    "def forward(self, x):\n",
    "    seq_len = x.size(1)\n",
    "    return x + self.pe[:, :seq_len, :]\n",
    "```\n",
    "\n",
    "\n",
    "* GiriÅŸ x boyutu: (batch, seq_len, embed_dim)\n",
    "\n",
    "* self.pe[:, :seq_len, :] â†’ sadece ilgili uzunluk kadar pozisyonel vektÃ¶r alÄ±r.\n",
    "\n",
    "* x + self.pe â†’ embeddingâ€™e pozisyon bilgisi eklenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04531ede",
   "metadata": {},
   "source": [
    "## ğŸ§  Ek Bilgi â€” torch FonksiyonlarÄ±nÄ±n GÃ¶revleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a5ab6",
   "metadata": {},
   "source": [
    "| Fonksiyon                  | AÃ§Ä±klama                                            |\n",
    "| -------------------------- | --------------------------------------------------- |\n",
    "| `torch.arange(start, end)` | Belirtilen aralÄ±kta ardÄ±ÅŸÄ±k sayÄ±lar Ã¼retir.         |\n",
    "| `.unsqueeze(dim)`          | Belirtilen boyuta 1 ekler (Ã¶r. (5000,) â†’ (5000,1)). |\n",
    "| `torch.exp(tensor)`        | Her elemanÄ±n e^xâ€™ini (Ã¼stelini) alÄ±r.               |\n",
    "| `torch.sin`, `torch.cos`   | Trigonometrik sinÃ¼s ve kosinÃ¼s fonksiyonlarÄ±dÄ±r.    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444abeee",
   "metadata": {},
   "source": [
    "### ğŸ”¸ Ã–zet:\n",
    "\n",
    "Bu yapÄ± sayesinde model, sÄ±radaki tokenâ€™larÄ±n:\n",
    "\n",
    "* sÄ±ralarÄ±nÄ±,\n",
    "\n",
    "* gÃ¶reli uzaklÄ±klarÄ±nÄ±,\n",
    "\n",
    "* ve baÄŸlamsal iliÅŸkilerini\n",
    "\n",
    "Ã¶ÄŸrenebilir hale gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c5309",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e209c7",
   "metadata": {},
   "source": [
    "## Soru 3-) FeedForward ne iÅŸe yarar ? Silu ve Gelu fonksiyonlarÄ±nÄ±n farkÄ± nedir? Neden bÃ¶yle bir ayrÄ±ÅŸÄ±m yapÄ±yoruz ? \n",
    "```python \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cec79",
   "metadata": {},
   "source": [
    "### ğŸ”¹ 1ï¸âƒ£ FeedForward (FFN) KatmanÄ± Nedir?\n",
    "\n",
    "* Transformerâ€™Ä±n her encoder veya decoder bloÄŸunun iÃ§inde, attention katmanÄ±ndan sonra yer alan bir tam baÄŸlantÄ±lÄ± (fully-connected) yapÄ±dÄ±r.\n",
    "\n",
    "AmaÃ§:\n",
    "\n",
    "â¡ï¸ Her tokenâ€™Ä±n (kelimenin) temsilini baÄŸÄ±msÄ±z olarak dÃ¶nÃ¼ÅŸtÃ¼rmek,\n",
    "\n",
    "â¡ï¸ Modele nonlineerlik ve daha yÃ¼ksek temsil gÃ¼cÃ¼ kazandÄ±rmak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b841c",
   "metadata": {},
   "source": [
    "### ğŸ§  Genel Ä°ÅŸleyiÅŸ\n",
    "\n",
    "Girdi:\n",
    "* Attention katmanÄ±ndan gelen x (ÅŸekil: [batch_size, seq_len, embed_dim])\n",
    "\n",
    "Ä°lk Linear:\n",
    "* Her token vektÃ¶rÃ¼nÃ¼ embed_dimâ€™den embed_dim * expansion boyutuna Ã§Ä±karÄ±r.\n",
    "(Bu geniÅŸletme, bilgiyi daha zengin bir uzayda iÅŸlemeye yarar.)\n",
    "\n",
    "Aktivasyon Fonksiyonu (GELU veya SiLU):\n",
    "* Nonlineer dÃ¶nÃ¼ÅŸÃ¼m saÄŸlar â€” modelin karmaÅŸÄ±k iliÅŸkileri Ã¶ÄŸrenmesine yardÄ±mcÄ± olur.\n",
    "\n",
    "Dropout:\n",
    "* Regularization, yani overfittingâ€™i Ã¶nleme.\n",
    "\n",
    "Ä°kinci Linear:\n",
    "* Boyutu tekrar embed_dimâ€™e indirir (geri sÄ±kÄ±ÅŸtÄ±rÄ±r)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2ef02",
   "metadata": {},
   "source": [
    "### ğŸ”¹ 2ï¸âƒ£ SiLU ve GELU FarkÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c87b02",
   "metadata": {},
   "source": [
    "| Ã–zellik               | **SiLU (Sigmoid Linear Unit)**                           | **GELU (Gaussian Error Linear Unit)**                            |\n",
    "| --------------------- | -------------------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **FormÃ¼l**            | `x * sigmoid(x)`                                         | `x * Î¦(x)` <br> (Î¦: normal daÄŸÄ±lÄ±mÄ±n kÃ¼mÃ¼latif fonksiyonu)       |\n",
    "| **YaklaÅŸÄ±m**          | Sigmoidâ€™e dayalÄ±, yumuÅŸak ReLU benzeri                   | Gaussian olasÄ±lÄ±k temelli                                        |\n",
    "| **DavranÄ±ÅŸ**          | KÃ¼Ã§Ã¼k pozitif giriÅŸlerde yumuÅŸak, bÃ¼yÃ¼k pozitiflerde â‰ˆ x | Benzer ÅŸekilde yumuÅŸak ama Gaussian eÄŸrisiyle Ã¶lÃ§eklenir         |\n",
    "| **Grafiksel GÃ¶rÃ¼nÃ¼m** | Daha dÃ¼z geÃ§iÅŸli, stabil                                 | Hafif asimetrik, â€œdaha doÄŸalâ€ aktivasyon saÄŸlar                  |\n",
    "| **KullanÄ±m**          | Genelde **SwiGLU** gibi kapÄ± mekanizmalarÄ±nda            | Transformer modellerinde (Ã¶rneÄŸin BERT, GPT, ViT) sÄ±k kullanÄ±lÄ±r |\n",
    "| **Avantaj**           | HesaplamasÄ± daha ucuz ve kararlÄ±                         | Empirik olarak bazÄ± modellerde daha yÃ¼ksek doÄŸruluk saÄŸlar       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cb928",
   "metadata": {},
   "source": [
    "### ğŸ”¹ 3ï¸âƒ£ Neden â€œSwiGLUâ€ AyrÄ±mÄ± YapÄ±yoruz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd43b10",
   "metadata": {},
   "source": [
    "Kodda bir parametre var:\n",
    "\n",
    "```python\n",
    "use_swiglu=True\n",
    "```\n",
    "\n",
    "\n",
    "* Bu, FeedForward iÃ§inde farklÄ± aktivasyon kombinasyonlarÄ±nÄ± denemeye yarÄ±yor.\n",
    "\n",
    "### ğŸ§© SwiGLU Nedir?\n",
    "\n",
    "* SwiGLU (Swish-Gated Linear Unit), SiLU temelli bir kapÄ±lÄ± (gated) yapÄ±.\n",
    "Burada Linear katmandan iki ayrÄ± parÃ§a Ã§Ä±karÄ±lÄ±r:\n",
    "\n",
    "* biri gate (kapÄ±),\n",
    "\n",
    "* diÄŸeri deÄŸer (value) kÄ±smÄ±."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979a5a0",
   "metadata": {},
   "source": [
    "Yani:\n",
    "\n",
    "* y = (x * W1) * SiLU(x * W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13795c8e",
   "metadata": {},
   "source": [
    "* Bu yapÄ±, modelin bilgiyi seÃ§ici olarak geÃ§irmesine olanak tanÄ±r â€”\n",
    "tÄ±pkÄ± attentionâ€™daki â€œhangi bilgi Ã¶nemli?â€ mantÄ±ÄŸÄ± gibi.\n",
    "\n",
    "### Avantaj:\n",
    "\n",
    "* Daha iyi bilgi akÄ±ÅŸÄ±,\n",
    "\n",
    "* Daha az â€œÃ¶lÃ¼ nÃ¶ronâ€,\n",
    "\n",
    "* DÃ¼ÅŸÃ¼k hesaplama maliyetiyle yÃ¼ksek performans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f61ccf",
   "metadata": {},
   "source": [
    "### ğŸ”¹ 4ï¸âƒ£ Ã–zetle AdÄ±m AdÄ±m Ä°ÅŸleyiÅŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457d510",
   "metadata": {},
   "source": [
    "| AdÄ±m | Ä°ÅŸlem                                         | AÃ§Ä±klama                              |\n",
    "| ---- | --------------------------------------------- | ------------------------------------- |\n",
    "| 1    | `nn.Linear(embed_dim, embed_dim * expansion)` | Boyutu geniÅŸletir (bilgi uzayÄ± bÃ¼yÃ¼r) |\n",
    "| 2    | `nn.GELU()` veya `nn.SiLU()`                  | Nonlineer dÃ¶nÃ¼ÅŸÃ¼m                     |\n",
    "| 3    | `nn.Dropout(dp)`                              | Overfitting Ã¶nleme                    |\n",
    "| 4    | `nn.Linear(embed_dim * expansion, embed_dim)` | Boyutu geri indirir                   |\n",
    "| 5    | `nn.Dropout(dp)`                              | Son dÃ¼zenleme, regularization         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589bd88",
   "metadata": {},
   "source": [
    "### ğŸ”¹ 5ï¸âƒ£ KÄ±sa SonuÃ§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e3d4f",
   "metadata": {},
   "source": [
    "| Parametre    | AnlamÄ±                                                             |\n",
    "| ------------ | ------------------------------------------------------------------ |\n",
    "| `embed_dim`  | Girdi vektÃ¶r boyutu                                                |\n",
    "| `expansion`  | Katman geniÅŸleme oranÄ± (genelde 4â€“8 arasÄ±)                         |\n",
    "| `dp`         | Dropout oranÄ±                                                      |\n",
    "| `use_swiglu` | True ise SiLU tabanlÄ± SwiGLU kullanÄ±lÄ±r, False ise GELU klasik FFN |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8980638",
   "metadata": {},
   "source": [
    "```bash\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " â”‚   Girdi: x (batch, seq, embed_dim) â”‚\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ Linear (embed_dim â†’ embed_dim * expansion) â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚  GELU      â”‚   â† Nonlineer dÃ¶nÃ¼ÅŸÃ¼m (Gaussian temelli)\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚ Dropout(dp)â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ Linear (embed_dim * expansion â†’ embed_dim) â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚ Dropout(dp)â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                â”‚\n",
    "                â–¼\n",
    " â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " â”‚  Ã‡Ä±ktÄ±: y (aynÄ± boyut: embed_dim) â”‚\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae24c39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271a993",
   "metadata": {},
   "source": [
    "## Soru 4-) TransformersEncoderBlockLLM sÄ±nÄ±fÄ± ne iÅŸe yarÄ±yor.Gamma parametreleri nedir ? Ã‡arpÄ±m iÃ§in kullanÄ±lan 1e-2 gibi deÄŸerler azalsa ya da artsa nasÄ±l bir deÄŸiÅŸim beklenir? AyrÄ±ca bu classÄ±n forward fonksiyonunda ne gibi iÅŸlemler yapÄ±lÄ±yor ? \n",
    "```python\n",
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41715c5f",
   "metadata": {},
   "source": [
    "### âš™ï¸ Soru 4 - TransformerEncoderBlockLLM\n",
    "#### 1ï¸âƒ£ AmaÃ§ ve Ä°ÅŸlevi\n",
    "\n",
    "* TransformerEncoderBlockLLM sÄ±nÄ±fÄ±, Transformer mimarisindeki tek bir encoder bloÄŸunu temsil eder.\n",
    "\n",
    "Bir bloÄŸun temel iÅŸlemleri ÅŸunlardÄ±r:\n",
    "\n",
    "* Self-Attention: Girdi sÄ±rasÄ±ndaki her tokenâ€™Ä±n diÄŸer tokenâ€™larla iliÅŸkisini Ã¶ÄŸrenir.\n",
    "\n",
    "* FeedForward (FFN): Token baÅŸÄ±na Ã¶ÄŸrenilen Ã¶zellikleri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve model kapasitesini artÄ±rÄ±r.\n",
    "\n",
    "- LayerNorm ve DropPath: EÄŸitimi stabilize eder ve dÃ¼zenliliÄŸi artÄ±rÄ±r.\n",
    "\n",
    "KÄ±saca:\n",
    "\n",
    "* â€œGirdi â†’ Normalizasyon â†’ Self-Attention â†’ Skip Connection â†’ FFN â†’ Skip Connection â†’ Ã‡Ä±kÄ±ÅŸâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4729204",
   "metadata": {},
   "source": [
    "#### 2ï¸âƒ£ gamma_1 ve gamma_2 Parametreleri\n",
    "```python\n",
    "self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Bunlar Ã¶lÃ§ekleme faktÃ¶rleri olarak Ã§alÄ±ÅŸÄ±r.\n",
    "\n",
    "* attn_out ve ffn_out deÄŸerlerine Ã§arpÄ±lÄ±r Ã¶nce DropPath, sonra residual ekleme yapÄ±lÄ±r:\n",
    "```python\n",
    "\n",
    "x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "```\n",
    "\n",
    "\n",
    "AmaÃ§: residual baÄŸlantÄ±daki katkÄ±yÄ± baÅŸlangÄ±Ã§ta kÃ¼Ã§Ã¼k tutmak, bÃ¶ylece Ã¶ÄŸrenme daha stabil olur.\n",
    "\n",
    "* EÄŸer 1e-2 deÄŸeri artarsa: attention/FFN Ã§Ä±ktÄ±sÄ± daha fazla eklenir â†’ gradient daha hÄ±zlÄ± yayÄ±lÄ±r â†’ risk: instabilite, patlayan deÄŸerler.\n",
    "\n",
    "* EÄŸer azalÄ±rsa: blok Ã§Ä±ktÄ±sÄ± daha az katkÄ±da bulunur â†’ yavaÅŸ Ã¶ÄŸrenme, yavaÅŸ adaptasyon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f684f",
   "metadata": {},
   "source": [
    "#### 3ï¸âƒ£ Forward Fonksiyonu AdÄ±m AdÄ±m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa129b06",
   "metadata": {},
   "source": [
    "```python \n",
    "def forward(self, x, mask=None):\n",
    "    # 1ï¸âƒ£ Normalizasyon\n",
    "    attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "    \n",
    "    # 2ï¸âƒ£ DropPath + Gamma Ã§arpÄ±mÄ± + Residual\n",
    "    x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "    # 3ï¸âƒ£ FeedForward\n",
    "    ffn_out = self.ffn(self.norm2(x))\n",
    "    \n",
    "    # 4ï¸âƒ£ DropPath + Gamma Ã§arpÄ±mÄ± + Residual\n",
    "    x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc04ae8",
   "metadata": {},
   "source": [
    "| AdÄ±m | Ä°ÅŸlem                                              | AmaÃ§                                        |\n",
    "| ---- | -------------------------------------------------- | ------------------------------------------- |\n",
    "| 1    | `self.norm1(x)`                                    | LayerNorm ile input stabilizasyonu          |\n",
    "| 2    | `self.self_attn(...)`                              | Tokenâ€™lar arasÄ± iliÅŸkileri Ã¶ÄŸrenme          |\n",
    "| 3    | `self.gamma_1 * attn_out`                          | Residual katkÄ±sÄ±nÄ± Ã¶lÃ§ekleme                |\n",
    "| 4    | `self.drop_path(...)`                              | Stochastic Depth, blok rastgele atlanabilir |\n",
    "| 5    | `x + ...`                                          | Residual baÄŸlantÄ± ekleme                    |\n",
    "| 6    | `self.norm2(x)` â†’ `self.ffn(...)`                  | FeedForward, token baÅŸÄ±na dÃ¶nÃ¼ÅŸÃ¼m           |\n",
    "| 7    | `self.gamma_2 * ffn_out` + `drop_path` + `x + ...` | Residual + Stabilizasyon                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5fa06",
   "metadata": {},
   "source": [
    "#### 4ï¸âƒ£ Ã–zet\n",
    "\n",
    "* TransformerEncoderBlockLLM: tek encoder bloÄŸu, Self-Attention + FFN + Residual + LayerNorm + DropPath iÃ§erir.\n",
    "\n",
    "* gamma_1 ve gamma_2: residual katkÄ±yÄ± baÅŸlangÄ±Ã§ta kÃ¼Ã§Ã¼k tutar. DeÄŸeri deÄŸiÅŸtirirsen stabilite ve Ã¶ÄŸrenme hÄ±zÄ± deÄŸiÅŸir.\n",
    "\n",
    "```bash4\n",
    "* forward:\n",
    "\n",
    "* LayerNorm\n",
    "\n",
    "* Self-Attention â†’ scaled + drop_path â†’ residual\n",
    "\n",
    "* FeedForward â†’ scaled + drop_path â†’ residual\n",
    "\n",
    "* Ã‡Ä±kÄ±ÅŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac398b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53581d",
   "metadata": {},
   "source": [
    "## Soru 5-) TransformerDecoderBlockLLM sÄ±nÄ±fÄ±nda bulunan iÅŸlem adÄ±mlarÄ± nedir?OluÅŸturulan gamma parametreleri ne iÅŸe yarar? Self Attention ve Cross Attention aynÄ± class iÃ§erisinde ise modele self kullanacaÄŸÄ±nÄ± ya da cross attention kullanacaÄŸÄ±nÄ± nasÄ±l aktarÄ±yoruz ? \n",
    "```python\n",
    "\n",
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1d17b",
   "metadata": {},
   "source": [
    "### âš™ï¸ Soru 5 - TransformerDecoderBlockLLM\n",
    "#### 1ï¸âƒ£ AmaÃ§ ve Ä°ÅŸlevi\n",
    "\n",
    "* TransformerDecoderBlockLLM, tek bir decoder bloÄŸunu temsil eder ve Ã¼Ã§ ana bileÅŸenden oluÅŸur:\n",
    "\n",
    "* Masked Self-Attention â†’ Decoder kendi Ã¶nceki tokenâ€™larÄ±nÄ± gÃ¶rebilir, geleceÄŸi gÃ¶remez (mask ile).\n",
    "\n",
    "* Cross-Attention â†’ Encoder Ã§Ä±kÄ±ÅŸÄ±nÄ± kullanarak decoder tokenâ€™larÄ±nÄ± context ile iliÅŸkilendirir.\n",
    "\n",
    "* FeedForward (FFN) â†’ Token baÅŸÄ±na dÃ¶nÃ¼ÅŸÃ¼m ve Ã¶zellik zenginleÅŸtirme.\n",
    "\n",
    "Ek olarak, LayerNorm, DropPath ve gamma parametreleri stabilite ve residual katkÄ±sÄ± iÃ§in kullanÄ±lÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944420fa",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ Gamma Parametreleri\n",
    "```python\n",
    "self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "```\n",
    "\n",
    "\n",
    "Her gamma, residual baÄŸlantÄ±nÄ±n katkÄ±sÄ±nÄ± Ã¶lÃ§ekler.\n",
    "\n",
    "* gamma_1 â†’ Self-Attention Ã§Ä±ktÄ±sÄ±\n",
    "\n",
    "* gamma_2 â†’ Cross-Attention Ã§Ä±ktÄ±sÄ±\n",
    "\n",
    "* gamma_3 â†’ FeedForward Ã§Ä±ktÄ±sÄ±\n",
    "\n",
    "> KÃ¼Ã§Ã¼k deÄŸerler (1e-2) â†’ baÅŸlangÄ±Ã§ta residual katkÄ±sÄ± dÃ¼ÅŸÃ¼k â†’ stabil Ã¶ÄŸrenme.\n",
    "\n",
    "> BÃ¼yÃ¼k deÄŸerler â†’ daha hÄ±zlÄ± katkÄ±, risk: instabilite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0f49f",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ Forward Fonksiyon AdÄ±m AdÄ±m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6bf03",
   "metadata": {},
   "source": [
    "```python \n",
    "def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "    # 1ï¸âƒ£ Masked Self-Attention\n",
    "    attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "    x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "    # 2ï¸âƒ£ Cross-Attention\n",
    "    if enc_out is not None:\n",
    "        cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "        x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "    # 3ï¸âƒ£ FeedForward\n",
    "    ffn_out = self.ffn(self.norm3(x))\n",
    "    x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff69a3",
   "metadata": {},
   "source": [
    "| AdÄ±m | Ä°ÅŸlem                                             | AmaÃ§                                                           |\n",
    "| ---- | ------------------------------------------------- | -------------------------------------------------------------- |\n",
    "| 1    | `self.norm1(x)` + `self.self_attn(...)`           | Decoder kendi geÃ§miÅŸ tokenâ€™larÄ±yla iliÅŸki kurar (masked)       |\n",
    "| 2    | `x + drop_path(gamma_1 * attn_out)`               | Residual baÄŸlantÄ± ve DropPath ile stabilizasyon                |\n",
    "| 3    | `if enc_out is not None` â†’ `self.cross_attn(...)` | Encoderâ€™dan gelen baÄŸlam bilgisi ile tokenâ€™larÄ± iliÅŸkilendirir |\n",
    "| 4    | `x + drop_path(gamma_2 * cross_out)`              | Cross-Attention residual eklenir                               |\n",
    "| 5    | `self.norm3(x)` + `ffn(...)`                      | FeedForward ile token Ã¶zellikleri zenginleÅŸtirilir             |\n",
    "| 6    | `x + drop_path(gamma_3 * ffn_out)`                | FFN residual eklenir ve Ã§Ä±ktÄ± dÃ¶ner                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce27ae",
   "metadata": {},
   "source": [
    "### 4ï¸âƒ£ Self-Attention ve Cross-Attention KullanÄ±mÄ±\n",
    "\n",
    "* Self-Attention her zaman Ã§alÄ±ÅŸÄ±r, decoder kendi geÃ§miÅŸ tokenâ€™larÄ±nÄ± gÃ¶rÃ¼r.\n",
    "\n",
    "* Cross-Attention sadece enc_out verilmiÅŸse Ã§alÄ±ÅŸÄ±r:\n",
    "\n",
    "```python\n",
    "if enc_out is not None:\n",
    "    cross_out = self.cross_attn(...)\n",
    "```\n",
    "\n",
    "\n",
    "* Yani modele hangi attentionâ€™Ä± kullanacaÄŸÄ±nÄ± enc_out parametresi ile aktarÄ±yoruz.\n",
    "\n",
    "* self_mask â†’ self attention maskesi (geleceÄŸi gÃ¶rmeyi engeller)\n",
    "\n",
    "* enc_mask â†’ encoder maskesi (padding tokenlarÄ±nÄ± gizler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010772c5",
   "metadata": {},
   "source": [
    "### 5ï¸âƒ£ Ã–zet\n",
    "\n",
    "* TransformerDecoderBlockLLM: Masked Self-Attention + Cross-Attention + FFN\n",
    "\n",
    "* Gamma parametreleri â†’ residual katkÄ±yÄ± Ã¶lÃ§ekler, stabilite saÄŸlar\n",
    "\n",
    "* Self ve Cross Attention aynÄ± class iÃ§inde, cross_attention opsiyoneldir (enc_out varsa Ã§alÄ±ÅŸÄ±r)\n",
    "\n",
    "* DropPath â†’ bloÄŸu rastgele atlayarak regularization saÄŸlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff402e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8515604f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
