{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34cb869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37197e4",
   "metadata": {},
   "source": [
    "# TOKENİZASYON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812302dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input IDs: torch.Size([2, 16])\n",
      "Encoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Input IDs: torch.Size([2, 16])\n",
      "Decoder Attention Mask: torch.Size([2, 16])\n",
      "Decoder Target IDs: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 1️⃣ Seq2Seq Tokenization Dataset (T5/BART uyumlu)\n",
    "# =====================================\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Tokenization pipeline for seq2seq LLMs (T5, BART, mBART)\n",
    "    Includes encoder/decoder tokenization, attention masks, shifted decoder input.\n",
    "    \"\"\"\n",
    "    def __init__(self, sources, targets, tokenizer_name=\"t5-small\", max_length=64):\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Seq2Seq modellerinde decoder_start_token_id önemli\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token  # zorunlu pad token\n",
    "        self.decoder_start_token_id = self.tokenizer.pad_token_id if self.tokenizer.bos_token_id is None else self.tokenizer.bos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sources)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # -------------------\n",
    "        # Encoder tokenization\n",
    "        # -------------------\n",
    "        enc = self.tokenizer(\n",
    "            self.sources[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder tokenization\n",
    "        # -------------------\n",
    "        dec = self.tokenizer(\n",
    "            self.targets[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # -------------------\n",
    "        # Decoder input (shifted right)\n",
    "        # -------------------\n",
    "        decoder_input_ids = torch.cat(\n",
    "            [torch.full((1,1), self.decoder_start_token_id), dec['input_ids'][:, :-1]], dim=1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': enc['input_ids'].squeeze(0),\n",
    "            'encoder_attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'decoder_input_ids': decoder_input_ids.squeeze(0),\n",
    "            'decoder_attention_mask': dec['attention_mask'].squeeze(0),\n",
    "            'decoder_target_ids': dec['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# =====================================\n",
    "# 2️⃣ Collate function\n",
    "# =====================================\n",
    "def collate_fn(batch):\n",
    "    enc_ids = torch.stack([item['encoder_input_ids'] for item in batch])\n",
    "    enc_mask = torch.stack([item['encoder_attention_mask'] for item in batch])\n",
    "    dec_ids = torch.stack([item['decoder_input_ids'] for item in batch])\n",
    "    dec_mask = torch.stack([item['decoder_attention_mask'] for item in batch])\n",
    "    dec_target = torch.stack([item['decoder_target_ids'] for item in batch])\n",
    "    return {\n",
    "        'encoder_input_ids': enc_ids,\n",
    "        'encoder_attention_mask': enc_mask,\n",
    "        'decoder_input_ids': dec_ids,\n",
    "        'decoder_attention_mask': dec_mask,\n",
    "        'decoder_target_ids': dec_target\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# 3️⃣ Örnek kullanım\n",
    "# =====================================\n",
    "input_texts = [\n",
    "    \"Merhaba dünya!\",\n",
    "    \"Transformers çok güçlü.\",\n",
    "    \"Memory-efficient pipeline.\"\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are powerful.\",\n",
    "    \"Super useful pipeline.\"\n",
    "]\n",
    "\n",
    "dataset = Seq2SeqDataset(input_texts, target_texts, tokenizer_name=\"t5-small\", max_length=16)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# 4️⃣ Test bir batch\n",
    "# =====================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    print(\"Encoder Input IDs:\", batch['encoder_input_ids'].shape)\n",
    "    print(\"Encoder Attention Mask:\", batch['encoder_attention_mask'].shape)\n",
    "    print(\"Decoder Input IDs:\", batch['decoder_input_ids'].shape)\n",
    "    print(\"Decoder Attention Mask:\", batch['decoder_attention_mask'].shape)\n",
    "    print(\"Decoder Target IDs:\", batch['decoder_target_ids'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2d281",
   "metadata": {},
   "source": [
    "# DECODER + ENCODER Ortak Class'lar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07639fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0) , ) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape , dtype=x.dtype , device = x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) *  random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f98b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, dp=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, C = x.size()\n",
    "        return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        B, _, T, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        Q = self.split_heads(self.q_proj(query))\n",
    "        K = self.split_heads(self.k_proj(key))\n",
    "        V = self.split_heads(self.v_proj(value))\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return self.out_proj(self.combine_heads(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e7fea",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79874ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersEncoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size , embed_dim = 1024 , num_layers = 12 , dp = 0.1 ,num_heads=16 ,  expansion = 8 , max_len= 5000 , drop_path = 0.1 , use_swiglu =False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = TokenEmbedding(vocab_size,embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim , max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerEncoderBlockLLM(embed_dim , num_heads , dp , drop_path , expansion , use_swiglu) for _ in range(num_layers)]\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self,src_tokens , src_mask =None):\n",
    "        x = self.tok_emb(src_tokens)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask = src_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3be73",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef9ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=1024, num_layers=12, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, max_len=5000, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlockLLM(embed_dim, num_heads, dp, drop_path, expansion, use_swiglu) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, enc_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e533afd",
   "metadata": {},
   "source": [
    "----\n",
    "# Yukarıda bulunan kodlarla alakalı ; \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cd921",
   "metadata": {},
   "source": [
    "## Soru 1-) DropPath classındaki parametreler ne işe yarıyor? İşleyişi nasıl oluyor?Adım adım açıkayalım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3ef89",
   "metadata": {},
   "source": [
    "```python\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self,x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.size(0) , ) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape , dtype=x.dtype , device = x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) *  random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f76bd9",
   "metadata": {},
   "source": [
    "### 🎯 1️⃣ Amaç — DropPath ne işe yarar?\n",
    "\n",
    "* Normal Dropout, tensor içindeki bazı nöron aktivasyonlarını 0 yapar.\n",
    "* DropPath ise, bütün bir residual dalını (katmanı) tamamen atlayabilir.\n",
    "*  Bu sayede modelin derin katmanları rastgele “devre dışı” bırakılır,\n",
    "*  Böylece model, daha sığ yolları da öğrenmeye zorlanır (regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b7ec2",
   "metadata": {},
   "source": [
    "### 🧩 2️⃣ Parametreler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91894d2e",
   "metadata": {},
   "source": [
    "| Parametre   | Anlamı            | Açıklama                                                                    |\n",
    "| ----------- | ----------------- | --------------------------------------------------------------------------- |\n",
    "| `drop_prob` | float (0–1 arası) | Katmanı atlama olasılığı. Örneğin 0.1 → %10 ihtimalle residual dal atlanır. |\n",
    "| `x`         | Tensor            | Modelden geçen aktivasyon (örneğin `F(x)` sonucu).                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb316ca6",
   "metadata": {},
   "source": [
    "## ⚙️ 3️⃣ Adım Adım İşleyiş\n",
    "\n",
    "\n",
    "### 🔸 Adım 1: \n",
    "```python\n",
    "if self.drop_prob == 0.0 or not self.training:\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129dd6ac",
   "metadata": {},
   "source": [
    "* DropPath sadece eğitim sırasında aktiftir, inference (tahmin) sırasında devre dışı kalır.\n",
    "\n",
    "* Eğer drop_prob = 0 ise, hiçbir katman atlanmaz (normal çalışır)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597825c8",
   "metadata": {},
   "source": [
    "### 🔸 Adım 2:\n",
    "```python\n",
    "keep_prob = 1 - self.drop_prob\n",
    "```\n",
    "\n",
    "\n",
    "* “Katmanı koruma olasılığı” belirlenir.\n",
    "* Örnek: drop_prob = 0.1 → keep_prob = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f95643",
   "metadata": {},
   "source": [
    "### 🔸 Adım 3:\n",
    "```python\n",
    "shape = (x.size(0),) + (1,) * (x.ndim - 1)\n",
    "```\n",
    "\n",
    "\n",
    "* Her batch örneği için tek bir rastgele sayı üretmek üzere bir şekil oluşturulur.\n",
    "\n",
    "Örnek:\n",
    "\n",
    "> x boyutu: (B, L, D)\n",
    "\n",
    "> shape: (B, 1, 1)\n",
    "\n",
    "\n",
    "* 👉 Bu sayede her örnek için tüm zaman adımlarına aynı maske uygulanır\n",
    "(yani “katman bazında dropout” yapılır)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde920c5",
   "metadata": {},
   "source": [
    "### 🔸 Adım 4:\n",
    "```python\n",
    "random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.rand(shape) → 0 ile 1 arasında rastgele değerler üretir.\n",
    "\n",
    "* Buna keep_prob eklenir → dağılım sağa kayar.\n",
    "* Örnek: keep_prob = 0.9 → random_tensor değerleri 0.9–1.9 aralığında olur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7695db7",
   "metadata": {},
   "source": [
    "### 🔸 Adım 5:\n",
    "```python\n",
    "random_tensor.floor_()\n",
    "```\n",
    "\n",
    "\n",
    "floor_() işlemi ile:\n",
    "\n",
    "* Değer < 1.0 → 0 olur (katman atlandı)\n",
    "\n",
    "* Değer ≥ 1.0 → 1 olur (katman korundu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6404217",
   "metadata": {},
   "source": [
    "## 🧠  Özet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4597e6",
   "metadata": {},
   "source": [
    "| Aşama                  | İşlev                                  |\n",
    "| ---------------------- | -------------------------------------- |\n",
    "| `drop_prob`            | Katmanı atlama oranı belirler          |\n",
    "| `keep_prob`            | Katmanı koruma olasılığı               |\n",
    "| `random_tensor`        | Hangi örneklerin atlanacağını belirler |\n",
    "| `x.div(keep_prob)`     | Aktivasyon ölçeğini dengeler           |\n",
    "| `* random_tensor`      | Katmanı aktif veya pasif hale getirir  |\n",
    "| `if not self.training` | Sadece eğitim sırasında etkin          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc251cb",
   "metadata": {},
   "source": [
    "### 📊  Görsel Örnek (basitleştirilmiş)\n",
    "```bash\n",
    "Input (batch_size=3)\n",
    "x = [[1,1,1], [2,2,2], [3,3,3]]\n",
    "\n",
    "drop_prob=0.33 → keep_prob=0.67\n",
    "random_tensor = [1, 0, 1]\n",
    "output = [[1.5,1.5,1.5], [0,0,0], [4.5,4.5,4.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7ebdd",
   "metadata": {},
   "source": [
    "----\n",
    "## Soru 2-) PositionelEncoding işlemindeki parametreler nedir? Torch içerisinde çağırılan .exp ve .arange gibi terimler ne işe yarar? Adım adım açıklayınız.\n",
    "```python\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a127c1",
   "metadata": {},
   "source": [
    "### ⚙️ 2️⃣ PositionalEncoding Sınıfı — Adım Adım Açıklama\n",
    "\n",
    "* Transformerlarda “pozisyon bilgisi”, sıradaki kelimelerin **konum farklarını** modele öğretmek için kullanılır.  \n",
    "* Bu sınıf, **her pozisyona sinüs ve kosinüs temelli benzersiz bir vektör** ekler.\n",
    "\n",
    "\n",
    "### 🔹 Adım 1: \n",
    "```python\n",
    "pe = torch.zeros(max_len, embed_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cb31b",
   "metadata": {},
   "source": [
    "* pe: Pozisyonel kodlamaları tutacak bir matris.\n",
    "\n",
    "* Boyut: (max_len, embed_dim)\n",
    "\n",
    "* max_len: Cümlenin maksimum uzunluğu (ör. 5000 token).\n",
    "\n",
    "* embed_dim: Her kelimenin embedding boyutu (ör. 512).\n",
    "\n",
    "* Başlangıçta sıfırlarla doldurulur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b38deb",
   "metadata": {},
   "source": [
    "### 🔹 Adım 2:\n",
    "```python\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.arange(0, max_len) → 0’dan max_len-1’e kadar bir sayı dizisi üretir.\n",
    "(örneğin [0, 1, 2, 3, ... 4999])\n",
    "\n",
    "* .unsqueeze(1) → 1 boyut ekler → (max_len, 1) haline getirir.\n",
    "\n",
    "- Bu vektör her pozisyonun indeksini temsil eder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1920d9",
   "metadata": {},
   "source": [
    "### 🔹 Adım 3:\n",
    "```python\n",
    "div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "```\n",
    "\n",
    "Bu kısım frekans ölçeklerini hesaplar:\n",
    "\n",
    "* torch.arange(0, embed_dim, 2) → 0, 2, 4, ... (her çift indeks için)\n",
    "\n",
    "* (-math.log(10000.0) / embed_dim) → sabit bir oran belirler.\n",
    "\n",
    "* torch.exp(...) → bu oranı üstel (exponential) fonksiyonla ölçeklendirir.\n",
    "\n",
    "👉 Amaç: Her boyutta farklı bir “dalga frekansı” oluşturmak.\n",
    "* Yani küçük boyutlar yavaş değişen sinüs/kosinüs, büyük boyutlar hızlı değişen frekans üretir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66581a3",
   "metadata": {},
   "source": [
    "### 🔹 Adım 4:\n",
    "```python\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Çift indeksler (0,2,4,...) için sinüs,\n",
    "\n",
    "* Tek indeksler (1,3,5,...) için kosinüs uygulanır.\n",
    "\n",
    "🔸 Sonuç:\n",
    "* Her pozisyon için embed_dim uzunluğunda benzersiz bir vektör elde edilir.\n",
    "* Bu vektörlerin sinüs ve kosinüs kombinasyonu, mutlak pozisyonu ve göreli farkları kodlar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd9e0",
   "metadata": {},
   "source": [
    "### 🔹 Adım 5:\n",
    "```python\n",
    "pe = pe.unsqueeze(0)\n",
    "self.register_buffer('pe', pe)\n",
    "```\n",
    "\n",
    "* .unsqueeze(0) → batch boyutu ekler: (1, max_len, embed_dim)\n",
    "\n",
    "* register_buffer → pe tensörünü modelin parametresi yapmaz, ama state_dict’e dahil eder.\n",
    "(yani requires_grad=False, ama modelle birlikte GPU’ya taşınır.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2bc2d",
   "metadata": {},
   "source": [
    "### 🔹 Adım 6:\n",
    "```python\n",
    "def forward(self, x):\n",
    "    seq_len = x.size(1)\n",
    "    return x + self.pe[:, :seq_len, :]\n",
    "```\n",
    "\n",
    "\n",
    "* Giriş x boyutu: (batch, seq_len, embed_dim)\n",
    "\n",
    "* self.pe[:, :seq_len, :] → sadece ilgili uzunluk kadar pozisyonel vektör alır.\n",
    "\n",
    "* x + self.pe → embedding’e pozisyon bilgisi eklenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04531ede",
   "metadata": {},
   "source": [
    "## 🧠 Ek Bilgi — torch Fonksiyonlarının Görevleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a5ab6",
   "metadata": {},
   "source": [
    "| Fonksiyon                  | Açıklama                                            |\n",
    "| -------------------------- | --------------------------------------------------- |\n",
    "| `torch.arange(start, end)` | Belirtilen aralıkta ardışık sayılar üretir.         |\n",
    "| `.unsqueeze(dim)`          | Belirtilen boyuta 1 ekler (ör. (5000,) → (5000,1)). |\n",
    "| `torch.exp(tensor)`        | Her elemanın e^x’ini (üstelini) alır.               |\n",
    "| `torch.sin`, `torch.cos`   | Trigonometrik sinüs ve kosinüs fonksiyonlarıdır.    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444abeee",
   "metadata": {},
   "source": [
    "### 🔸 Özet:\n",
    "\n",
    "Bu yapı sayesinde model, sıradaki token’ların:\n",
    "\n",
    "* sıralarını,\n",
    "\n",
    "* göreli uzaklıklarını,\n",
    "\n",
    "* ve bağlamsal ilişkilerini\n",
    "\n",
    "öğrenebilir hale gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c5309",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e209c7",
   "metadata": {},
   "source": [
    "## Soru 3-) FeedForward ne işe yarar ? Silu ve Gelu fonksiyonlarının farkı nedir? Neden böyle bir ayrışım yapıyoruz ? \n",
    "```python \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion=8, dp=0.1, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        if use_swiglu:\n",
    "            # SwiGLU activation\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion * 2),\n",
    "                nn.SiLU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim * expansion),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dp),\n",
    "                nn.Linear(embed_dim * expansion, embed_dim),\n",
    "                nn.Dropout(dp)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cec79",
   "metadata": {},
   "source": [
    "### 🔹 1️⃣ FeedForward (FFN) Katmanı Nedir?\n",
    "\n",
    "* Transformer’ın her encoder veya decoder bloğunun içinde, attention katmanından sonra yer alan bir tam bağlantılı (fully-connected) yapıdır.\n",
    "\n",
    "Amaç:\n",
    "\n",
    "➡️ Her token’ın (kelimenin) temsilini bağımsız olarak dönüştürmek,\n",
    "\n",
    "➡️ Modele nonlineerlik ve daha yüksek temsil gücü kazandırmak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b841c",
   "metadata": {},
   "source": [
    "### 🧠 Genel İşleyiş\n",
    "\n",
    "Girdi:\n",
    "* Attention katmanından gelen x (şekil: [batch_size, seq_len, embed_dim])\n",
    "\n",
    "İlk Linear:\n",
    "* Her token vektörünü embed_dim’den embed_dim * expansion boyutuna çıkarır.\n",
    "(Bu genişletme, bilgiyi daha zengin bir uzayda işlemeye yarar.)\n",
    "\n",
    "Aktivasyon Fonksiyonu (GELU veya SiLU):\n",
    "* Nonlineer dönüşüm sağlar — modelin karmaşık ilişkileri öğrenmesine yardımcı olur.\n",
    "\n",
    "Dropout:\n",
    "* Regularization, yani overfitting’i önleme.\n",
    "\n",
    "İkinci Linear:\n",
    "* Boyutu tekrar embed_dim’e indirir (geri sıkıştırır)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2ef02",
   "metadata": {},
   "source": [
    "### 🔹 2️⃣ SiLU ve GELU Farkı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c87b02",
   "metadata": {},
   "source": [
    "| Özellik               | **SiLU (Sigmoid Linear Unit)**                           | **GELU (Gaussian Error Linear Unit)**                            |\n",
    "| --------------------- | -------------------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Formül**            | `x * sigmoid(x)`                                         | `x * Φ(x)` <br> (Φ: normal dağılımın kümülatif fonksiyonu)       |\n",
    "| **Yaklaşım**          | Sigmoid’e dayalı, yumuşak ReLU benzeri                   | Gaussian olasılık temelli                                        |\n",
    "| **Davranış**          | Küçük pozitif girişlerde yumuşak, büyük pozitiflerde ≈ x | Benzer şekilde yumuşak ama Gaussian eğrisiyle ölçeklenir         |\n",
    "| **Grafiksel Görünüm** | Daha düz geçişli, stabil                                 | Hafif asimetrik, “daha doğal” aktivasyon sağlar                  |\n",
    "| **Kullanım**          | Genelde **SwiGLU** gibi kapı mekanizmalarında            | Transformer modellerinde (örneğin BERT, GPT, ViT) sık kullanılır |\n",
    "| **Avantaj**           | Hesaplaması daha ucuz ve kararlı                         | Empirik olarak bazı modellerde daha yüksek doğruluk sağlar       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cb928",
   "metadata": {},
   "source": [
    "### 🔹 3️⃣ Neden “SwiGLU” Ayrımı Yapıyoruz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd43b10",
   "metadata": {},
   "source": [
    "Kodda bir parametre var:\n",
    "\n",
    "```python\n",
    "use_swiglu=True\n",
    "```\n",
    "\n",
    "\n",
    "* Bu, FeedForward içinde farklı aktivasyon kombinasyonlarını denemeye yarıyor.\n",
    "\n",
    "### 🧩 SwiGLU Nedir?\n",
    "\n",
    "* SwiGLU (Swish-Gated Linear Unit), SiLU temelli bir kapılı (gated) yapı.\n",
    "Burada Linear katmandan iki ayrı parça çıkarılır:\n",
    "\n",
    "* biri gate (kapı),\n",
    "\n",
    "* diğeri değer (value) kısmı."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979a5a0",
   "metadata": {},
   "source": [
    "Yani:\n",
    "\n",
    "* y = (x * W1) * SiLU(x * W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13795c8e",
   "metadata": {},
   "source": [
    "* Bu yapı, modelin bilgiyi seçici olarak geçirmesine olanak tanır —\n",
    "tıpkı attention’daki “hangi bilgi önemli?” mantığı gibi.\n",
    "\n",
    "### Avantaj:\n",
    "\n",
    "* Daha iyi bilgi akışı,\n",
    "\n",
    "* Daha az “ölü nöron”,\n",
    "\n",
    "* Düşük hesaplama maliyetiyle yüksek performans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f61ccf",
   "metadata": {},
   "source": [
    "### 🔹 4️⃣ Özetle Adım Adım İşleyiş"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457d510",
   "metadata": {},
   "source": [
    "| Adım | İşlem                                         | Açıklama                              |\n",
    "| ---- | --------------------------------------------- | ------------------------------------- |\n",
    "| 1    | `nn.Linear(embed_dim, embed_dim * expansion)` | Boyutu genişletir (bilgi uzayı büyür) |\n",
    "| 2    | `nn.GELU()` veya `nn.SiLU()`                  | Nonlineer dönüşüm                     |\n",
    "| 3    | `nn.Dropout(dp)`                              | Overfitting önleme                    |\n",
    "| 4    | `nn.Linear(embed_dim * expansion, embed_dim)` | Boyutu geri indirir                   |\n",
    "| 5    | `nn.Dropout(dp)`                              | Son düzenleme, regularization         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589bd88",
   "metadata": {},
   "source": [
    "### 🔹 5️⃣ Kısa Sonuç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e3d4f",
   "metadata": {},
   "source": [
    "| Parametre    | Anlamı                                                             |\n",
    "| ------------ | ------------------------------------------------------------------ |\n",
    "| `embed_dim`  | Girdi vektör boyutu                                                |\n",
    "| `expansion`  | Katman genişleme oranı (genelde 4–8 arası)                         |\n",
    "| `dp`         | Dropout oranı                                                      |\n",
    "| `use_swiglu` | True ise SiLU tabanlı SwiGLU kullanılır, False ise GELU klasik FFN |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8980638",
   "metadata": {},
   "source": [
    "```bash\n",
    " ┌────────────────────────────┐\n",
    " │   Girdi: x (batch, seq, embed_dim) │\n",
    " └──────────────┬─────────────┘\n",
    "                │\n",
    "                ▼\n",
    "      ┌────────────────────┐\n",
    "      │ Linear (embed_dim → embed_dim * expansion) │\n",
    "      └────────────────────┘\n",
    "                │\n",
    "                ▼\n",
    "          ┌────────────┐\n",
    "          │  GELU      │   ← Nonlineer dönüşüm (Gaussian temelli)\n",
    "          └────────────┘\n",
    "                │\n",
    "                ▼\n",
    "          ┌────────────┐\n",
    "          │ Dropout(dp)│\n",
    "          └────────────┘\n",
    "                │\n",
    "                ▼\n",
    "      ┌────────────────────┐\n",
    "      │ Linear (embed_dim * expansion → embed_dim) │\n",
    "      └────────────────────┘\n",
    "                │\n",
    "                ▼\n",
    "          ┌────────────┐\n",
    "          │ Dropout(dp)│\n",
    "          └────────────┘\n",
    "                │\n",
    "                ▼\n",
    " ┌────────────────────────────┐\n",
    " │  Çıktı: y (aynı boyut: embed_dim) │\n",
    " └────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae24c39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271a993",
   "metadata": {},
   "source": [
    "## Soru 4-) TransformersEncoderBlockLLM sınıfı ne işe yarıyor.Gamma parametreleri nedir ? Çarpım için kullanılan 1e-2 gibi değerler azalsa ya da artsa nasıl bir değişim beklenir? Ayrıca bu classın forward fonksiyonunda ne gibi işlemler yapılıyor ? \n",
    "```python\n",
    "class TransformerEncoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41715c5f",
   "metadata": {},
   "source": [
    "### ⚙️ Soru 4 - TransformerEncoderBlockLLM\n",
    "#### 1️⃣ Amaç ve İşlevi\n",
    "\n",
    "* TransformerEncoderBlockLLM sınıfı, Transformer mimarisindeki tek bir encoder bloğunu temsil eder.\n",
    "\n",
    "Bir bloğun temel işlemleri şunlardır:\n",
    "\n",
    "* Self-Attention: Girdi sırasındaki her token’ın diğer token’larla ilişkisini öğrenir.\n",
    "\n",
    "* FeedForward (FFN): Token başına öğrenilen özellikleri dönüştürür ve model kapasitesini artırır.\n",
    "\n",
    "- LayerNorm ve DropPath: Eğitimi stabilize eder ve düzenliliği artırır.\n",
    "\n",
    "Kısaca:\n",
    "\n",
    "* “Girdi → Normalizasyon → Self-Attention → Skip Connection → FFN → Skip Connection → Çıkış”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4729204",
   "metadata": {},
   "source": [
    "#### 2️⃣ gamma_1 ve gamma_2 Parametreleri\n",
    "```python\n",
    "self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "* Bunlar ölçekleme faktörleri olarak çalışır.\n",
    "\n",
    "* attn_out ve ffn_out değerlerine çarpılır önce DropPath, sonra residual ekleme yapılır:\n",
    "```python\n",
    "\n",
    "x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "```\n",
    "\n",
    "\n",
    "Amaç: residual bağlantıdaki katkıyı başlangıçta küçük tutmak, böylece öğrenme daha stabil olur.\n",
    "\n",
    "* Eğer 1e-2 değeri artarsa: attention/FFN çıktısı daha fazla eklenir → gradient daha hızlı yayılır → risk: instabilite, patlayan değerler.\n",
    "\n",
    "* Eğer azalırsa: blok çıktısı daha az katkıda bulunur → yavaş öğrenme, yavaş adaptasyon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f684f",
   "metadata": {},
   "source": [
    "#### 3️⃣ Forward Fonksiyonu Adım Adım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa129b06",
   "metadata": {},
   "source": [
    "```python \n",
    "def forward(self, x, mask=None):\n",
    "    # 1️⃣ Normalizasyon\n",
    "    attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "    \n",
    "    # 2️⃣ DropPath + Gamma çarpımı + Residual\n",
    "    x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "    # 3️⃣ FeedForward\n",
    "    ffn_out = self.ffn(self.norm2(x))\n",
    "    \n",
    "    # 4️⃣ DropPath + Gamma çarpımı + Residual\n",
    "    x = x + self.drop_path(self.gamma_2 * ffn_out)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc04ae8",
   "metadata": {},
   "source": [
    "| Adım | İşlem                                              | Amaç                                        |\n",
    "| ---- | -------------------------------------------------- | ------------------------------------------- |\n",
    "| 1    | `self.norm1(x)`                                    | LayerNorm ile input stabilizasyonu          |\n",
    "| 2    | `self.self_attn(...)`                              | Token’lar arası ilişkileri öğrenme          |\n",
    "| 3    | `self.gamma_1 * attn_out`                          | Residual katkısını ölçekleme                |\n",
    "| 4    | `self.drop_path(...)`                              | Stochastic Depth, blok rastgele atlanabilir |\n",
    "| 5    | `x + ...`                                          | Residual bağlantı ekleme                    |\n",
    "| 6    | `self.norm2(x)` → `self.ffn(...)`                  | FeedForward, token başına dönüşüm           |\n",
    "| 7    | `self.gamma_2 * ffn_out` + `drop_path` + `x + ...` | Residual + Stabilizasyon                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5fa06",
   "metadata": {},
   "source": [
    "#### 4️⃣ Özet\n",
    "\n",
    "* TransformerEncoderBlockLLM: tek encoder bloğu, Self-Attention + FFN + Residual + LayerNorm + DropPath içerir.\n",
    "\n",
    "* gamma_1 ve gamma_2: residual katkıyı başlangıçta küçük tutar. Değeri değiştirirsen stabilite ve öğrenme hızı değişir.\n",
    "\n",
    "```bash4\n",
    "* forward:\n",
    "\n",
    "* LayerNorm\n",
    "\n",
    "* Self-Attention → scaled + drop_path → residual\n",
    "\n",
    "* FeedForward → scaled + drop_path → residual\n",
    "\n",
    "* Çıkış"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac398b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53581d",
   "metadata": {},
   "source": [
    "## Soru 5-) TransformerDecoderBlockLLM sınıfında bulunan işlem adımları nedir?Oluşturulan gamma parametreleri ne işe yarar? Self Attention ve Cross Attention aynı class içerisinde ise modele self kullanacağını ya da cross attention kullanacağını nasıl aktarıyoruz ? \n",
    "```python\n",
    "\n",
    "class TransformerDecoderBlockLLM(nn.Module):\n",
    "    def __init__(self, embed_dim=1024, num_heads=16, dp=0.1, drop_path=0.1, expansion=8, use_swiglu=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads, dp)\n",
    "        self.ffn = FeedForward(embed_dim, expansion, dp, use_swiglu)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "        self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "\n",
    "    def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "        x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "        # Cross-Attention\n",
    "        if enc_out is not None:\n",
    "            cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "            x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "        # FeedForward\n",
    "        ffn_out = self.ffn(self.norm3(x))\n",
    "        x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1d17b",
   "metadata": {},
   "source": [
    "### ⚙️ Soru 5 - TransformerDecoderBlockLLM\n",
    "#### 1️⃣ Amaç ve İşlevi\n",
    "\n",
    "* TransformerDecoderBlockLLM, tek bir decoder bloğunu temsil eder ve üç ana bileşenden oluşur:\n",
    "\n",
    "* Masked Self-Attention → Decoder kendi önceki token’larını görebilir, geleceği göremez (mask ile).\n",
    "\n",
    "* Cross-Attention → Encoder çıkışını kullanarak decoder token’larını context ile ilişkilendirir.\n",
    "\n",
    "* FeedForward (FFN) → Token başına dönüşüm ve özellik zenginleştirme.\n",
    "\n",
    "Ek olarak, LayerNorm, DropPath ve gamma parametreleri stabilite ve residual katkısı için kullanılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944420fa",
   "metadata": {},
   "source": [
    "### 2️⃣ Gamma Parametreleri\n",
    "```python\n",
    "self.gamma_1 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "self.gamma_2 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "self.gamma_3 = nn.Parameter(torch.ones(embed_dim) * 1e-2)\n",
    "```\n",
    "\n",
    "\n",
    "Her gamma, residual bağlantının katkısını ölçekler.\n",
    "\n",
    "* gamma_1 → Self-Attention çıktısı\n",
    "\n",
    "* gamma_2 → Cross-Attention çıktısı\n",
    "\n",
    "* gamma_3 → FeedForward çıktısı\n",
    "\n",
    "> Küçük değerler (1e-2) → başlangıçta residual katkısı düşük → stabil öğrenme.\n",
    "\n",
    "> Büyük değerler → daha hızlı katkı, risk: instabilite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0f49f",
   "metadata": {},
   "source": [
    "### 3️⃣ Forward Fonksiyon Adım Adım"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6bf03",
   "metadata": {},
   "source": [
    "```python \n",
    "def forward(self, x, enc_out=None, self_mask=None, enc_mask=None):\n",
    "    # 1️⃣ Masked Self-Attention\n",
    "    attn_out = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), self_mask)\n",
    "    x = x + self.drop_path(self.gamma_1 * attn_out)\n",
    "\n",
    "    # 2️⃣ Cross-Attention\n",
    "    if enc_out is not None:\n",
    "        cross_out = self.cross_attn(self.norm2(x), self.norm2(enc_out), self.norm2(enc_out), enc_mask)\n",
    "        x = x + self.drop_path(self.gamma_2 * cross_out)\n",
    "\n",
    "    # 3️⃣ FeedForward\n",
    "    ffn_out = self.ffn(self.norm3(x))\n",
    "    x = x + self.drop_path(self.gamma_3 * ffn_out)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff69a3",
   "metadata": {},
   "source": [
    "| Adım | İşlem                                             | Amaç                                                           |\n",
    "| ---- | ------------------------------------------------- | -------------------------------------------------------------- |\n",
    "| 1    | `self.norm1(x)` + `self.self_attn(...)`           | Decoder kendi geçmiş token’larıyla ilişki kurar (masked)       |\n",
    "| 2    | `x + drop_path(gamma_1 * attn_out)`               | Residual bağlantı ve DropPath ile stabilizasyon                |\n",
    "| 3    | `if enc_out is not None` → `self.cross_attn(...)` | Encoder’dan gelen bağlam bilgisi ile token’ları ilişkilendirir |\n",
    "| 4    | `x + drop_path(gamma_2 * cross_out)`              | Cross-Attention residual eklenir                               |\n",
    "| 5    | `self.norm3(x)` + `ffn(...)`                      | FeedForward ile token özellikleri zenginleştirilir             |\n",
    "| 6    | `x + drop_path(gamma_3 * ffn_out)`                | FFN residual eklenir ve çıktı döner                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce27ae",
   "metadata": {},
   "source": [
    "### 4️⃣ Self-Attention ve Cross-Attention Kullanımı\n",
    "\n",
    "* Self-Attention her zaman çalışır, decoder kendi geçmiş token’larını görür.\n",
    "\n",
    "* Cross-Attention sadece enc_out verilmişse çalışır:\n",
    "\n",
    "```python\n",
    "if enc_out is not None:\n",
    "    cross_out = self.cross_attn(...)\n",
    "```\n",
    "\n",
    "\n",
    "* Yani modele hangi attention’ı kullanacağını enc_out parametresi ile aktarıyoruz.\n",
    "\n",
    "* self_mask → self attention maskesi (geleceği görmeyi engeller)\n",
    "\n",
    "* enc_mask → encoder maskesi (padding tokenlarını gizler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010772c5",
   "metadata": {},
   "source": [
    "### 5️⃣ Özet\n",
    "\n",
    "* TransformerDecoderBlockLLM: Masked Self-Attention + Cross-Attention + FFN\n",
    "\n",
    "* Gamma parametreleri → residual katkıyı ölçekler, stabilite sağlar\n",
    "\n",
    "* Self ve Cross Attention aynı class içinde, cross_attention opsiyoneldir (enc_out varsa çalışır)\n",
    "\n",
    "* DropPath → bloğu rastgele atlayarak regularization sağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff402e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8515604f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
