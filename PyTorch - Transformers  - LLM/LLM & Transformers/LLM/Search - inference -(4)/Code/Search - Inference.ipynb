{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d64bcd",
   "metadata": {},
   "source": [
    "# greedy_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec15e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27331f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, enc_out, start_token_id, max_len=50, enc_mask=None, device='cuda'):\n",
    "    batch_size = enc_out.size(0)\n",
    "    ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9853d",
   "metadata": {},
   "source": [
    "### ⚙️ Adım Adım Açıklama\n",
    "\n",
    "### 1️⃣ Başlangıç Tokeni Oluşturma\n",
    "```python\n",
    "\n",
    "ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "```\n",
    "\n",
    "\n",
    "* Her örnek için decoder’ın başlayacağı token (<BOS> veya start_token_id) oluşturulur.\n",
    "\n",
    "* batch_size kadar satır, 1 token uzunluğunda sütun.\n",
    "\n",
    "### 2️⃣ Token Üretim Döngüsü\n",
    "\n",
    "```python\n",
    "for _ in range(max_len - 1):\n",
    "```\n",
    "\n",
    "\n",
    "* Maksimum max_len kadar token üretmek için döngü.\n",
    "\n",
    "* -1, çünkü başlangıç tokeni zaten eklenmiş durumda.\n",
    "\n",
    "### 3️⃣ Decoder’dan Logitleri Almak\n",
    "\n",
    "```python\n",
    "logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder, mevcut ys dizisini kullanarak sonraki token olasılıklarını (logits) üretir.\n",
    "\n",
    "* enc_out → Encoder çıktısı (cross-attention için)\n",
    "\n",
    "* enc_mask → Encoder attention maskesi\n",
    "\n",
    "### 4️⃣ En Yüksek Olasılığı Seçmek (Greedy)\n",
    "\n",
    "```python\n",
    "next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "```\n",
    "\n",
    "\n",
    "* logits[:, -1, :] → Sadece en son üretilen tokenin olasılıkları alınır.\n",
    "\n",
    "* torch.argmax → En yüksek olasılığa sahip token seçilir (tam Greedy).\n",
    "\n",
    "### 5️⃣ Yeni Tokeni Mevcut Dizinin Sonuna Eklemek\n",
    "```python\n",
    "ys = torch.cat([ys, next_token], dim=1)\n",
    "```\n",
    "\n",
    "\n",
    "* Mevcut token dizisine yeni token eklenir.\n",
    "\n",
    "* Döngü tekrar ettiğinde decoder yeni diziyi kullanır.\n",
    "\n",
    "### 6️⃣ Sonuç\n",
    "\n",
    "* Döngü tamamlandığında ys → tüm üretilmiş token dizilerini içerir.\n",
    "\n",
    "* Greedy strateji her zaman en yüksek olasılığı seçer, bu yüzden deterministic ve hızlıdır.\n",
    "\n",
    "## 💡 Not:\n",
    "\n",
    "* Avantaj: Basit, hızlı, deterministic.\n",
    "\n",
    "* Dezavantaj: “Daha iyi uzun cümleler” veya “yaratıcı” sonuç üretmekte zayıf olabilir.\n",
    "\n",
    "* Daha yaratıcı/çeşitlilik için Top-K / Top-p sampling gerekir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a680b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d817c64",
   "metadata": {},
   "source": [
    "# top_k_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29339286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_k_sampling(model, enc_out, enc_mask, start_token_id, max_len=50, k=10, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Top-K Sampling ile token üretimi\n",
    "    model      : Decoder modeli\n",
    "    enc_out    : Encoder çıktısı (cross-attention için)\n",
    "    enc_mask   : Encoder attention maskesi\n",
    "    start_token_id : Başlangıç token ID\n",
    "    max_len    : Maksimum üretilen token sayısı\n",
    "    k          : Top-K değeri\n",
    "    device     : \"cuda\" veya \"cpu\"\n",
    "    \"\"\"\n",
    "    batch_size = enc_out.size(0)\n",
    "    ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        # Decoder’dan logitleri al\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        \n",
    "        # Son token logitleri\n",
    "        logits_last = logits[:, -1, :]\n",
    "        \n",
    "        # Top-K maskesi uygula\n",
    "        topk_values, topk_indices = torch.topk(logits_last, k=k, dim=-1)\n",
    "        probs = torch.zeros_like(logits_last)\n",
    "        probs.scatter_(-1, topk_indices, torch.softmax(topk_values, dim=-1))\n",
    "        \n",
    "        # Top-K üzerinden rastgele token seç\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Yeni tokeni mevcut dizinin sonuna ekle\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd42dc",
   "metadata": {},
   "source": [
    "### ⚙️ Adım Adım Açıklama\n",
    "#### 1️⃣ Başlangıç Tokeni Oluşturma\n",
    "```python\n",
    "ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder’ın başlayacağı token (<BOS> veya start_token_id) oluşturulur.\n",
    "\n",
    "* Her örnek için bir satır, 1 token uzunluğunda sütun.\n",
    "\n",
    "* Batch boyutu kadar token dizisi başlatılır.\n",
    "\n",
    "### 2️⃣ Token Üretim Döngüsü\n",
    "```python\n",
    "for _ in range(max_len - 1):\n",
    "```\n",
    "\n",
    "\n",
    "* Maksimum max_len kadar token üretmek için döngü.\n",
    "\n",
    "* -1 çünkü başlangıç tokeni zaten eklenmiş durumda.\n",
    "\n",
    "#### 3️⃣ Decoder’dan Logitleri Almak\n",
    "```python\n",
    "logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder mevcut ys dizisini kullanarak sonraki token olasılıklarını (logits) üretir.\n",
    "\n",
    "* enc_out → Encoder çıktısı (cross-attention için).\n",
    "\n",
    "* enc_mask → Encoder attention maskesi.\n",
    "\n",
    "#### 4️⃣ Son Token Logitlerini Al\n",
    "```python\n",
    "logits_last = logits[:, -1, :]\n",
    "```\n",
    "\n",
    "\n",
    "* Sadece en son üretilen tokenin logitleri alınır.\n",
    "\n",
    "* Önceki tokenler dikkate alınmaz, çünkü bu adımda bir sonraki token üretiliyor.\n",
    "\n",
    "#### 5️⃣ Top-K Seçimi\n",
    "```python\n",
    "topk_values, topk_indices = torch.topk(logits_last, k=k, dim=-1)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.topk ile en yüksek K olasılığa sahip tokenler seçilir.\n",
    "\n",
    "* topk_values → olasılık değerleri\n",
    "\n",
    "* topk_indices → token ID’leri\n",
    "\n",
    "#### 6️⃣ Olasılıkları Normalize Et\n",
    "```python\n",
    "probs = torch.zeros_like(logits_last)\n",
    "probs.scatter_(-1, topk_indices, torch.softmax(topk_values, dim=-1))\n",
    "```\n",
    "\n",
    "\n",
    "* Top-K tokenler dışındaki tüm tokenler sıfırlanır.\n",
    "\n",
    "* torch.softmax ile seçilen K tokenin olasılıkları normalize edilir.\n",
    "\n",
    "* scatter_ ile olasılık tensorüne yerleştirilir.\n",
    "\n",
    "### 7️⃣ Rastgele Token Seçimi\n",
    "```python\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.multinomial ile olasılıklara göre rastgele token seçilir.\n",
    "\n",
    "* Yani en yüksek olasılığa sahip token her zaman seçilmez, bu sayede çeşitlilik sağlanır.\n",
    "\n",
    "#### 8️⃣ Tokeni Mevcut Dizinin Sonuna Ekle\n",
    "```python\n",
    "ys = torch.cat([ys, next_token], dim=1)\n",
    "```\n",
    "\n",
    "* Yeni token, mevcut token dizisinin sonuna eklenir.\n",
    "\n",
    "* Döngü tekrar ettiğinde decoder, güncellenmiş diziyi kullanır.\n",
    "\n",
    "#### 9️⃣ Döngü Sonrası\n",
    "\n",
    "* Döngü tamamlandığında ys → tüm üretilmiş token dizilerini içerir.\n",
    "\n",
    "* Top-K Sampling, yaratıcı ve çeşitli sonuçlar üretmek için deterministic olmayan bir yöntemdir.\n",
    "\n",
    "💡 Notlar:\n",
    "\n",
    "* k değeri küçüldükçe → daha deterministic (daha az çeşitlilik)\n",
    "\n",
    "* k değeri büyüdükçe → daha yaratıcı ve çeşitli çıktı\n",
    "\n",
    "* Greedy’ye göre daha fazla rastgelelik sağlar ama olasılık hesaplama biraz daha maliyetlidir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed1fca",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac29fca",
   "metadata": {},
   "source": [
    "# 🔹 Top-p Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(model, enc_out, enc_mask, start_token_id, max_len=20, p=0.9, device=\"cpu\"):\n",
    "    batch_size = enc_out.size(0)\n",
    "    ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        logits_last = logits[:, -1, :]  # Son token logitleri\n",
    "\n",
    "        # Softmax ile olasılıkları hesapla\n",
    "        probs = torch.softmax(logits_last, dim=-1)\n",
    "\n",
    "        # Top-P maskesi oluştur\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "        cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cum_probs > p\n",
    "\n",
    "        # İlk True değer dışındakileri kapat\n",
    "        mask[:, 1:] = mask[:, :-1].clone()\n",
    "        mask[:, 0] = False\n",
    "\n",
    "        # P’nin dışındaki tokenleri sıfırla\n",
    "        sorted_probs[mask] = 0.0\n",
    "\n",
    "        # Normalize et\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Tokenleri geri yerleştir\n",
    "        probs.zero_()\n",
    "        probs.scatter_(-1, sorted_indices, sorted_probs)\n",
    "\n",
    "        # Rastgele token seç\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Tokeni mevcut dizinin sonuna ekle\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679bebb",
   "metadata": {},
   "source": [
    "### ⚙️ Adım Adım Açıklama (Markdown Formatı)\n",
    "#### 1️⃣ Başlangıç Tokeni Oluşturma\n",
    "```PYTHON\n",
    "ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder’ın başlayacağı token (<BOS> veya start_token_id) oluşturulur.\n",
    "\n",
    "* Her örnek için bir satır, 1 token uzunluğunda sütun.\n",
    "\n",
    "* Batch boyutu kadar token dizisi başlatılır.\n",
    "\n",
    "#### 2️⃣ Token Üretim Döngüsü\n",
    "```python\n",
    "for _ in range(max_len - 1):\n",
    "```\n",
    "\n",
    "\n",
    "* Maksimum max_len kadar token üretmek için döngü.\n",
    "\n",
    "* -1 çünkü başlangıç tokeni zaten eklenmiş durumda.\n",
    "\n",
    "#### 3️⃣ Decoder’dan Logitleri Almak\n",
    "```python\n",
    "logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder mevcut ys dizisini kullanarak sonraki token olasılıklarını (logits) üretir.\n",
    "\n",
    "* enc_out → Encoder çıktısı (cross-attention için).\n",
    "\n",
    "* enc_mask → Encoder attention maskesi.\n",
    "\n",
    "### 4️⃣ Son Token Logitlerini Al\n",
    "```python\n",
    "logits_last = logits[:, -1, :]\n",
    "```\n",
    "\n",
    "\n",
    "* Sadece en son üretilen tokenin logitleri alınır.\n",
    "\n",
    "#### 5️⃣ Olasılıkları Hesapla\n",
    "```python\n",
    "probs = torch.softmax(logits_last, dim=-1)\n",
    "```\n",
    "\n",
    "\n",
    "* Logitleri olasılıklara dönüştürmek için softmax uygulanır.\n",
    "\n",
    "#### 6️⃣ Top-P Maskesi Oluştur\n",
    "```python\n",
    "sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "mask = cum_probs > p\n",
    "```\n",
    "\n",
    "\n",
    "* Tokenleri olasılıklarına göre sırala (yüksekten düşüğe).\n",
    "\n",
    "* Kümülatif olasılık (cum_probs) hesaplanır.\n",
    "\n",
    "* Kümülatif olasılık p değerini aşan tokenler maskelenir.\n",
    "\n",
    "#### 7️⃣ P Dışındaki Tokenleri Sıfırla\n",
    "```python\n",
    "mask[:, 1:] = mask[:, :-1].clone()\n",
    "mask[:, 0] = False\n",
    "sorted_probs[mask] = 0.0\n",
    "```\n",
    "\n",
    "* İlk tokenin dışındaki aşan kümülatif olasılıkları sıfırla.\n",
    "\n",
    "* Bu şekilde olasılık dağılımı Nucleus (Top-P) içinde kalır.\n",
    "\n",
    "#### 8️⃣ Normalize Et\n",
    "```python\n",
    "sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "probs.zero_()\n",
    "probs.scatter_(-1, sorted_indices, sorted_probs)\n",
    "```\n",
    "\n",
    "\n",
    "* Top-P içerisindeki token olasılıkları normalize edilir.\n",
    "\n",
    "* Scatter ile orijinal token pozisyonlarına geri yerleştirilir.\n",
    "\n",
    "#### 9️⃣ Rastgele Token Seçimi\n",
    "```python\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "\n",
    "* Softmax olasılıklarına göre rastgele token seçilir.\n",
    "\n",
    "* Böylece, her zaman en yüksek olasılık seçilmez → yaratıcı ve çeşitli çıktılar sağlanır.\n",
    "\n",
    "#### 10️⃣ Tokeni Mevcut Dizinin Sonuna Ekle\n",
    "```python\n",
    "ys = torch.cat([ys, next_token], dim=1)\n",
    "```\n",
    "\n",
    "\n",
    "* Yeni token, mevcut token dizisinin sonuna eklenir.\n",
    "\n",
    "#### 11️⃣ Döngü Sonrası\n",
    "\n",
    "* Döngü tamamlandığında ys → tüm üretilmiş token dizilerini içerir.\n",
    "\n",
    "* Top-P Sampling, Top-K’den farklı olarak belirli bir olasılık kütlesini (p) kapsar.\n",
    "\n",
    "💡 Notlar:\n",
    "\n",
    "* p değeri küçüldükçe → daha deterministic, daha az çeşitlilik\n",
    "\n",
    "* p değeri büyüdükçe → daha yaratıcı ve çeşitli çıktı\n",
    "\n",
    "Greedy’ye göre çok daha rastgele ve yaratıcı sonuçlar sağlar, özellikle uzun metin üretiminde tercih edilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34d31b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ef2ad",
   "metadata": {},
   "source": [
    "# 🔹 Top-K ve Top-P Karşılaştırma Tablosu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456b02c",
   "metadata": {},
   "source": [
    "| Özellik                 | Top-K Sampling                                                          | Top-P (Nucleus) Sampling                                                           |\n",
    "| ----------------------- | ----------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **Seçim Kriteri**       | En yüksek K olasılığa sahip tokenler arasından rastgele seçim           | Kümülatif olasılığı `p`’yi aşmayan tokenler arasından rastgele seçim               |\n",
    "| **Deterministik mi?**   | Hayır, rastgelelik mevcut                                               | Hayır, rastgelelik mevcut                                                          |\n",
    "| **Kontrol Parametresi** | `k` (seçilecek en yüksek olasılıklı token sayısı)                       | `p` (toplam kümülatif olasılık eşiği, 0-1)                                         |\n",
    "| **Avantaj**             | Basit, hızlı, çeşitlilik kolayca kontrol edilebilir                     | Olasılık dağılımına göre dinamik seçim → daha doğal ve yaratıcı metin üretimi      |\n",
    "| **Dezavantaj**          | Sabit K nedeniyle bazen çok sıkıcı veya çok riskli tokenler seçilebilir | Hesaplama biraz daha maliyetli, p seçimi hassas                                    |\n",
    "| **Yaratıcılık**         | Orta → K küçükse deterministic, büyükse çeşitlilik artar                | Yüksek → doğal ve çeşitli çıktılar sağlar, özellikle uzun metinlerde tercih edilir |\n",
    "| **Tipik Kullanım**      | Kısa metin, basit cevaplar, hızlı inference                             | Uzun metin, yaratıcı üretim, doğal dil üretimi                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25a8b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034f3ee",
   "metadata": {},
   "source": [
    "# LLM ve transformer tabanlı modeller için hem Top-K hem Top-P Sampling’i destekleyen, esnek ve dinamik bir Python fonksiyon seti hazırlayalım. Kod, batch desteği ve GPU uyumluluğu içeriyor olsun. Ayrıca temperature parametresi ile sıcaklık kontrolü de ekleyelim; böylece daha yaratıcı veya deterministik sonuçlar elde edebiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4a9b7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6e2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_logits(logits, top_k=None, top_p=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Logitlerden token seçimi için Top-K ve Top-P Sampling.\n",
    "\n",
    "    Args:\n",
    "        logits: [batch_size, vocab_size] tensor\n",
    "        top_k: int, en yüksek K olasılığa sahip tokenler\n",
    "        top_p: float, kümülatif olasılık eşiği (0-1)\n",
    "        temperature: float, 1.0 varsayılan, <1 deterministik, >1 çeşitlilik\n",
    "\n",
    "    Returns:\n",
    "        next_token: [batch_size, 1] seçilen tokenler\n",
    "    \"\"\"\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Top-K Filtering\n",
    "    if top_k is not None and top_k > 0:\n",
    "        topk_values, topk_indices = torch.topk(probs, top_k, dim=-1)\n",
    "        probs_filtered = torch.zeros_like(probs).scatter_(-1, topk_indices, topk_values)\n",
    "        probs = probs_filtered\n",
    "\n",
    "    # Top-P (Nucleus) Filtering\n",
    "    if top_p is not None and 0.0 < top_p < 1.0:\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        # Mask dışındaki tokenleri sıfırla\n",
    "        sorted_mask = cumulative_probs > top_p\n",
    "        # İlk maskeli token dahil edilmez\n",
    "        sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "        sorted_mask[..., 0] = 0\n",
    "        sorted_probs[sorted_mask] = 0.0\n",
    "        # Tensorü orijinal sıraya geri döndür\n",
    "        probs = torch.zeros_like(probs).scatter_(-1, sorted_indices, sorted_probs)\n",
    "\n",
    "    # Normalize edelim\n",
    "    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Token seç\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def generate_sequence(model, start_tokens, max_len=50, top_k=None, top_p=None, temperature=1.0, enc_out=None, enc_mask=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    LLM Decoder için esnek Top-K / Top-P / Temperature sampling üretimi.\n",
    "    Args:\n",
    "        model: decoder modeli\n",
    "        start_tokens: [batch_size, 1] başlangıç tokenleri\n",
    "        max_len: üretilecek token sayısı\n",
    "        top_k: Top-K parametresi\n",
    "        top_p: Top-P parametresi\n",
    "        temperature: sıcaklık kontrolü\n",
    "        enc_out: encoder çıktısı (cross-attention)\n",
    "        enc_mask: encoder attention maskesi\n",
    "        device: cpu/cuda\n",
    "\n",
    "    Returns:\n",
    "        ys: [batch_size, max_len] üretilmiş tokenler\n",
    "    \"\"\"\n",
    "    ys = start_tokens.to(device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        logits_last = logits[:, -1, :]  # Sadece en son token\n",
    "        next_token = sample_logits(logits_last, top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1dc3",
   "metadata": {},
   "source": [
    "## ⚡ Özellikler ve Avantajlar\n",
    "\n",
    "* Hem Top-K hem Top-P\n",
    "\n",
    "Aynı fonksiyon içinde seçilebilir.\n",
    "\n",
    "* Temperature kontrolü\n",
    "\n",
    "temperature<1 → deterministik, güvenli seçimler\n",
    "\n",
    "temperature>1 → daha yaratıcı, çeşitlilik artar\n",
    "\n",
    "* Batch desteği\n",
    "\n",
    "Tek seferde tüm batch için token üretir.\n",
    "\n",
    "* Encoder uyumlu\n",
    "\n",
    "Cross-attention (enc_out ve enc_mask) parametreleri ile seq2seq modelleri için hazır.\n",
    "\n",
    "* Dinamik seçim\n",
    "\n",
    "Top-K veya Top-P tek başına veya birlikte kullanılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b0d21",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7a49f",
   "metadata": {},
   "source": [
    "# 📍 Kodun Genel Yapısında “Search” Nerede Olur?\n",
    "## 🔹 1. Model tanımı (Encoder–Decoder)\n",
    "\n",
    "* Önce zaten yaptığımız gibi:\n",
    "\n",
    "```bash\n",
    "encoder = Encoder(...)\n",
    "decoder = Decoder(...)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "```\n",
    "\n",
    "\n",
    "* Bu kısımda modelin yapısı tanımlanır.\n",
    "* Henüz herhangi bir “search” stratejisi uygulanmaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93170f9",
   "metadata": {},
   "source": [
    "##🔹 2. Train Loop (Eğitim)\n",
    "\n",
    "* Burada model, gerçek hedef dizilerini (“teacher forcing” ile) öğrenir.\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        enc_out = encoder(src)\n",
    "        outputs = decoder(trg_inp, enc_out)\n",
    "        loss = criterion(outputs, trg_labels)\n",
    "        ...\n",
    "```\n",
    "\n",
    "\n",
    "* Burada “search” YOK çünkü:\n",
    "\n",
    "* Eğitim sırasında hedef token’lar zaten bilinir.\n",
    "Model tahmin etmez, öğrenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4ad41",
   "metadata": {},
   "source": [
    "## 🔹 3. Search / Generation (Inference veya Evaluation Aşaması)\n",
    "\n",
    "* İşte bizim yazdığımız Top-K / Top-P / Greedy kodu burada devreye girer.\n",
    "\n",
    "* Yani modeli eğittikten sonra:\n",
    "```python\n",
    "\n",
    "# ---- Inference / Generation ----\n",
    "model.eval()\n",
    "\n",
    "start_tokens = torch.tensor([[tokenizer.bos_token_id]]).to(device)\n",
    "\n",
    "# LLM veya Transformer üretim aşaması\n",
    "generated = generate_sequence(\n",
    "    model=model.decoder,   # sadece decoder veya tam seq2seq\n",
    "    start_tokens=start_tokens,\n",
    "    max_len=50,\n",
    "    top_k=50,              # veya None\n",
    "    top_p=0.9,             # veya None\n",
    "    temperature=0.8,\n",
    "    enc_out=encoder_output, # encoder varsa\n",
    "    enc_mask=encoder_mask,  # encoder maskesi\n",
    "    device=device\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(generated[0])\n",
    "print(decoded_text)\n",
    "```\n",
    "\n",
    "\n",
    "Bu kısımda:\n",
    "\n",
    "* Model artık kendi tahmin ettiği token’ları üretir.\n",
    "\n",
    "* generate_sequence() içinde search stratejisi (greedy, top-k, top-p) uygulanır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a054a",
   "metadata": {},
   "source": [
    "## 🔹 4. Metrics ve Evaluation\n",
    "\n",
    "* Search sonrası elde ettiğin çıktı, burada ölçülür:\n",
    "```python\n",
    "\n",
    "bleu_score = calculate_bleu(pred_texts, ref_texts)\n",
    "rouge = compute_rouge(pred_texts, ref_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c0fa4",
   "metadata": {},
   "source": [
    "# 🔧 Özetle Dosya Akışı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655d495",
   "metadata": {},
   "source": [
    "| Aşama         | Kod Bölümü                     | Açıklama                            |\n",
    "| ------------- | ------------------------------ | ----------------------------------- |\n",
    "| 🔹 Model      | `encoder.py`, `decoder.py`     | Mimari tanımı                       |\n",
    "| 🔹 Train Loop | `train_loop.py`                | Teacher forcing, loss hesaplama     |\n",
    "| 🔹 Search     | `search.py` veya `generate.py` | **Greedy / Top-K / Top-P sampling** |\n",
    "| 🔹 Metrics    | `metrics.py`                   | BLEU, ROUGE, Perplexity hesaplama   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdde65e",
   "metadata": {},
   "source": [
    "# 🧩 LLM / Seq2Seq Üretim Akış Diyagramı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43674356",
   "metadata": {},
   "source": [
    "```bash\n",
    "                ┌────────────────────────┐\n",
    "                │      DataLoader        │\n",
    "                │  (tokenized inputs)    │\n",
    "                └──────────┬─────────────┘\n",
    "                           │\n",
    "                           ▼\n",
    "                ┌────────────────────────┐\n",
    "                │        Encoder         │\n",
    "                │  - Attention Layers    │\n",
    "                │  - Positional Embeds   │\n",
    "                └──────────┬─────────────┘\n",
    "                           │\n",
    "                 enc_out, enc_mask\n",
    "                           │\n",
    "                           ▼\n",
    "                ┌────────────────────────┐\n",
    "                │        Decoder         │\n",
    "                │  - Self-Attention      │\n",
    "                │  - Cross-Attention     │\n",
    "                │  - FFN Layers          │\n",
    "                └──────────┬─────────────┘\n",
    "                           │\n",
    "                           ▼\n",
    "          ┌────────────────────────────────┐\n",
    "          │        Search / Generation      │\n",
    "          │--------------------------------│\n",
    "          │  Greedy Sampling               │\n",
    "          │  Top-K Sampling                │\n",
    "          │  Top-P (Nucleus) Sampling      │\n",
    "          └──────────┬─────────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "          ┌─────────────────────────────┐\n",
    "          │        Postprocessing       │\n",
    "          │  - Detokenize output        │\n",
    "          │  - Trim <EOS> tokens        │\n",
    "          └──────────┬──────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "          ┌─────────────────────────────┐\n",
    "          │         Evaluation          │\n",
    "          │  - BLEU / ROUGE / PPL       │\n",
    "          │  - Accuracy (optional)      │\n",
    "          └─────────────────────────────┘\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8fd74",
   "metadata": {},
   "source": [
    "# ⚙️ Akış Sırası (main.py’de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f154e82",
   "metadata": {},
   "source": [
    "```python\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder\n",
    "from model.seq2seq import Seq2Seq\n",
    "from model.search import generate_sequence\n",
    "from utils.tokenizer import tokenizer\n",
    "from eval.metrics import calculate_bleu\n",
    "\n",
    "# 1️⃣ Model oluştur\n",
    "encoder = Encoder(...)\n",
    "decoder = Decoder(...)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# 2️⃣ Eğitim (train_loop.py)\n",
    "train_model(model, train_loader, ...)\n",
    "\n",
    "# 3️⃣ Inference (search.py)\n",
    "enc_out, enc_mask = encoder(src)\n",
    "generated_tokens = generate_sequence(\n",
    "    model.decoder,\n",
    "    enc_out=enc_out,\n",
    "    enc_mask=enc_mask,\n",
    "    start_token_id=tokenizer.bos_token_id,\n",
    "    top_k=50, top_p=None,\n",
    "    temperature=0.8,\n",
    "    max_len=100\n",
    ")\n",
    "\n",
    "# 4️⃣ Çözümleme\n",
    "decoded_text = tokenizer.decode(generated_tokens[0])\n",
    "print(\"🧠 Model Çıktısı:\", decoded_text)\n",
    "\n",
    "# 5️⃣ Değerlendirme\n",
    "score = calculate_bleu(decoded_text, reference_text)\n",
    "print(\"🎯 BLEU Skoru:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
