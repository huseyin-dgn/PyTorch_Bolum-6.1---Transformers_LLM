{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d64bcd",
   "metadata": {},
   "source": [
    "# greedy_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec15e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27331f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, enc_out, start_token_id, max_len=50, enc_mask=None, device='cuda'):\n",
    "    batch_size = enc_out.size(0)\n",
    "    ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9853d",
   "metadata": {},
   "source": [
    "### âš™ï¸ AdÄ±m AdÄ±m AÃ§Ä±klama\n",
    "\n",
    "### 1ï¸âƒ£ BaÅŸlangÄ±Ã§ Tokeni OluÅŸturma\n",
    "```python\n",
    "\n",
    "ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "```\n",
    "\n",
    "\n",
    "* Her Ã¶rnek iÃ§in decoderâ€™Ä±n baÅŸlayacaÄŸÄ± token (<BOS> veya start_token_id) oluÅŸturulur.\n",
    "\n",
    "* batch_size kadar satÄ±r, 1 token uzunluÄŸunda sÃ¼tun.\n",
    "\n",
    "### 2ï¸âƒ£ Token Ãœretim DÃ¶ngÃ¼sÃ¼\n",
    "\n",
    "```python\n",
    "for _ in range(max_len - 1):\n",
    "```\n",
    "\n",
    "\n",
    "* Maksimum max_len kadar token Ã¼retmek iÃ§in dÃ¶ngÃ¼.\n",
    "\n",
    "* -1, Ã§Ã¼nkÃ¼ baÅŸlangÄ±Ã§ tokeni zaten eklenmiÅŸ durumda.\n",
    "\n",
    "### 3ï¸âƒ£ Decoderâ€™dan Logitleri Almak\n",
    "\n",
    "```python\n",
    "logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder, mevcut ys dizisini kullanarak sonraki token olasÄ±lÄ±klarÄ±nÄ± (logits) Ã¼retir.\n",
    "\n",
    "* enc_out â†’ Encoder Ã§Ä±ktÄ±sÄ± (cross-attention iÃ§in)\n",
    "\n",
    "* enc_mask â†’ Encoder attention maskesi\n",
    "\n",
    "### 4ï¸âƒ£ En YÃ¼ksek OlasÄ±lÄ±ÄŸÄ± SeÃ§mek (Greedy)\n",
    "\n",
    "```python\n",
    "next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "```\n",
    "\n",
    "\n",
    "* logits[:, -1, :] â†’ Sadece en son Ã¼retilen tokenin olasÄ±lÄ±klarÄ± alÄ±nÄ±r.\n",
    "\n",
    "* torch.argmax â†’ En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip token seÃ§ilir (tam Greedy).\n",
    "\n",
    "### 5ï¸âƒ£ Yeni Tokeni Mevcut Dizinin Sonuna Eklemek\n",
    "```python\n",
    "ys = torch.cat([ys, next_token], dim=1)\n",
    "```\n",
    "\n",
    "\n",
    "* Mevcut token dizisine yeni token eklenir.\n",
    "\n",
    "* DÃ¶ngÃ¼ tekrar ettiÄŸinde decoder yeni diziyi kullanÄ±r.\n",
    "\n",
    "### 6ï¸âƒ£ SonuÃ§\n",
    "\n",
    "* DÃ¶ngÃ¼ tamamlandÄ±ÄŸÄ±nda ys â†’ tÃ¼m Ã¼retilmiÅŸ token dizilerini iÃ§erir.\n",
    "\n",
    "* Greedy strateji her zaman en yÃ¼ksek olasÄ±lÄ±ÄŸÄ± seÃ§er, bu yÃ¼zden deterministic ve hÄ±zlÄ±dÄ±r.\n",
    "\n",
    "## ğŸ’¡ Not:\n",
    "\n",
    "* Avantaj: Basit, hÄ±zlÄ±, deterministic.\n",
    "\n",
    "* Dezavantaj: â€œDaha iyi uzun cÃ¼mlelerâ€ veya â€œyaratÄ±cÄ±â€ sonuÃ§ Ã¼retmekte zayÄ±f olabilir.\n",
    "\n",
    "* Daha yaratÄ±cÄ±/Ã§eÅŸitlilik iÃ§in Top-K / Top-p sampling gerekir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a680b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d817c64",
   "metadata": {},
   "source": [
    "# top_k_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29339286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def top_k_sampling(model, enc_out, enc_mask, start_token_id, max_len=50, k=10, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Top-K Sampling ile token Ã¼retimi\n",
    "    model      : Decoder modeli\n",
    "    enc_out    : Encoder Ã§Ä±ktÄ±sÄ± (cross-attention iÃ§in)\n",
    "    enc_mask   : Encoder attention maskesi\n",
    "    start_token_id : BaÅŸlangÄ±Ã§ token ID\n",
    "    max_len    : Maksimum Ã¼retilen token sayÄ±sÄ±\n",
    "    k          : Top-K deÄŸeri\n",
    "    device     : \"cuda\" veya \"cpu\"\n",
    "    \"\"\"\n",
    "    batch_size = enc_out.size(0)\n",
    "    ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        # Decoderâ€™dan logitleri al\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        \n",
    "        # Son token logitleri\n",
    "        logits_last = logits[:, -1, :]\n",
    "        \n",
    "        # Top-K maskesi uygula\n",
    "        topk_values, topk_indices = torch.topk(logits_last, k=k, dim=-1)\n",
    "        probs = torch.zeros_like(logits_last)\n",
    "        probs.scatter_(-1, topk_indices, torch.softmax(topk_values, dim=-1))\n",
    "        \n",
    "        # Top-K Ã¼zerinden rastgele token seÃ§\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Yeni tokeni mevcut dizinin sonuna ekle\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd42dc",
   "metadata": {},
   "source": [
    "### âš™ï¸ AdÄ±m AdÄ±m AÃ§Ä±klama\n",
    "#### 1ï¸âƒ£ BaÅŸlangÄ±Ã§ Tokeni OluÅŸturma\n",
    "```python\n",
    "ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoderâ€™Ä±n baÅŸlayacaÄŸÄ± token (<BOS> veya start_token_id) oluÅŸturulur.\n",
    "\n",
    "* Her Ã¶rnek iÃ§in bir satÄ±r, 1 token uzunluÄŸunda sÃ¼tun.\n",
    "\n",
    "* Batch boyutu kadar token dizisi baÅŸlatÄ±lÄ±r.\n",
    "\n",
    "### 2ï¸âƒ£ Token Ãœretim DÃ¶ngÃ¼sÃ¼\n",
    "```python\n",
    "for _ in range(max_len - 1):\n",
    "```\n",
    "\n",
    "\n",
    "* Maksimum max_len kadar token Ã¼retmek iÃ§in dÃ¶ngÃ¼.\n",
    "\n",
    "* -1 Ã§Ã¼nkÃ¼ baÅŸlangÄ±Ã§ tokeni zaten eklenmiÅŸ durumda.\n",
    "\n",
    "#### 3ï¸âƒ£ Decoderâ€™dan Logitleri Almak\n",
    "```python\n",
    "logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder mevcut ys dizisini kullanarak sonraki token olasÄ±lÄ±klarÄ±nÄ± (logits) Ã¼retir.\n",
    "\n",
    "* enc_out â†’ Encoder Ã§Ä±ktÄ±sÄ± (cross-attention iÃ§in).\n",
    "\n",
    "* enc_mask â†’ Encoder attention maskesi.\n",
    "\n",
    "#### 4ï¸âƒ£ Son Token Logitlerini Al\n",
    "```python\n",
    "logits_last = logits[:, -1, :]\n",
    "```\n",
    "\n",
    "\n",
    "* Sadece en son Ã¼retilen tokenin logitleri alÄ±nÄ±r.\n",
    "\n",
    "* Ã–nceki tokenler dikkate alÄ±nmaz, Ã§Ã¼nkÃ¼ bu adÄ±mda bir sonraki token Ã¼retiliyor.\n",
    "\n",
    "#### 5ï¸âƒ£ Top-K SeÃ§imi\n",
    "```python\n",
    "topk_values, topk_indices = torch.topk(logits_last, k=k, dim=-1)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.topk ile en yÃ¼ksek K olasÄ±lÄ±ÄŸa sahip tokenler seÃ§ilir.\n",
    "\n",
    "* topk_values â†’ olasÄ±lÄ±k deÄŸerleri\n",
    "\n",
    "* topk_indices â†’ token IDâ€™leri\n",
    "\n",
    "#### 6ï¸âƒ£ OlasÄ±lÄ±klarÄ± Normalize Et\n",
    "```python\n",
    "probs = torch.zeros_like(logits_last)\n",
    "probs.scatter_(-1, topk_indices, torch.softmax(topk_values, dim=-1))\n",
    "```\n",
    "\n",
    "\n",
    "* Top-K tokenler dÄ±ÅŸÄ±ndaki tÃ¼m tokenler sÄ±fÄ±rlanÄ±r.\n",
    "\n",
    "* torch.softmax ile seÃ§ilen K tokenin olasÄ±lÄ±klarÄ± normalize edilir.\n",
    "\n",
    "* scatter_ ile olasÄ±lÄ±k tensorÃ¼ne yerleÅŸtirilir.\n",
    "\n",
    "### 7ï¸âƒ£ Rastgele Token SeÃ§imi\n",
    "```python\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "\n",
    "* torch.multinomial ile olasÄ±lÄ±klara gÃ¶re rastgele token seÃ§ilir.\n",
    "\n",
    "* Yani en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip token her zaman seÃ§ilmez, bu sayede Ã§eÅŸitlilik saÄŸlanÄ±r.\n",
    "\n",
    "#### 8ï¸âƒ£ Tokeni Mevcut Dizinin Sonuna Ekle\n",
    "```python\n",
    "ys = torch.cat([ys, next_token], dim=1)\n",
    "```\n",
    "\n",
    "* Yeni token, mevcut token dizisinin sonuna eklenir.\n",
    "\n",
    "* DÃ¶ngÃ¼ tekrar ettiÄŸinde decoder, gÃ¼ncellenmiÅŸ diziyi kullanÄ±r.\n",
    "\n",
    "#### 9ï¸âƒ£ DÃ¶ngÃ¼ SonrasÄ±\n",
    "\n",
    "* DÃ¶ngÃ¼ tamamlandÄ±ÄŸÄ±nda ys â†’ tÃ¼m Ã¼retilmiÅŸ token dizilerini iÃ§erir.\n",
    "\n",
    "* Top-K Sampling, yaratÄ±cÄ± ve Ã§eÅŸitli sonuÃ§lar Ã¼retmek iÃ§in deterministic olmayan bir yÃ¶ntemdir.\n",
    "\n",
    "ğŸ’¡ Notlar:\n",
    "\n",
    "* k deÄŸeri kÃ¼Ã§Ã¼ldÃ¼kÃ§e â†’ daha deterministic (daha az Ã§eÅŸitlilik)\n",
    "\n",
    "* k deÄŸeri bÃ¼yÃ¼dÃ¼kÃ§e â†’ daha yaratÄ±cÄ± ve Ã§eÅŸitli Ã§Ä±ktÄ±\n",
    "\n",
    "* Greedyâ€™ye gÃ¶re daha fazla rastgelelik saÄŸlar ama olasÄ±lÄ±k hesaplama biraz daha maliyetlidir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed1fca",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac29fca",
   "metadata": {},
   "source": [
    "# ğŸ”¹ Top-p Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(model, enc_out, enc_mask, start_token_id, max_len=20, p=0.9, device=\"cpu\"):\n",
    "    batch_size = enc_out.size(0)\n",
    "    ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        logits_last = logits[:, -1, :]  # Son token logitleri\n",
    "\n",
    "        # Softmax ile olasÄ±lÄ±klarÄ± hesapla\n",
    "        probs = torch.softmax(logits_last, dim=-1)\n",
    "\n",
    "        # Top-P maskesi oluÅŸtur\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "        cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cum_probs > p\n",
    "\n",
    "        # Ä°lk True deÄŸer dÄ±ÅŸÄ±ndakileri kapat\n",
    "        mask[:, 1:] = mask[:, :-1].clone()\n",
    "        mask[:, 0] = False\n",
    "\n",
    "        # Pâ€™nin dÄ±ÅŸÄ±ndaki tokenleri sÄ±fÄ±rla\n",
    "        sorted_probs[mask] = 0.0\n",
    "\n",
    "        # Normalize et\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Tokenleri geri yerleÅŸtir\n",
    "        probs.zero_()\n",
    "        probs.scatter_(-1, sorted_indices, sorted_probs)\n",
    "\n",
    "        # Rastgele token seÃ§\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Tokeni mevcut dizinin sonuna ekle\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679bebb",
   "metadata": {},
   "source": [
    "### âš™ï¸ AdÄ±m AdÄ±m AÃ§Ä±klama (Markdown FormatÄ±)\n",
    "#### 1ï¸âƒ£ BaÅŸlangÄ±Ã§ Tokeni OluÅŸturma\n",
    "```PYTHON\n",
    "ys = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoderâ€™Ä±n baÅŸlayacaÄŸÄ± token (<BOS> veya start_token_id) oluÅŸturulur.\n",
    "\n",
    "* Her Ã¶rnek iÃ§in bir satÄ±r, 1 token uzunluÄŸunda sÃ¼tun.\n",
    "\n",
    "* Batch boyutu kadar token dizisi baÅŸlatÄ±lÄ±r.\n",
    "\n",
    "#### 2ï¸âƒ£ Token Ãœretim DÃ¶ngÃ¼sÃ¼\n",
    "```python\n",
    "for _ in range(max_len - 1):\n",
    "```\n",
    "\n",
    "\n",
    "* Maksimum max_len kadar token Ã¼retmek iÃ§in dÃ¶ngÃ¼.\n",
    "\n",
    "* -1 Ã§Ã¼nkÃ¼ baÅŸlangÄ±Ã§ tokeni zaten eklenmiÅŸ durumda.\n",
    "\n",
    "#### 3ï¸âƒ£ Decoderâ€™dan Logitleri Almak\n",
    "```python\n",
    "logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "```\n",
    "\n",
    "\n",
    "* Decoder mevcut ys dizisini kullanarak sonraki token olasÄ±lÄ±klarÄ±nÄ± (logits) Ã¼retir.\n",
    "\n",
    "* enc_out â†’ Encoder Ã§Ä±ktÄ±sÄ± (cross-attention iÃ§in).\n",
    "\n",
    "* enc_mask â†’ Encoder attention maskesi.\n",
    "\n",
    "### 4ï¸âƒ£ Son Token Logitlerini Al\n",
    "```python\n",
    "logits_last = logits[:, -1, :]\n",
    "```\n",
    "\n",
    "\n",
    "* Sadece en son Ã¼retilen tokenin logitleri alÄ±nÄ±r.\n",
    "\n",
    "#### 5ï¸âƒ£ OlasÄ±lÄ±klarÄ± Hesapla\n",
    "```python\n",
    "probs = torch.softmax(logits_last, dim=-1)\n",
    "```\n",
    "\n",
    "\n",
    "* Logitleri olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in softmax uygulanÄ±r.\n",
    "\n",
    "#### 6ï¸âƒ£ Top-P Maskesi OluÅŸtur\n",
    "```python\n",
    "sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "mask = cum_probs > p\n",
    "```\n",
    "\n",
    "\n",
    "* Tokenleri olasÄ±lÄ±klarÄ±na gÃ¶re sÄ±rala (yÃ¼ksekten dÃ¼ÅŸÃ¼ÄŸe).\n",
    "\n",
    "* KÃ¼mÃ¼latif olasÄ±lÄ±k (cum_probs) hesaplanÄ±r.\n",
    "\n",
    "* KÃ¼mÃ¼latif olasÄ±lÄ±k p deÄŸerini aÅŸan tokenler maskelenir.\n",
    "\n",
    "#### 7ï¸âƒ£ P DÄ±ÅŸÄ±ndaki Tokenleri SÄ±fÄ±rla\n",
    "```python\n",
    "mask[:, 1:] = mask[:, :-1].clone()\n",
    "mask[:, 0] = False\n",
    "sorted_probs[mask] = 0.0\n",
    "```\n",
    "\n",
    "* Ä°lk tokenin dÄ±ÅŸÄ±ndaki aÅŸan kÃ¼mÃ¼latif olasÄ±lÄ±klarÄ± sÄ±fÄ±rla.\n",
    "\n",
    "* Bu ÅŸekilde olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± Nucleus (Top-P) iÃ§inde kalÄ±r.\n",
    "\n",
    "#### 8ï¸âƒ£ Normalize Et\n",
    "```python\n",
    "sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "probs.zero_()\n",
    "probs.scatter_(-1, sorted_indices, sorted_probs)\n",
    "```\n",
    "\n",
    "\n",
    "* Top-P iÃ§erisindeki token olasÄ±lÄ±klarÄ± normalize edilir.\n",
    "\n",
    "* Scatter ile orijinal token pozisyonlarÄ±na geri yerleÅŸtirilir.\n",
    "\n",
    "#### 9ï¸âƒ£ Rastgele Token SeÃ§imi\n",
    "```python\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "\n",
    "* Softmax olasÄ±lÄ±klarÄ±na gÃ¶re rastgele token seÃ§ilir.\n",
    "\n",
    "* BÃ¶ylece, her zaman en yÃ¼ksek olasÄ±lÄ±k seÃ§ilmez â†’ yaratÄ±cÄ± ve Ã§eÅŸitli Ã§Ä±ktÄ±lar saÄŸlanÄ±r.\n",
    "\n",
    "#### 10ï¸âƒ£ Tokeni Mevcut Dizinin Sonuna Ekle\n",
    "```python\n",
    "ys = torch.cat([ys, next_token], dim=1)\n",
    "```\n",
    "\n",
    "\n",
    "* Yeni token, mevcut token dizisinin sonuna eklenir.\n",
    "\n",
    "#### 11ï¸âƒ£ DÃ¶ngÃ¼ SonrasÄ±\n",
    "\n",
    "* DÃ¶ngÃ¼ tamamlandÄ±ÄŸÄ±nda ys â†’ tÃ¼m Ã¼retilmiÅŸ token dizilerini iÃ§erir.\n",
    "\n",
    "* Top-P Sampling, Top-Kâ€™den farklÄ± olarak belirli bir olasÄ±lÄ±k kÃ¼tlesini (p) kapsar.\n",
    "\n",
    "ğŸ’¡ Notlar:\n",
    "\n",
    "* p deÄŸeri kÃ¼Ã§Ã¼ldÃ¼kÃ§e â†’ daha deterministic, daha az Ã§eÅŸitlilik\n",
    "\n",
    "* p deÄŸeri bÃ¼yÃ¼dÃ¼kÃ§e â†’ daha yaratÄ±cÄ± ve Ã§eÅŸitli Ã§Ä±ktÄ±\n",
    "\n",
    "Greedyâ€™ye gÃ¶re Ã§ok daha rastgele ve yaratÄ±cÄ± sonuÃ§lar saÄŸlar, Ã¶zellikle uzun metin Ã¼retiminde tercih edilir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34d31b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ef2ad",
   "metadata": {},
   "source": [
    "# ğŸ”¹ Top-K ve Top-P KarÅŸÄ±laÅŸtÄ±rma Tablosu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456b02c",
   "metadata": {},
   "source": [
    "| Ã–zellik                 | Top-K Sampling                                                          | Top-P (Nucleus) Sampling                                                           |\n",
    "| ----------------------- | ----------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **SeÃ§im Kriteri**       | En yÃ¼ksek K olasÄ±lÄ±ÄŸa sahip tokenler arasÄ±ndan rastgele seÃ§im           | KÃ¼mÃ¼latif olasÄ±lÄ±ÄŸÄ± `p`â€™yi aÅŸmayan tokenler arasÄ±ndan rastgele seÃ§im               |\n",
    "| **Deterministik mi?**   | HayÄ±r, rastgelelik mevcut                                               | HayÄ±r, rastgelelik mevcut                                                          |\n",
    "| **Kontrol Parametresi** | `k` (seÃ§ilecek en yÃ¼ksek olasÄ±lÄ±klÄ± token sayÄ±sÄ±)                       | `p` (toplam kÃ¼mÃ¼latif olasÄ±lÄ±k eÅŸiÄŸi, 0-1)                                         |\n",
    "| **Avantaj**             | Basit, hÄ±zlÄ±, Ã§eÅŸitlilik kolayca kontrol edilebilir                     | OlasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na gÃ¶re dinamik seÃ§im â†’ daha doÄŸal ve yaratÄ±cÄ± metin Ã¼retimi      |\n",
    "| **Dezavantaj**          | Sabit K nedeniyle bazen Ã§ok sÄ±kÄ±cÄ± veya Ã§ok riskli tokenler seÃ§ilebilir | Hesaplama biraz daha maliyetli, p seÃ§imi hassas                                    |\n",
    "| **YaratÄ±cÄ±lÄ±k**         | Orta â†’ K kÃ¼Ã§Ã¼kse deterministic, bÃ¼yÃ¼kse Ã§eÅŸitlilik artar                | YÃ¼ksek â†’ doÄŸal ve Ã§eÅŸitli Ã§Ä±ktÄ±lar saÄŸlar, Ã¶zellikle uzun metinlerde tercih edilir |\n",
    "| **Tipik KullanÄ±m**      | KÄ±sa metin, basit cevaplar, hÄ±zlÄ± inference                             | Uzun metin, yaratÄ±cÄ± Ã¼retim, doÄŸal dil Ã¼retimi                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25a8b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034f3ee",
   "metadata": {},
   "source": [
    "# LLM ve transformer tabanlÄ± modeller iÃ§in hem Top-K hem Top-P Samplingâ€™i destekleyen, esnek ve dinamik bir Python fonksiyon seti hazÄ±rlayalÄ±m. Kod, batch desteÄŸi ve GPU uyumluluÄŸu iÃ§eriyor olsun. AyrÄ±ca temperature parametresi ile sÄ±caklÄ±k kontrolÃ¼ de ekleyelim; bÃ¶ylece daha yaratÄ±cÄ± veya deterministik sonuÃ§lar elde edebiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4a9b7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6e2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_logits(logits, top_k=None, top_p=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Logitlerden token seÃ§imi iÃ§in Top-K ve Top-P Sampling.\n",
    "\n",
    "    Args:\n",
    "        logits: [batch_size, vocab_size] tensor\n",
    "        top_k: int, en yÃ¼ksek K olasÄ±lÄ±ÄŸa sahip tokenler\n",
    "        top_p: float, kÃ¼mÃ¼latif olasÄ±lÄ±k eÅŸiÄŸi (0-1)\n",
    "        temperature: float, 1.0 varsayÄ±lan, <1 deterministik, >1 Ã§eÅŸitlilik\n",
    "\n",
    "    Returns:\n",
    "        next_token: [batch_size, 1] seÃ§ilen tokenler\n",
    "    \"\"\"\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Top-K Filtering\n",
    "    if top_k is not None and top_k > 0:\n",
    "        topk_values, topk_indices = torch.topk(probs, top_k, dim=-1)\n",
    "        probs_filtered = torch.zeros_like(probs).scatter_(-1, topk_indices, topk_values)\n",
    "        probs = probs_filtered\n",
    "\n",
    "    # Top-P (Nucleus) Filtering\n",
    "    if top_p is not None and 0.0 < top_p < 1.0:\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        # Mask dÄ±ÅŸÄ±ndaki tokenleri sÄ±fÄ±rla\n",
    "        sorted_mask = cumulative_probs > top_p\n",
    "        # Ä°lk maskeli token dahil edilmez\n",
    "        sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "        sorted_mask[..., 0] = 0\n",
    "        sorted_probs[sorted_mask] = 0.0\n",
    "        # TensorÃ¼ orijinal sÄ±raya geri dÃ¶ndÃ¼r\n",
    "        probs = torch.zeros_like(probs).scatter_(-1, sorted_indices, sorted_probs)\n",
    "\n",
    "    # Normalize edelim\n",
    "    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Token seÃ§\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def generate_sequence(model, start_tokens, max_len=50, top_k=None, top_p=None, temperature=1.0, enc_out=None, enc_mask=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    LLM Decoder iÃ§in esnek Top-K / Top-P / Temperature sampling Ã¼retimi.\n",
    "    Args:\n",
    "        model: decoder modeli\n",
    "        start_tokens: [batch_size, 1] baÅŸlangÄ±Ã§ tokenleri\n",
    "        max_len: Ã¼retilecek token sayÄ±sÄ±\n",
    "        top_k: Top-K parametresi\n",
    "        top_p: Top-P parametresi\n",
    "        temperature: sÄ±caklÄ±k kontrolÃ¼\n",
    "        enc_out: encoder Ã§Ä±ktÄ±sÄ± (cross-attention)\n",
    "        enc_mask: encoder attention maskesi\n",
    "        device: cpu/cuda\n",
    "\n",
    "    Returns:\n",
    "        ys: [batch_size, max_len] Ã¼retilmiÅŸ tokenler\n",
    "    \"\"\"\n",
    "    ys = start_tokens.to(device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model(ys, enc_out=enc_out, enc_mask=enc_mask)\n",
    "        logits_last = logits[:, -1, :]  # Sadece en son token\n",
    "        next_token = sample_logits(logits_last, top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c1dc3",
   "metadata": {},
   "source": [
    "## âš¡ Ã–zellikler ve Avantajlar\n",
    "\n",
    "* Hem Top-K hem Top-P\n",
    "\n",
    "AynÄ± fonksiyon iÃ§inde seÃ§ilebilir.\n",
    "\n",
    "* Temperature kontrolÃ¼\n",
    "\n",
    "temperature<1 â†’ deterministik, gÃ¼venli seÃ§imler\n",
    "\n",
    "temperature>1 â†’ daha yaratÄ±cÄ±, Ã§eÅŸitlilik artar\n",
    "\n",
    "* Batch desteÄŸi\n",
    "\n",
    "Tek seferde tÃ¼m batch iÃ§in token Ã¼retir.\n",
    "\n",
    "* Encoder uyumlu\n",
    "\n",
    "Cross-attention (enc_out ve enc_mask) parametreleri ile seq2seq modelleri iÃ§in hazÄ±r.\n",
    "\n",
    "* Dinamik seÃ§im\n",
    "\n",
    "Top-K veya Top-P tek baÅŸÄ±na veya birlikte kullanÄ±labilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b0d21",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7a49f",
   "metadata": {},
   "source": [
    "# ğŸ“ Kodun Genel YapÄ±sÄ±nda â€œSearchâ€ Nerede Olur?\n",
    "## ğŸ”¹ 1. Model tanÄ±mÄ± (Encoderâ€“Decoder)\n",
    "\n",
    "* Ã–nce zaten yaptÄ±ÄŸÄ±mÄ±z gibi:\n",
    "\n",
    "```bash\n",
    "encoder = Encoder(...)\n",
    "decoder = Decoder(...)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "```\n",
    "\n",
    "\n",
    "* Bu kÄ±sÄ±mda modelin yapÄ±sÄ± tanÄ±mlanÄ±r.\n",
    "* HenÃ¼z herhangi bir â€œsearchâ€ stratejisi uygulanmaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93170f9",
   "metadata": {},
   "source": [
    "##ğŸ”¹ 2. Train Loop (EÄŸitim)\n",
    "\n",
    "* Burada model, gerÃ§ek hedef dizilerini (â€œteacher forcingâ€ ile) Ã¶ÄŸrenir.\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        enc_out = encoder(src)\n",
    "        outputs = decoder(trg_inp, enc_out)\n",
    "        loss = criterion(outputs, trg_labels)\n",
    "        ...\n",
    "```\n",
    "\n",
    "\n",
    "* Burada â€œsearchâ€ YOK Ã§Ã¼nkÃ¼:\n",
    "\n",
    "* EÄŸitim sÄ±rasÄ±nda hedef tokenâ€™lar zaten bilinir.\n",
    "Model tahmin etmez, Ã¶ÄŸrenir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4ad41",
   "metadata": {},
   "source": [
    "## ğŸ”¹ 3. Search / Generation (Inference veya Evaluation AÅŸamasÄ±)\n",
    "\n",
    "* Ä°ÅŸte bizim yazdÄ±ÄŸÄ±mÄ±z Top-K / Top-P / Greedy kodu burada devreye girer.\n",
    "\n",
    "* Yani modeli eÄŸittikten sonra:\n",
    "```python\n",
    "\n",
    "# ---- Inference / Generation ----\n",
    "model.eval()\n",
    "\n",
    "start_tokens = torch.tensor([[tokenizer.bos_token_id]]).to(device)\n",
    "\n",
    "# LLM veya Transformer Ã¼retim aÅŸamasÄ±\n",
    "generated = generate_sequence(\n",
    "    model=model.decoder,   # sadece decoder veya tam seq2seq\n",
    "    start_tokens=start_tokens,\n",
    "    max_len=50,\n",
    "    top_k=50,              # veya None\n",
    "    top_p=0.9,             # veya None\n",
    "    temperature=0.8,\n",
    "    enc_out=encoder_output, # encoder varsa\n",
    "    enc_mask=encoder_mask,  # encoder maskesi\n",
    "    device=device\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(generated[0])\n",
    "print(decoded_text)\n",
    "```\n",
    "\n",
    "\n",
    "Bu kÄ±sÄ±mda:\n",
    "\n",
    "* Model artÄ±k kendi tahmin ettiÄŸi tokenâ€™larÄ± Ã¼retir.\n",
    "\n",
    "* generate_sequence() iÃ§inde search stratejisi (greedy, top-k, top-p) uygulanÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a054a",
   "metadata": {},
   "source": [
    "## ğŸ”¹ 4. Metrics ve Evaluation\n",
    "\n",
    "* Search sonrasÄ± elde ettiÄŸin Ã§Ä±ktÄ±, burada Ã¶lÃ§Ã¼lÃ¼r:\n",
    "```python\n",
    "\n",
    "bleu_score = calculate_bleu(pred_texts, ref_texts)\n",
    "rouge = compute_rouge(pred_texts, ref_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c0fa4",
   "metadata": {},
   "source": [
    "# ğŸ”§ Ã–zetle Dosya AkÄ±ÅŸÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655d495",
   "metadata": {},
   "source": [
    "| AÅŸama         | Kod BÃ¶lÃ¼mÃ¼                     | AÃ§Ä±klama                            |\n",
    "| ------------- | ------------------------------ | ----------------------------------- |\n",
    "| ğŸ”¹ Model      | `encoder.py`, `decoder.py`     | Mimari tanÄ±mÄ±                       |\n",
    "| ğŸ”¹ Train Loop | `train_loop.py`                | Teacher forcing, loss hesaplama     |\n",
    "| ğŸ”¹ Search     | `search.py` veya `generate.py` | **Greedy / Top-K / Top-P sampling** |\n",
    "| ğŸ”¹ Metrics    | `metrics.py`                   | BLEU, ROUGE, Perplexity hesaplama   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdde65e",
   "metadata": {},
   "source": [
    "# ğŸ§© LLM / Seq2Seq Ãœretim AkÄ±ÅŸ DiyagramÄ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43674356",
   "metadata": {},
   "source": [
    "```bash\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚      DataLoader        â”‚\n",
    "                â”‚  (tokenized inputs)    â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚        Encoder         â”‚\n",
    "                â”‚  - Attention Layers    â”‚\n",
    "                â”‚  - Positional Embeds   â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                 enc_out, enc_mask\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚        Decoder         â”‚\n",
    "                â”‚  - Self-Attention      â”‚\n",
    "                â”‚  - Cross-Attention     â”‚\n",
    "                â”‚  - FFN Layers          â”‚\n",
    "                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                           â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚        Search / Generation      â”‚\n",
    "          â”‚--------------------------------â”‚\n",
    "          â”‚  Greedy Sampling               â”‚\n",
    "          â”‚  Top-K Sampling                â”‚\n",
    "          â”‚  Top-P (Nucleus) Sampling      â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚        Postprocessing       â”‚\n",
    "          â”‚  - Detokenize output        â”‚\n",
    "          â”‚  - Trim <EOS> tokens        â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â”‚\n",
    "                     â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚         Evaluation          â”‚\n",
    "          â”‚  - BLEU / ROUGE / PPL       â”‚\n",
    "          â”‚  - Accuracy (optional)      â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8fd74",
   "metadata": {},
   "source": [
    "# âš™ï¸ AkÄ±ÅŸ SÄ±rasÄ± (main.pyâ€™de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f154e82",
   "metadata": {},
   "source": [
    "```python\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder\n",
    "from model.seq2seq import Seq2Seq\n",
    "from model.search import generate_sequence\n",
    "from utils.tokenizer import tokenizer\n",
    "from eval.metrics import calculate_bleu\n",
    "\n",
    "# 1ï¸âƒ£ Model oluÅŸtur\n",
    "encoder = Encoder(...)\n",
    "decoder = Decoder(...)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# 2ï¸âƒ£ EÄŸitim (train_loop.py)\n",
    "train_model(model, train_loader, ...)\n",
    "\n",
    "# 3ï¸âƒ£ Inference (search.py)\n",
    "enc_out, enc_mask = encoder(src)\n",
    "generated_tokens = generate_sequence(\n",
    "    model.decoder,\n",
    "    enc_out=enc_out,\n",
    "    enc_mask=enc_mask,\n",
    "    start_token_id=tokenizer.bos_token_id,\n",
    "    top_k=50, top_p=None,\n",
    "    temperature=0.8,\n",
    "    max_len=100\n",
    ")\n",
    "\n",
    "# 4ï¸âƒ£ Ã‡Ã¶zÃ¼mleme\n",
    "decoded_text = tokenizer.decode(generated_tokens[0])\n",
    "print(\"ğŸ§  Model Ã‡Ä±ktÄ±sÄ±:\", decoded_text)\n",
    "\n",
    "# 5ï¸âƒ£ DeÄŸerlendirme\n",
    "score = calculate_bleu(decoded_text, reference_text)\n",
    "print(\"ğŸ¯ BLEU Skoru:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
