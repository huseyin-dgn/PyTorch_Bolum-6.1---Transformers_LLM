# ğŸš€ PyTorch_Bolum-6.1---Transformers_LLM

## ğŸ“Œ AÃ§Ä±klama
Bu repo, PyTorch ile Transformer tabanlÄ± LLM Ã§alÄ±ÅŸmalarÄ±nÄ± adÄ±m adÄ±m Ã¶ÄŸretmek iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r. Tokenizasyon, model tasarÄ±mÄ±, eÄŸitim dÃ¶ngÃ¼leri, loss & metric hesaplamalarÄ± ve attention mekanizmalarÄ±nÄ± iÃ§erir.

---

## ğŸ“‚ Dosya YapÄ±sÄ±
```bash
/LLM
â”‚
â”œâ”€ 1_VerÄ±_HazÄ±rlama_-Tokenizasyon/
â”‚ â”œâ”€ code/            # Tokenizasyon ve Dataset implementasyonu
â”‚ â””â”€ teorikler/       # Tokenizasyon teorik anlatÄ±mÄ±
â”‚
â”œâ”€ 2_Model_TasarÄ±mÄ±-_LLM/
â”‚ â”œâ”€ code/            # Encoder & Decoder bloklarÄ± implementasyonu
â”‚ â””â”€ teorikler/       # LLM mimarisi ve teorik anlatÄ±m
â”‚
â”œâ”€ 3_Tokenizasyon_ve_Model/   # Tokenizasyon ve model entegrasyonu
â”‚
â”œâ”€ 4_Search-Inference/
â”‚ â”œâ”€ code/            # Inference ve search Ã¶rnekleri
â”‚ â””â”€ teorikler/       # Teorik anlatÄ±m
â”‚
â”œâ”€ 5_Train_Loop_Eval/
â”‚ â”œâ”€ code/            # EÄŸitim dÃ¶ngÃ¼sÃ¼ ve evaluation implementasyonu
â”‚ â””â”€ teorikler/       # EÄŸitim mantÄ±ÄŸÄ±, backprop, loss, optimizer
â”‚
â”œâ”€ 6_Tokenizasyon-Model-TrainLoop/  # TÃ¼m pipeline tek dosyada
â”‚
â”œâ”€ 7_Compile&Loss/    # Compile ve loss fonksiyonlarÄ±
â”‚
â”œâ”€ 8_Metrics/         # Metric fonksiyonlarÄ±
â”‚
â”œâ”€ 9_BaÅŸtan_Sona_LLM/ # Komple LLM uygulama Ã¶rneÄŸi
â”‚
â”œâ”€ Attention_ModÃ¼lleri/
â”‚ â”œâ”€ Cross_Attention/
â”‚ â”‚ â”œâ”€ code/         # Cross-Attention implementasyonu
â”‚ â”‚ â””â”€ teorikler/    # Cross-Attention teorik anlatÄ±mÄ±
â”‚ â”œâ”€ Dot_Product_Attention/
â”‚ â”‚ â”œâ”€ code/         # Dot-Product Attention implementasyonu
â”‚ â”‚ â””â”€ teorikler/    # Dot-Product Attention teorik anlatÄ±mÄ±
â”‚ â”œâ”€ MultiHead_Attention/
â”‚ â”‚ â”œâ”€ code/         # Multi-Head Attention implementasyonu
â”‚ â”‚ â””â”€ teorikler/    # Multi-Head Attention teorik anlatÄ±mÄ±
â”‚ â””â”€ Self_Attention/
â”‚   â”œâ”€ code/         # Self-Attention implementasyonu
â”‚   â””â”€ teorikler/    # Self-Attention teorik anlatÄ±mÄ±
â”‚
â””â”€ Uygulama-_LLM/
  â”œâ”€ gui/                        # KullanÄ±cÄ± arayÃ¼zÃ¼
  â”œâ”€ model/                      # AdÄ±m adÄ±m eÄŸittiÄŸimiz model ve tokenizer
  â”œâ”€ aÃ§Ä±klamalar/                # Uygulama diyagramÄ± ve aÃ§Ä±klamalar
  â”œâ”€ masked_cross_entropy/       # Maskeli CrossEntropy Loss implementasyonu
  â””â”€ calculate_llm_metrics/      # LLM metrik hesaplama (PPL, BLEU vb.)

```

---

## âš™ KullanÄ±lan Teknikler ve ModÃ¼ller

- ğŸ§  Encoder-Decoder LLM: Girdi metnini gizli temsile dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve Ã§Ä±ktÄ± metni Ã¼retir.  
- ğŸ”¤ Tokenizasyon: HuggingFace AutoTokenizer veya Ã¶zel tokenizatÃ¶r ile girdi/Ã§Ä±ktÄ± metinlerini tensÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.  
- ğŸ”„ Transformer BloklarÄ±:  
  - Self-Attention: Girdinin kendi iÃ§indeki baÄŸÄ±ntÄ±larÄ± Ã¶ÄŸrenir.  
  - Cross-Attention: Decoder, encoder Ã§Ä±ktÄ±sÄ±na bakarak anlamlÄ± Ã¼retim yapar.  
  - Multi-Head Attention: Paralel attention baÅŸlÄ±klarÄ± ile bilgi kapsar.  
  - Dot-Product Attention: Attention skorlarÄ±nÄ± hÄ±zlÄ± hesaplar.  
- âš¡ Feed-Forward & LayerNorm: Lineer dÃ¶nÃ¼ÅŸÃ¼mler ve normalizasyon saÄŸlar.  
- ğŸ”— Residual Connection: Katmanlar arasÄ± bilgi kaybÄ±nÄ± Ã¶nler.  
- ğŸ›¡ DropPath & Dropout: Overfittingâ€™i engeller.  
- ğŸ¯ SwiGLU & GELU AktivasyonlarÄ±: Non-linearity saÄŸlar.  
- ğŸ“‰ Masked Cross-Entropy Loss: Pad tokenlarÄ± dikkate almaz, kaybÄ± doÄŸru hesaplar.  
- ğŸ§° Optimizer ve Scheduler: AdamW ve get_linear_schedule_with_warmup ile Ã¶ÄŸrenme oranÄ± kontrolÃ¼.  
- ğŸ² Top-K / Top-P Sampling: Sequence generation sÄ±rasÄ±nda olasÄ± tokenlarÄ± filtreler.  
- ğŸ“Š Metrics: Accuracy, Top-K accuracy, Perplexity, Sent-BLEU ve Corpus-BLEU.  
- ğŸ›‘ NaN / Inf Kontrolleri: Numerical stabilite saÄŸlar.  
- ğŸ’¾ EMA (Exponential Moving Average): AÄŸÄ±rlÄ±klarÄ± stabilize eder.  
- ğŸ“¦ Seq2Seq Dataset YapÄ±sÄ±: Encoder input ve decoder targetâ€™larÄ±nÄ± organize eder.  
- ğŸš§ Attention Masking: Padding ve future tokenâ€™larÄ± gizler.  
- ğŸ“ Generation Pipeline: Greedy, Top-K ve Top-P yÃ¶ntemleri ile gÃ¼venli metin Ã¼retimi.  

---

## ğŸ“Š Veri Seti

- Kaynak: [HuggingFace â€“ Renicames/turkish-law-chatbot](https://huggingface.co/datasets/Renicames/turkish-law-chatbot)
- KullanÄ±m AlanlarÄ±: Hukuki metinlerin Ã¶zetlenmesi, hukuki sorulara yanÄ±t verilmesi, anayasal kavramlarÄ±n anlaÅŸÄ±lmasÄ±.
- Lisans: Apache 2.0
- Kaynaklar: TÃ¼rkiye Cumhuriyeti AnayasasÄ±, Hukuki Metinler ve AÃ§Ä±klamalar, Avukatlara SÄ±kÃ§a Sorulan Sorular

---

# âš  Notlar:  
- Kaydedilen .pt modeli boyut olarak Ã§ok yÃ¼ksek olduÄŸundan repoya dahil edilmemiÅŸtir.  
- YapÄ±lan uygulamada **epoch sayÄ±sÄ± 5 olarak ayarlanmÄ±ÅŸtÄ±r. UygulamanÄ±n daha net bir sonuÃ§ vermesini istiyorsanÄ±z CUDA belleÄŸine dikkat ederek epoch sayÄ±nÄ±nÄ± arttÄ±rabilirsiniz.**
- Tokenizer, modelin tÃ¼m adÄ±mlarÄ±nda uyumlu ÅŸekilde kullanÄ±lmÄ±ÅŸtÄ±r.
